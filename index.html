<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">rongyuewu</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="https://github.com/smackgg/hexo-theme-smackdown">smackdown</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">rongyuewu</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">rongyuewu</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-Hive的优化：" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/03/Hive的优化：/" class="article-date">
  	<time datetime="2018-05-03T13:00:30.000Z" itemprop="datePublished">2018-05-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/03/Hive的优化：/">
        HIVE 总结
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="HIVE-总结："><a href="#HIVE-总结：" class="headerlink" title="HIVE 总结："></a>HIVE 总结：</h3><h4 id="内部表-管理表"><a href="#内部表-管理表" class="headerlink" title="内部表(管理表)"></a>内部表(管理表)</h4><pre><code>内部表也称之为 MANAGED_TABLE；默认存储在/user/hive/warehouse下，也可以通过location指定;
**重点**：删除表时也会删除元数据和数据本身
</code></pre><h4 id="外部表-托管表"><a href="#外部表-托管表" class="headerlink" title="外部表(托管表)"></a>外部表(托管表)</h4><pre><code>外部表也称之为 EXTERNAL_TABLE;在创建表时可以自己指定目录位置(location);
**重点**：删除表时只会删除元数据不会删除数据本身
</code></pre><h3 id="分区表："><a href="#分区表：" class="headerlink" title="分区表："></a>分区表：</h3><pre><code>分区表实际上就是对应一个hdfs文件系统上的独立的文件夹，该文件夹下是该分区的所有数据文件。
hive中的分区就是分目录，把大的数据集根据业务分割成更小的数据集。
**作用**：在查询时通过where子句中的表达式来选择查询所需要的指定分区，这样的查询效率会提高很高。
</code></pre><h5 id="分区表的修复："><a href="#分区表的修复：" class="headerlink" title="分区表的修复："></a>分区表的修复：</h5><pre><code>面试题：
    方法一[msck repair table emp]：加入创建了一个hive分区表，然后用hdfs的方式去-mkdir分区字段的目录，然后向这个目录下-put数据文件，然后去select,会显示表的内容为空，但是数据在的，这个时候查看元数据的partitions里面是没有这个分区字段的，此时可以用 **msck repair table emp** 命令来进行修复后就可以查到内容
    方法二[alter table emp add partition(day = 15)]
</code></pre><h3 id="hive中的高级查询："><a href="#hive中的高级查询：" class="headerlink" title="hive中的高级查询："></a>hive中的高级查询：</h3><pre><code>1. order by  ---&gt; 对全局数据的排序，仅仅只有一个reduce ***数据量比较大的时候慎用！！！
2. sort by ---&gt;对每一个reduce内部数据进行排序，对于全局结果集来说不是排序 使用前先设置reduce的个数：set mapreduce.job.reduces = 3
3. distribute by ---&gt; 作用就是分区(partition) ,类似于MapReduce中分区partition，对数据进行分区，结合sort by进行使用 如：select * from emp distribute by depno sort by empno asc;
    注意：distribute by 要放在 sort by 前面[先分区再排序]
4.cluster by ---&gt; 当distribute by 和 sort by字段相同时可以使用cluster by
</code></pre><h3 id="UDF【user-defined-function】"><a href="#UDF【user-defined-function】" class="headerlink" title="UDF【user defined function】:"></a>UDF【user defined function】:</h3><h6 id="hive自带了很多UDF，如：max、min、split，但是往往在开发中不能满足我们的也无需求"><a href="#hive自带了很多UDF，如：max、min、split，但是往往在开发中不能满足我们的也无需求" class="headerlink" title="hive自带了很多UDF，如：max、min、split，但是往往在开发中不能满足我们的也无需求"></a>hive自带了很多UDF，如：max、min、split，但是往往在开发中不能满足我们的也无需求</h6><pre><code>UDF 一进一出;         
UDAF(aggregation) 多进一出;  
UDTF(table-generating) 一进多出
</code></pre><p>UDF开发步骤：</p>
<pre><code>1.继承hive.ql.UDF类
2.实现一个或多个evaluate方法 即：重载
3.把程序打成jar包 
4. [jar包在本地]: 
   4.11 add jar /xxx/xxx.jar 
   4.12 create temporary function funcname as &quot;类的全路径&quot; 
   [jar包在hdfs] 
   4.2 create function funcname as &quot;类的全路径&quot; using jar &apos;hdfs://xxx/xx.jar&apos;
</code></pre><p>UDF开发注意事项：</p>
<pre><code>1.UDF必须要有返回值，不能为void，且可以返回null
2.UDF中常用Text/LongWritable等类型，不推荐使用java类型，因为hive底层是用MapReduce，而mr的实现里面都是他自己独有的可序列化类型
</code></pre><h2 id="Hive的优化："><a href="#Hive的优化：" class="headerlink" title="Hive的优化："></a>Hive的优化：</h2><h4 id="优化1-数据压缩-使用snappy格式-snappy是谷歌开源的"><a href="#优化1-数据压缩-使用snappy格式-snappy是谷歌开源的" class="headerlink" title="优化1.数据压缩 [使用snappy格式]    **snappy是谷歌开源的"></a>优化1.数据压缩 [使用snappy格式]    **snappy是谷歌开源的</h4><pre><code>作用：数据量小，减少网络IO流
hive底层还是走的MapReduce，也就是压缩mr阶段的数据；
具体--&gt; 1.1 CompressedInput上传之前进行压缩;  然后客户端进行InputSplit分片，这个阶段不需要配置，因为如果是压缩后的snappy文件，map会自动进行解压
       1.2map接收数据后进行DecompressInput[解压缩]对数据进行处理
       1.3 map阶段SpillToDisk的时候进行压缩 
               配置：mapreduce.map.output.compress=true;
    mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.Defaul tCodec
       1.4 reduce 来map的disk取数据后进行解压缩
       1.5 reduce 进行数据汇总 然后 进行压缩 输出[CompressReduceOutput]
       配置：mapreduce.output.fileoutputformat.compress=true;
    mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.Defaul tCodec
</code></pre><h4 id="优化2-数据的存储格式【storage-format】：列式存储的Parquet是Apache的顶级项目"><a href="#优化2-数据的存储格式【storage-format】：列式存储的Parquet是Apache的顶级项目" class="headerlink" title="优化2.数据的存储格式【storage format】：列式存储的Parquet是Apache的顶级项目"></a>优化2.数据的存储格式【storage format】：列式存储的Parquet是Apache的顶级项目</h4><h6 id="我们一般select-name-age-from-table-group-by-xx-都是查询的列，如果读取文件时按照列来读取，那么效率会高很多"><a href="#我们一般select-name-age-from-table-group-by-xx-都是查询的列，如果读取文件时按照列来读取，那么效率会高很多" class="headerlink" title="我们一般select name,age from table group by xx..都是查询的列，如果读取文件时按照列来读取，那么效率会高很多"></a>我们一般select name,age from table group by xx..都是查询的列，如果读取文件时按照列来读取，那么效率会高很多</h6><pre><code>SequenceFile、TextFile：按行存储数据
ParquetFile、ORCFile ：按列存储数据 【ORC里面是 strip index，索引查找】
**重要**测试显示存储在hdfs上的相同文件 create table name(...) stored as orc;和 create...stored as textfile;文件大小比例：orc : text = 1:7 大大的减少了存储的数据大小
小结：1.orc和parquet的存储格式使得存储相同的数据占得空间变小，尤其是orc
     2.而列式存储相对于text、sequence的行式存储查询也是很占优势的
</code></pre><h5 id="以上两点：压缩-数据存储格式-两者结合效果更好"><a href="#以上两点：压缩-数据存储格式-两者结合效果更好" class="headerlink" title="以上两点：压缩+数据存储格式 两者结合效果更好"></a>以上两点：压缩+数据存储格式 两者结合效果更好</h5><pre><code>create table page_views_orc_snappy(...) row format ... stored as orc tblproperties (&apos;orc.compress&apos;=&apos;SNAPPY&apos;);
insert into table page_views_orc_snappy select * from page_views;
//注释：这个插入语句会执行三个job: 查询page_views + 转化为orc格式 + snappy compress[压缩]
</code></pre><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><pre><code>在实际的项目开发中，hive表的数据
    *存储格式
        orcfile / parquet
    *数据压缩
        snappy
</code></pre><h4 id="优化3-Fetch-Task-值改为-more"><a href="#优化3-Fetch-Task-值改为-more" class="headerlink" title="优化3.Fetch-Task     值改为 more"></a>优化3.Fetch-Task     值改为 more</h4><pre><code>存在的问题分析：select * from t 不会走mr，select name from t 就会走mr，为什么？
因为：
    在hive-default.xml里面有个&lt;property&gt;是hive.fetch.task.conversion = minimal
[默认的--只有三种情况下不会走MapReduce：1.select * from t 2.select * from t where partitionname = xx 3.select * from t limit n]
我们可以在hive-site.xml里配置这个属性的值为 more 
</code></pre><h3 id="优化4-hive高级优化"><a href="#优化4-hive高级优化" class="headerlink" title="优化4.hive高级优化"></a>优化4.hive高级优化</h3><pre><code>1.大表拆成子表 
    create table table1... AS select * from table2
    比如订单表，都是几百列，当我们不同需求的时候只用一些字段的时候，就到大表创建的字表里取查询，这样就比较快
2.外部表 + 分区表   [因为有可能多个部门同时在用这些数据] + [一般二级分区表较多--月/日]
  create internal table name ... partitioned by (month string,day string) row format delimited...
3.数据存储格式 + 压缩   [orcfile/parquetfile   +   snappy]

    set parquet.compress = snappy;
    create table order_parquet_snappy(...) 
    row format ... 
    stored as parquet 
    as select * from order;
4.SQL语句的优化
</code></pre><h3 id="优化5-数据倾斜问题："><a href="#优化5-数据倾斜问题：" class="headerlink" title="优化5.数据倾斜问题："></a>优化5.数据倾斜问题：</h3><pre><code>Join:
    1.Shuffle/Reduce/Commen Join
        连接发生的阶段是 Reduce Task
        一般是大表对大表
        每个表的数据都是从文件中读取的
    2.Map Join
        连接发生在 Map Task
        一般是小表对大表
        大表的数据从文件中读取，小表的数据从内存中读取
        用到了 DistributedCache，把小表缓存到了个个节点的内存中
    3.SMB Join   [Sort + Merge + Bucket]
        一般在大公司里会用到，其实就是对reduce join的优化，因为两个大表都特别大，那么会吃不消的
        set hive.enforce.bucketing = true;
        set mapreduce.job.reduces = 4  [举例]
        create table tablename(...) clustered by (cid) into 4 buckets row format ...
        load data inpath &apos;xxx&apos; into ...实质上是 hdfs dfs -put xxx 不会走MapReduce的
        所以插入分桶表bucket需要用 insert into table table_bucket select * from xxx cluster by (cid),这样才会走mr
    4.group by 和 count(distinct)容易造成数据倾斜 //todo:待解决
</code></pre><p>hdfs dfs -du -h /root/xxx 是看目录或者文件有多大</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-Markdown 编辑阅读器" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/02/Markdown 编辑阅读器/" class="article-date">
  	<time datetime="2018-05-02T13:29:30.000Z" itemprop="datePublished">2018-05-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/02/Markdown 编辑阅读器/">
        Markdown 编辑阅读器
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="欢迎使用-Cmd-Markdown-编辑阅读器"><a href="#欢迎使用-Cmd-Markdown-编辑阅读器" class="headerlink" title="欢迎使用 Cmd Markdown 编辑阅读器"></a>欢迎使用 Cmd Markdown 编辑阅读器</h1><hr>
<h1 id="nihao"><a href="#nihao" class="headerlink" title="nihao"></a>nihao</h1><h2 id="nihao-1"><a href="#nihao-1" class="headerlink" title="nihao"></a>nihao</h2><h3 id="nihao-2"><a href="#nihao-2" class="headerlink" title="nihao"></a>nihao</h3><h1>232</h1>

<blockquote>
<p>nidshfisdfjksdfjksdf<br>dsfjaksdfj df<br><strong>&amp;dfsdf</strong><br>nihao</p>
</blockquote>
<p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1525278572217&amp;di=902ca4f761f90828017346ac0a110f2d&amp;imgtype=0&amp;src=http%3A%2F%2Fr4.ykimg.com%2F0541040852660CC56A0A471F0F5B03E1" alt=""></p>
<p><a href="https://imgchr.com/i/CY82sH" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2018/05/02/CY82sH.md.png" alt="CY82sH.md.png"></a></p>
<p>我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，<strong>Cmd Markdown</strong> 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown：</p>
<blockquote>
<ul>
<li>整理知识，学习笔记</li>
<li>发布日记，杂文，所见所想</li>
<li>撰写发布技术文稿（代码支持）</li>
<li>撰写发布学术论文（LaTeX 公式支持）</li>
</ul>
</blockquote>
<p><img src="https://www.zybuluo.com/static/img/logo.png" alt="cmd-markdown-logo"></p>
<p>除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载：</p>
<h3 id="Windows-Mac-Linux-全平台客户端"><a href="#Windows-Mac-Linux-全平台客户端" class="headerlink" title="Windows/Mac/Linux 全平台客户端"></a><a href="https://www.zybuluo.com/cmd/" target="_blank" rel="noopener">Windows/Mac/Linux 全平台客户端</a></h3><blockquote>
<p>请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 <i class="icon-file"></i> <strong>新文稿</strong> 或者使用快捷键 <code>Ctrl+Alt+N</code>。</p>
</blockquote>
<hr>
<h2 id="什么是-Markdown"><a href="#什么是-Markdown" class="headerlink" title="什么是 Markdown"></a>什么是 Markdown</h2><p>Markdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，<strong>粗体</strong> 或者 <em>斜体</em> 某些文字，更棒的是，它还可以</p>
<h3 id="1-制作一份待办事宜-Todo-列表"><a href="#1-制作一份待办事宜-Todo-列表" class="headerlink" title="1. 制作一份待办事宜 Todo 列表"></a>1. 制作一份待办事宜 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#13-待办事宜-todo-列表" target="_blank" rel="noopener">Todo 列表</a></h3><ul>
<li style="list-style: none"><input type="checkbox"> 支持以 PDF 格式导出文稿</li>
<li style="list-style: none"><input type="checkbox"> 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率</li>
<li style="list-style: none"><input type="checkbox" checked> 新增 Todo 列表功能</li>
<li style="list-style: none"><input type="checkbox" checked> 修复 LaTex 公式渲染问题</li>
<li style="list-style: none"><input type="checkbox" checked> 新增 LaTex 公式编号功能</li>
</ul>
<h3 id="2-书写一个质能守恒公式-LaTeX"><a href="#2-书写一个质能守恒公式-LaTeX" class="headerlink" title="2. 书写一个质能守恒公式[^LaTeX]"></a>2. 书写一个质能守恒公式[^LaTeX]</h3><p>$$E=mc^2$$</p>
<h3 id="3-高亮一段代码-code"><a href="#3-高亮一段代码-code" class="headerlink" title="3. 高亮一段代码[^code]"></a>3. 高亮一段代码[^code]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@requires_authorization</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SomeClass</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># A comment</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'hello world'</span></span><br></pre></td></tr></table></figure>
<h3 id="4-高效绘制-流程图"><a href="#4-高效绘制-流程图" class="headerlink" title="4. 高效绘制 流程图"></a>4. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#7-流程图" target="_blank" rel="noopener">流程图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">st=&gt;start: Start</span><br><span class="line">op=&gt;operation: Your Operation</span><br><span class="line">cond=&gt;condition: Yes or No?</span><br><span class="line">e=&gt;end</span><br><span class="line"></span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br></pre></td></tr></table></figure>
<h3 id="5-高效绘制-序列图"><a href="#5-高效绘制-序列图" class="headerlink" title="5. 高效绘制 序列图"></a>5. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#8-序列图" target="_blank" rel="noopener">序列图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Alice-&gt;Bob: Hello Bob, how are you?</span><br><span class="line">Note right of Bob: Bob thinks</span><br><span class="line">Bob--&gt;Alice: I am good thanks!</span><br></pre></td></tr></table></figure>
<h3 id="6-高效绘制-甘特图"><a href="#6-高效绘制-甘特图" class="headerlink" title="6. 高效绘制 甘特图"></a>6. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#9-甘特图" target="_blank" rel="noopener">甘特图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">title 项目开发流程</span><br><span class="line">section 项目确定</span><br><span class="line">    需求分析       :a1, 2016-06-22, 3d</span><br><span class="line">    可行性报告     :after a1, 5d</span><br><span class="line">    概念验证       : 5d</span><br><span class="line">section 项目实施</span><br><span class="line">    概要设计      :2016-07-05  , 5d</span><br><span class="line">    详细设计      :2016-07-08, 10d</span><br><span class="line">    编码          :2016-07-15, 10d</span><br><span class="line">    测试          :2016-07-22, 5d</span><br><span class="line">section 发布验收</span><br><span class="line">    发布: 2d</span><br><span class="line">    验收: 3d</span><br></pre></td></tr></table></figure>
<h3 id="7-绘制表格"><a href="#7-绘制表格" class="headerlink" title="7. 绘制表格"></a>7. 绘制表格</h3><table>
<thead>
<tr>
<th>项目</th>
<th style="text-align:right">价格</th>
<th style="text-align:center">数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>计算机</td>
<td style="text-align:right">\$1600</td>
<td style="text-align:center">5</td>
</tr>
<tr>
<td>手机</td>
<td style="text-align:right">\$12</td>
<td style="text-align:center">12</td>
</tr>
<tr>
<td>管线</td>
<td style="text-align:right">\$1</td>
<td style="text-align:center">234</td>
</tr>
</tbody>
</table>
<h3 id="8-更详细语法说明"><a href="#8-更详细语法说明" class="headerlink" title="8. 更详细语法说明"></a>8. 更详细语法说明</h3><p>想要查看更详细的语法说明，可以参考我们准备的 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown" target="_blank" rel="noopener">Cmd Markdown 简明语法手册</a>，进阶用户可以参考 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#cmd-markdown-高阶语法手册" target="_blank" rel="noopener">Cmd Markdown 高阶语法手册</a> 了解更多高级功能。</p>
<p>总而言之，不同于其它 <em>所见即所得</em> 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。<strong>Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。</strong> 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。</p>
<hr>
<h2 id="什么是-Cmd-Markdown"><a href="#什么是-Cmd-Markdown" class="headerlink" title="什么是 Cmd Markdown"></a>什么是 Cmd Markdown</h2><p>您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 <strong>编辑/发布/阅读</strong> Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。</p>
<h3 id="1-实时同步预览"><a href="#1-实时同步预览" class="headerlink" title="1. 实时同步预览"></a>1. 实时同步预览</h3><p>我们将 Cmd Markdown 的主界面一分为二，左边为<strong>编辑区</strong>，右边为<strong>预览区</strong>，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！</p>
<h3 id="2-编辑工具栏"><a href="#2-编辑工具栏" class="headerlink" title="2. 编辑工具栏"></a>2. 编辑工具栏</h3><p>也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 <strong>编辑区</strong> 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。</p>
<p><img src="https://www.zybuluo.com/static/img/toolbar-editor.png" alt="tool-editor"></p>
<h3 id="3-编辑模式"><a href="#3-编辑模式" class="headerlink" title="3. 编辑模式"></a>3. 编辑模式</h3><p>完全心无旁骛的方式编辑文字：点击 <strong>编辑工具栏</strong> 最右侧的拉伸按钮或者按下 <code>Ctrl + M</code>，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！</p>
<h3 id="4-实时的云端文稿"><a href="#4-实时的云端文稿" class="headerlink" title="4. 实时的云端文稿"></a>4. 实时的云端文稿</h3><p>为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 <strong>编辑工具栏</strong> 的最右侧提示 <code>已保存</code> 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。</p>
<h3 id="5-离线模式"><a href="#5-离线模式" class="headerlink" title="5. 离线模式"></a>5. 离线模式</h3><p>在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。</p>
<h3 id="6-管理工具栏"><a href="#6-管理工具栏" class="headerlink" title="6. 管理工具栏"></a>6. 管理工具栏</h3><p>为了便于管理您的文稿，在 <strong>预览区</strong> 的顶部放置了如下所示的 <strong>管理工具栏</strong>：</p>
<p><img src="https://www.zybuluo.com/static/img/toolbar-manager.jpg" alt="tool-manager"></p>
<p>通过管理工具栏可以：</p>
<p><i class="icon-share"></i> 发布：将当前的文稿生成固定链接，在网络上发布，分享<br><i class="icon-file"></i> 新建：开始撰写一篇新的文稿<br><i class="icon-trash"></i> 删除：删除当前的文稿<br><i class="icon-cloud"></i> 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地<br><i class="icon-reorder"></i> 列表：所有新增和过往的文稿都可以在这里查看、操作<br><i class="icon-pencil"></i> 模式：切换 普通/Vim/Emacs 编辑模式</p>
<h3 id="7-阅读工具栏"><a href="#7-阅读工具栏" class="headerlink" title="7. 阅读工具栏"></a>7. 阅读工具栏</h3><p><img src="https://www.zybuluo.com/static/img/toolbar-reader.jpg" alt="tool-manager"></p>
<p>通过 <strong>预览区</strong> 右上角的 <strong>阅读工具栏</strong>，可以查看当前文稿的目录并增强阅读体验。</p>
<p>工具栏上的五个图标依次为：</p>
<p><i class="icon-list"></i> 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落<br><i class="icon-chevron-sign-left"></i> 视图：互换左边编辑区和右边预览区的位置<br><i class="icon-adjust"></i> 主题：内置了黑白两种模式的主题，试试 <strong>黑色主题</strong>，超炫！<br><i class="icon-desktop"></i> 阅读：心无旁骛的阅读模式提供超一流的阅读体验<br><i class="icon-fullscreen"></i> 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境</p>
<h3 id="8-阅读模式"><a href="#8-阅读模式" class="headerlink" title="8. 阅读模式"></a>8. 阅读模式</h3><p>在 <strong>阅读工具栏</strong> 点击 <i class="icon-desktop"></i> 或者按下 <code>Ctrl+Alt+M</code> 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。</p>
<h3 id="9-标签、分类和搜索"><a href="#9-标签、分类和搜索" class="headerlink" title="9. 标签、分类和搜索"></a>9. 标签、分类和搜索</h3><p>在编辑区任意行首位置输入以下格式的文字可以标签当前文档：</p>
<p>标签： 未分类</p>
<p>标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示：</p>
<p><img src="https://www.zybuluo.com/static/img/file-list.png" alt="file-list"></p>
<h3 id="10-文稿发布和分享"><a href="#10-文稿发布和分享" class="headerlink" title="10. 文稿发布和分享"></a>10. 文稿发布和分享</h3><p>在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 <i class="icon-share"></i> (Ctrl+Alt+P) 发布这份文档给好友吧！</p>
<hr>
<p>再一次感谢您花费时间阅读这份欢迎稿，点击 <i class="icon-file"></i> (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！</p>
<p>作者 <a href="http://weibo.com/ghosert" target="_blank" rel="noopener">@ghosert</a><br>2016 年 07月 07日    </p>
<p>[^LaTeX]: 支持 <strong>LaTeX</strong> 编辑显示支持，例如：$\sum_{i=1}^n a_i=0$， 访问 <a href="http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener">MathJax</a> 参考更多使用方法。</p>
<p>[^code]: 代码高亮功能支持包括 Java, Python, JavaScript 在内的，<strong>四十一</strong>种主流编程语言。</p>
<h1>nihao</h1>


      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-随笔" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/02/随笔/" class="article-date">
  	<time datetime="2018-05-02T13:29:30.000Z" itemprop="datePublished">2018-05-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/02/随笔/">
        随笔
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>复习知识点：<br>spar全部知识点，kafka，hdfs，hive及预处理，azkaban，hbase（会搭集群，roykey，会设计建表，有哪些特性）,redis，zookeeper java基础(罗海清脑图复习)</p>
<p>在hive中:<br>select * from 表名：<br>是查询该表名的所有字段记录</p>
<p>describe formatted 表名：<br>是查看该表的详细信息，而并查看不了表中的数据</p>
<p>十大经典数据挖掘算法之一:<br>kmeans<br>    7、cos余弦相似度和欧式距离的区</p>
<p>solid converter PDF软件下载破解<br>Name: SolidConverterPDFv9<br>E-mail: <a href="mailto:user@ru.ru" target="_blank" rel="noopener">user@ru.ru</a><br>Organization: any<br>Unlock Code: KTGK</p>
<p>HTTP 协议中 URI 和 URL 有什么区别？<br>统一资源标志符URI就是在某一规则下能把一个资源独一无二地标识出来<br>统一资源定位符URL同样标识出了唯一的一个人，起到了URI的作用，所以URL是URI的子集<br>可以用身份证号是uri(包含了url)可以确定一个人,而地址也可以确定一个人</p>
<p>本质而言Kylin麒麟系统就是Ubuntu 13.04</p>
<p>艾维奇电音大师:<br>代表之作:”wake me up” “levels” “x you” “a sky full of stars” “lay me down”</p>
<p>查看某个后台进程:<br>ps aux | grep redis</p>
<p>查看所有正在使用的端口：<br>netstat -ntlp</p>
<p>idea破解教程：<a href="http://blog.csdn.net/qq_38637558/article/details/78914772" target="_blank" rel="noopener">http://blog.csdn.net/qq_38637558/article/details/78914772</a></p>
<p><a href="http://tlias-stu.boxuegu.com/#/index" target="_blank" rel="noopener">http://tlias-stu.boxuegu.com/#/index</a><br>博学谷</p>
<p>mkdir -p /export/server<br>rm -rf jdk-8u65-linux-x64.tar.gz<br>mv zookeeper-3.4.5 zookeeper</p>
<p>js自调用匿名函数：<br>(function(){})();</p>
<p>shell命令：<br>-p 表示递归<br>-f 表示覆盖原有文件目录<br>-w 表示写的命令</p>
<p>有时间研究一下matlab<br>脱敏</p>
<p>有时间研究一下在简书，csdn,51cto,主要是github和脸书发表代码和作品的流程，还有有时间玩下阿里，腾讯的服务器，买个域名练练手熟悉一下。</p>
<p>菜鸟教程<br><a href="http://www.runoob.com/linux/linux-tutorial.html" target="_blank" rel="noopener">http://www.runoob.com/linux/linux-tutorial.html</a><br><a href="http://www.runoob.com/mysql/mysql-tutorial.html" target="_blank" rel="noopener">http://www.runoob.com/mysql/mysql-tutorial.html</a></p>
<p><a href="https://www.csdn.net/nav/cloud" target="_blank" rel="noopener">https://www.csdn.net/nav/cloud</a><br><a href="http://blog.csdn.net/superzyl" target="_blank" rel="noopener">http://blog.csdn.net/superzyl</a></p>
<p>在线json生成java实体类<br><a href="https://www.bejson.com/" target="_blank" rel="noopener">https://www.bejson.com/</a></p>
<p><a href="https://www.cnblogs.com/aipan/p/7770611.html" target="_blank" rel="noopener">https://www.cnblogs.com/aipan/p/7770611.html</a></p>
<p>启用hadoop本地模式：<br>在hive中设置（常用）<br>set hive.exec.mode.local.auto=true;</p>
<p>boss直聘、<br>拉钩</p>
<p>解决面试题中的脑筋急转弯<br>小袁搜题<br>作业帮</p>
<p>自古评论出奇才，<br>内涵佳句随口来。<br>若是生在隋唐代，<br>哪有诗仙李太白？</p>
<p>内事问百度，外事问谷歌，床事问天涯，绿事问虎扑”</p>
<p>C:\Windows\System32\drivers\etc  hosts目录</p>
<p>31773766<br>31773767</p>
<p><a href="http://blog.csdn.net/superzyl" target="_blank" rel="noopener">http://blog.csdn.net/superzyl</a><br>周老师的CSDN博客，里面有老师总结的50列sql面试题，面试前做一做</p>
<p>hourenren 13:52:23<br>在定义变量的时候是引用,如int &amp;a = b; a为b的一个引用<br>在表达式中为取地址如int *a = &b; a位指向b整型的一个指针<br>hourenren 14:15:41<br><a href="https://ideone.com/4okUHi" target="_blank" rel="noopener">https://ideone.com/4okUHi</a><br>hourenren 14:16:07<br>在线编译C++代码</p>
<p>八爪鱼爬虫</p>
<p>datagrip 智能sql编辑器</p>
<p><a href="https://www.cnblogs.com/zhangyinhua/p/8037599.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhangyinhua/p/8037599.html</a><br>jsoup文档讲解</p>
<p><a href="https://www.boxuegu.com/course/free/" target="_blank" rel="noopener">https://www.boxuegu.com/course/free/</a><br>redis免费学习面试热点</p>
<p><a href="https://www.cnblogs.com/lizichao1991/p/7809156.html" target="_blank" rel="noopener">https://www.cnblogs.com/lizichao1991/p/7809156.html</a> </p>
<p><a href="http://blog.csdn.net/yao970953039/article/details/62047755" target="_blank" rel="noopener">http://blog.csdn.net/yao970953039/article/details/62047755</a></p>
<p> Intellij IDEA 2017 debug断点调试技巧与总结详解篇<br><a href="http://blog.csdn.net/qq_27093465/article/details/64124330" target="_blank" rel="noopener">http://blog.csdn.net/qq_27093465/article/details/64124330</a></p>
<p>import scala.collection.mutable.Map<br>scala导包</p>
<p><a href="https://www.iteblog.com/archives/1542.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/1542.html</a><br>定时在线激活idea</p>
<p>在线pdf转word网站<br><a href="http://app.xunjiepdf.com/pdf2word" target="_blank" rel="noopener">http://app.xunjiepdf.com/pdf2word</a></p>
<p>乔布简历,一个挺不错的简历样式模板网站</p>
<p>快递单号：<br>0491 2969 6061</p>
<p>某个zookeeper挂掉了，解决方案：<br>cd /export/data/zkdata<br>rm -rf v<em> zoo</em><br>再重新启动zookeeper即可</p>
<p>linux中\转义空格<br>///两个//转义/，目录</p>
<p>农历2月12号 爸爸<br>农历4月初九 妈妈<br>农历四月一号  姐姐</p>
<p>13787692647</p>
<p><a href="http://my.tv.sohu.com/us/254995980/79868928.shtml" target="_blank" rel="noopener">http://my.tv.sohu.com/us/254995980/79868928.shtml</a><br>hadoop年薪23万学员分享面试经验</p>
<p>万里面试<br>kafka 和spark 都会问的很多<br>数据库ods 层数据怎么清楚？</p>
<p>面试常见问题：<br>链接：<a href="https://pan.baidu.com/s/1hIWUx01oOcilf2O_p_Fg3Q?密码：7z7s" target="_blank" rel="noopener">https://pan.baidu.com/s/1hIWUx01oOcilf2O_p_Fg3Q?密码：7z7s</a></p>
<p>搭建外网服务器网站：<br><a href="https://www.vultr.com/?ref=7323908" target="_blank" rel="noopener">https://www.vultr.com/?ref=7323908</a><br>用crt连上后，依次输入以下三条命令：</p>
<p>wget –no-check-certificate -O shadowsocks-all.sh <a href="https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh" target="_blank" rel="noopener">https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh</a></p>
<p>chmod +x shadowsocks-all.sh</p>
<p>./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log</p>
<p>图像识别物体<br><a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet</a></p>
<p>leetcode 刷题网站</p>
<p>l kafka删除 topic<br>bin/kafka-topics.sh –delete –zookeeper zk01:2181 –topic test<br>需要 server.properties 中设置 delete.topic.enable=true 否则只是标记删除或者直接重启。</p>
<p>你可以通过命令：./bin/kafka-topics –zookeeper 【zookeeper server】  –list 来查看所有topic</p>
<p> C:\Users\RongYue.jupyter\jupyter_notebook_config.py</p>
<p> kill和kill -9 和区别（原理）<br> 有时候我们使用kill无法杀掉一个进程，但是用kill -9却可以，kill的作用是向进程发送一个信号（并没有说是杀掉进程哈）。具体发送什么信号由后面接的参数决定。<br> kill默认参数是TERM。也就是说，如果没指定具体的信号作为参数，则默认使用kill TERM pid。因此kill pid是可以杀掉一个进程<br> 大多数信号可以被捕获的。而TERM信号就是在这个大多数里的，一些进程可能为了特殊的用途捕获了TERM信号，导致了你使用kill pid时无法杀掉进程。 另外《APUE》中也强调了，有两个信号不能被捕获，SIGKILL 和SIGSTOP<br> 没错，kill -9 就是向进程发送SIGKILL信号</p>
<p>spark 的几种运行模式<br>spark的yarn管理资源不够用了怎么办<br>azkaban任务提交我只修改任务里的一部分，如何避免每次都上传zip包<br>shuffle的排序算法 归并排序<br>azkaban调度失败了该怎么检查和恢复，dug的思路<br>spark运行内存不够会发生什么，如何解决<br>你们集群中hdfs和yarn的使用率是多少<br>hive的调优，数据量大的情况和不仅仅数据倾斜</p>
<p>有没有做过cdh的升级和改造，cdh的版本<br>集群部署规划情况，内存，大小，磁盘，<br>工作中接触的数据量大小，我答500–1g</p>
<p>思维题，1亿个用户每个用户都有一个动态的打分值(1–100)，不用排序，如果快速知道排名前十的用户是谁</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-数据倾斜解决方案" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/02/数据倾斜解决方案/" class="article-date">
  	<time datetime="2018-05-02T13:29:30.000Z" itemprop="datePublished">2018-05-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/02/数据倾斜解决方案/">
        Hive 数据倾斜解决方案
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Hive-数据倾斜解决方案："><a href="#Hive-数据倾斜解决方案：" class="headerlink" title="Hive 数据倾斜解决方案："></a>Hive 数据倾斜解决方案：</h4><pre><code>1.调节参数
    hive.map.aggr=true  Map端部分聚合，相当于combiner
    hive.groupby.skewindata=true
        有数据倾斜的时候进行负载均衡，当选项设定为true，生成的查询计划会有两个mr job，
        第一个job中，map的输出结果集合会随机分不到reduce中，每个reduce做部分聚合操作，
        并输出结果，这样的处理结果是相同的groupByKey会分到不同的reduce中，从而达到敷在君更的目的；
        第二个job再根据预处理的数据结果按照GroupByKey分不到redcue，最终完成聚合
2. 小表和大表进行join操作
    使用map join让小的维度表(1000条以下的记录数)先进内存[distributedCache],在map端完成reduce

3.1 空值产生的数据倾斜
    赋予空值新的key值
        select *
        from log a
        left join users b
        on
        case when 
                a.user_id is null 
                then 
                concat(&apos;hive&apos;,rand())
                else a.user_id
                end = b.user_id
    好处：这个方法只有一个job，把空值的key变成了字符串加上随机数，就能把倾斜的数据分到不同的reduce上，
    解决数据倾斜问题。
3.2 不同数据类型关联产生数据倾斜
    比如 用户表user_id为int类型，log表中的user_id 字段既有int也有string类型，
    当按照user_id进行两个表的join操作时，默认的hash操作会按照int类型的id来进行分配，
    这样就会导致所有string类型的id记录都分配到一个reducer中
    解决：把数字类型转换成字符串类型
        select * 
        from 
            users a
        left outer join logs b
        on
        a.user_id = cast(b.user_id as string)
3.3 users表有600w+的记录，把users分发到所有的map也是不小的开销，而且map join不支持这么大的小表，
如果用普通的join，又会碰到数据倾斜的问题
    解决：
        select * from log a
            left outer join 
                (
                    select d.* from (select distinct user_id from log) c
                    join users d
                    on c.user_id = d.user_id
                ) x
            on a.user_id = x.user_id;
</code></pre><h4 id="spark解决数据倾斜："><a href="#spark解决数据倾斜：" class="headerlink" title="spark解决数据倾斜："></a>spark解决数据倾斜：</h4><pre><code>1.增加并行度，也就是增加task的个数，可以缓解数据倾斜 这样可以将分配到统一task上的key散开，
2.自定义 partition 默认是 HashPartition，这样可以将不同的key分配到多个task上，
但是也只是缓解，而且也不灵活，不能解决同一个key数据量很好的场景
3.将reduce端join变成map端join -- broadcast 
    优势：
        避免了shuffle，彻底解除了数据倾斜
    劣势：
        要求join的一侧数据集合足够小，适用于join，不适用于聚合
4.在数据倾斜的key前面加前缀，让这个key分不到不同的task中
    将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，
    另外一个RDD每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N    倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。
    最后将两次Join的结果集通过union合并，即可得到全部Join结果。
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kafka - scala 语言写的 版本 1.0.0 scala 2.11 官方推荐" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/kafka - scala 语言写的 版本 1.0.0 scala 2.11 官方推荐/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/kafka - scala 语言写的 版本 1.0.0 scala 2.11 官方推荐/">
        kafka总结相关笔记
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="kafka-scala-语言写的-版本-1-0-0-scala-2-11-官方推荐"><a href="#kafka-scala-语言写的-版本-1-0-0-scala-2-11-官方推荐" class="headerlink" title="kafka - scala 语言写的 版本 1.0.0 scala 2.11 官方推荐"></a>kafka - scala 语言写的 版本 1.0.0 scala 2.11 官方推荐</h1><p>kafka 是什么？</p>
<pre><code>1.kafka是一个消息队列(生产者消费者模式)
2.目标：构建企业中统一的、高通量、低延时的消息平台
3.大多的是消息队列（消息中间件）都是基于JMS标准实现的，Kafka类似于JMS的实现
</code></pre><p>kafka 有什么用？(消息队列有什么用？)</p>
<pre><code>作为缓冲，来异构、解耦系统
    a. 用户注册需要多个步骤，每个步骤执行都需要很长时间，代表用户等待时间是所有步骤的累计时间
    b. 为了减少用户等待的时间，使用并行执行，有多少步骤，就开启多少个线程来执行
        代表用户等待时间是所有步骤中耗时最多的那个步骤时间
    c. 问题：开启多个线程执行每个步骤，如果以一个步骤执行异常，或者严重超时，
        用户的等待时间就不可控了
使用消息队列来保证
    1.注册时，立刻返回成功
    2.发送注册成功的消息到消息平台
    3.对注册信息感兴趣的程序，可以消费消息
</code></pre><h3 id="kafka的基本架构"><a href="#kafka的基本架构" class="headerlink" title="kafka的基本架构"></a>kafka的基本架构</h3><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/x.psMta.aW44s9upJo9IrOikdwcFORL9vPWx0WhaZgA!/b/dFYBAAAAAAAA&amp;bo=CgMQAgAAAAARBys!&amp;rf=viewer_4" alt=""><br>    kafka cluster：由多个服务器组成，每个服务器单独的名字broker(server)<br>    kafka producer：生产者、负责生产数据<br>    kafka consumer：消费者、负责消费数据<br>    kafka topic：主题，一类消息的名称。存储数据时将一类数据存放在某个topic下，消费数据也是消费一类数据<br>        订单系统：创建一个topic，叫做order<br>        用户系统：创建一个topic，叫做user<br>        商品系统：创建一个topic，叫做product<br><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/lmVbiS8e*xd5wkNzFNMQTFPDCKYrpGIGlMDPkcySlww!/b/dDEBAAAAAAAA&amp;bo=PgPcAAAAAAARB9E!&amp;rf=viewer_4" alt=""></p>
<h5 id="配置kafka需要修改配置文件的三个地方："><a href="#配置kafka需要修改配置文件的三个地方：" class="headerlink" title="配置kafka需要修改配置文件的三个地方："></a>配置kafka需要修改配置文件的三个地方：</h5><pre><code>1.broker.id
2.数据存放的目录，注意：目录如果不存在，需要新建
3.zookeeper的地址信息
</code></pre><h5 id="查看kafka集群"><a href="#查看kafka集群" class="headerlink" title="查看kafka集群"></a>查看kafka集群</h5><pre><code>由于kafka集群没有UI界面，需要借助外部工具，来查看kafka的集群
这个工具是一个java程序，必须要安装好jdk --- ZooInspector
</code></pre><h5 id="1-创建一个订单的topic。"><a href="#1-创建一个订单的topic。" class="headerlink" title="1) 创建一个订单的topic。"></a>1) 创建一个订单的topic。</h5><pre><code>bin/kafka-topics.sh --create --zookeeper zk01:2181 --replication-factor 1 --partitions 1 --topic order
</code></pre><h5 id="2）编写代码启动一个生产者，生产数据"><a href="#2）编写代码启动一个生产者，生产数据" class="headerlink" title="2）编写代码启动一个生产者，生产数据"></a>2）编写代码启动一个生产者，生产数据</h5><pre><code>bin/kafka-console-producer.sh --broker-list kafka01:9092 --topic order
</code></pre><h5 id="3）-编写代码启动给一个消费者，消费数据"><a href="#3）-编写代码启动给一个消费者，消费数据" class="headerlink" title="3）    编写代码启动给一个消费者，消费数据"></a>3）    编写代码启动给一个消费者，消费数据</h5><pre><code>bin/kafka-console-consumer.sh --zookeeper zk01:2181 --from-beginning --topic order
</code></pre><h3 id="kafka原理"><a href="#kafka原理" class="headerlink" title="kafka原理"></a>kafka原理</h3><h4 id="1-分片与副本机制"><a href="#1-分片与副本机制" class="headerlink" title="1.分片与副本机制"></a>1.分片与副本机制</h4><pre><code>分片：
    当数据量非常大的时候，一个服务器存放不了，就将数据分成两个或者多个部分，
    存放在多台服务器上。每个服务器上的数据，叫做一个分片。
        问题：
            如果一个partition中有10T数据，如何存放？是放在一个文件还是多个文件？
                kafka的解决方案是多个文件！
                这里说的多个文件就是segment段
                    [我们在kafka配置文件中配置的数据存储目录：/export/data/kafka]
                    里面有topicname-index [如：order-0,意思是order这个topic的第0个副本]
                    这个order-0目录下就是存放着诸如：
                        00000000000000000.index
                        00000000000000000.log
                        segment段包含了这两个文件，segment段默认是1G大小
                        segment段中有两个核心的文件.log和.index，当log文件等于1G的时候，
                        新的数据会写入到下个segment中，同时我们也可以看到segment段中有.timeindex文件生成,
                        而且通过查看，可以看到一个segment段差不多会存储70万条数据。
</code></pre><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/hn5PDC6zOwPd68flbGfMrNXsek.8DkiFksRzxPIWngA!/b/dEEBAAAAAAAA&amp;bo=hAMCAgAAAAARB7c!&amp;rf=viewer_4" alt=""><br>                    如上图所述：</p>
<pre><code>                *Segment文件命名规则：
                    partition全局的第一个segment从0开始，后续每个sgment文件名为上一个segment文件最后一条消息的offset值。
                    数值最大的为64位long大小，19位数字字符长度，没有数字用0填充。
                *索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中
                message的物理偏移地址
                结论：
                    kafka查找segment file 只需要两步
                        1.先找到这个数据对应的segment[通过查看segment的区间，offset在哪个segment段中]
                        2.根据某个segment段中的.index 索引文件中查找该条数据所在.log文件中的位置
        kafka为什么要对文件进行切分，保存多个文件中？
            kafka作为消息中间件，只负责消息的临时存储，并不是永久存储，
            需要删除过期的数据。
            如果将所有的数据都存放在一个文件中，要删除过期数据的时候，就麻烦了。
            因为文件有日期属性，删除过期数据，只需要根据文件的日属性删除就好了

副本：
    当数据只保存一份的时候，有丢失的风险，为了更好的容错和容灾，
    将数据拷贝几份，保存到不同的机器上。
</code></pre><h4 id="kafka生产数据的分发策略"><a href="#kafka生产数据的分发策略" class="headerlink" title="kafka生产数据的分发策略"></a>kafka生产数据的分发策略</h4><pre><code>kafka在生产数据的实惠，有一个数据分发策略。默认的情况使用DefaultPartitoner.class类
这个类就定义数据分发的策略。
    1.如果用指定partition，生产就不会条用DefaultPartitoner.partition()方法，直接发到指定的分区
    [这种不常用！]
    public ProducerRecord(String topic, Integer partition, K key, V value) {
        this(topic, partition, null, key, value, null);
    }

    2.当用户指定key，使用hash算法。如果key一直不变，同一个key算出来的hash值是一个固定值。
    如果是固定值，这种hash取模就没意义。
    public ProducerRecord(String topic, K key, V value) {
        this(topic, null, null, key, value, null);
    }

    3.当用户既没有指定partition也没有指定key时，使用轮询[round-robin]的方式发送数据
    public ProducerRecord(String topic, V value) {
        this(topic, null, null, null, value, null);
    }
</code></pre><h3 id="kafka-消费者的负载均衡"><a href="#kafka-消费者的负载均衡" class="headerlink" title="kafka-消费者的负载均衡"></a>kafka-消费者的负载均衡</h3><pre><code>举例说明：[问题重现]
    当每秒钟有400条数据过来，分了3个partition来存储，但是只有1个消费者并且每秒能消费100条，这样的话，生产者的速度很快，但是消费者跟不上，怎么办？
    造成了数据大量滞后和延时！
解决：
    多几个消费者，共同来消费数据
        比如3个消费者来共同消费数据这样就解决了。
新的问题：
    消费组中消费者的数量和partition的数量一致，但是消费者消费的熟读还是跟不上[比如每个消费者只能消费100条]，怎么办？
        再加个消费者吗？ 答案是 no！！！！
        因为根据kafka负载均衡策略规定，多出来的笑着是处于空闲状态的！
    也就是1个partition只能被1个消费者消费
真是解决办法：
    要么修改topic的partition数量；
    要么减少消费者处理时间，提高处理速度；
</code></pre><h4 id="kafka消息不丢失机制："><a href="#kafka消息不丢失机制：" class="headerlink" title="kafka消息不丢失机制："></a>kafka消息不丢失机制：</h4><p> 1.producer端消息不丢失机制：</p>
<pre><code>如果有多个副本，就需要选择一个leader出来，负责消息的读写请求。
比如：
    有一条数据经过partitioner.class计算把数据发送给了broker2[producerRecord ---&gt; 2]
    关于ack的响应有3个状态值：
        0：生产者只管发数据，并不关心数据是否丢失
        1：partition的leader收到数据后，就返回响应码状态
        -1：所有的从节点和leader都收到数据后，才返回响应码状态
    问题：
        如果broker端一直不给ack状态码，producer永远不知道是否成功。
            producer可以设置一个超时时间  10s，超过时间就认为失败。
    问题又来了：
        如果一条消息发送一次，得到一次ack相应，在大量数据情况下会占用很多带宽怎么办？
    解决：
        生产者将数据线缓存到producer端，达到一定的数量阈值或者时间阈值之后发送
        [比如：设置缓冲池中可以放2万条数据，或者等待时间设置成500ms]
    问题：
        如果设置buffer，按照500条每个批次发送数据到broker，但是broker迟迟不给相应，buffer中的数据如何处理？
        [而且producer端还源源不断的生产数据，这时候就造成了阻塞情况]
    解决：
        可以对buffer进行设置，如果满了，并不确定是否发送，
        如果需要继续生产数据，就可以选择buffer清空，或者不清空 [消息不丢失，一般会设置不清空]
    同步模式和异步模式：[异步就是没有设置缓冲池 buffer]
        在同步模式下：
            1. 生产者等待10s，如果broker没有给出ack响应，就认为失败。
            2. 生产者重置3次，如果还没响应，就报错。
        在异步模式下：[认为设置]
            1. 现将数据保存在生产者端的buffer中。buffer大小是2万条
            2. 满足数据阈值或者数量阈值其中的一个条件就可以发送数据
            3. 发送一批数据的大小是500条
        如果broker迟迟不给ack，而buffer又满了，开发者自己设置是否直接清空buffer中的数据。
</code></pre><p>2.broker端消息不丢失机制：</p>
<pre><code>broker端的消息不丢失，其实就是用partition副本机制来保证的。
</code></pre><p>3.consumer端消息不丢失机制：</p>
<pre><code>partition中有个segment段，log和index文件，log文件存放的是消息本身。
index文件存放的是消息offset值和存放在log文件的哪儿文件。
问题：[在kafka 0.8版本之前consumer消费数据的offset值是保存在zookeeper上的，但是
        这样会导致一种现象，就是consumer已经消费完了，但是等还没把offset值保存到zookeeper
        上的时候，consumer挂了，再次重启后，那么就会出现**重复**消费的问题]
解决：
    kafka从0.8版本以后，offset的值是保存在了kafka的内置topic上，
    这样就不会造成重复消费的问题了
</code></pre><h4 id="补充："><a href="#补充：" class="headerlink" title="补充："></a>补充：</h4><pre><code>Consumer Group [CG]：
    kafka用来实现一个topic消息的广播(发给所有的consumer)和单播(发给任意一个consumer)的手段
    一个topic可以有多个consumer group。topic的消息会复制(不是真的复制，只是概念上的)到所有的CG
    但每个partition只会把消息发给该CG中的一个consumer。
    用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic
broker：
    一台kafka服务器就是一个broker，一个集群由多个broker组成。
    一个broker可以容纳多个topic。
Partition：
    为了负债均衡
    kafka只保证按照一个partition中的顺序将消息发给consumer，
    不保证一个topic的整体（多个partition）的顺序。
leader：
    每一个replication集合中的partition都会选出一个唯一的leader，所有的读写请求都是由leader处理，
    其他的replicas从leader处把数据更新同步到本地。每个cluster当中会选举出一个broker来担任controller，
    负责处理partition的leader选举，协调partition迁移等工作。
ISR（In-Sync-Replica）：
    是Replicas的一个子集，表示目前Alive且与leader能够‘catch-up’的replicas集合。
    由于读写都是首先落到leader上，
    所以一般来说通过同步机制从leader上拉取数据的replica都会和leader有一些延迟
    [包括延迟时间和延迟条数2个维度]
    任意一个超过阈值都会把该replica提出ISR。每个Partition都有他自己独立的ISR。
</code></pre><h5 id="配置文件当中配置的kafka多久删除数据"><a href="#配置文件当中配置的kafka多久删除数据" class="headerlink" title="配置文件当中配置的kafka多久删除数据"></a>配置文件当中配置的kafka多久删除数据</h5><pre><code>The minimum age of a log file to be eligible for deletion
log.retention.hours=168 
定时检查周期，发现数据存了超过上面配置的时间，就干数据
log.retention.check.interval.ms=30000
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-HDFS读写流程&amp;Yarn执行流程" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/HDFS读写流程&Yarn执行流程/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/HDFS读写流程&Yarn执行流程/">
        客户端向HDFS读写数据流程
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="客户端向HDFS写数据流程"><a href="#客户端向HDFS写数据流程" class="headerlink" title="客户端向HDFS写数据流程"></a>客户端向HDFS写数据流程</h2><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/KYeqSTrKzIYBJCYrfZ6gqk7Ztvhkl3Ax*HFAUXpr1fM!/b/dGgBAAAAAAAA&amp;bo=SgVRAgAAAAADBz4!&amp;rf=viewer_4" alt=""></p>
<pre><code>1. 客户端向Namenode请求上传文件 /aaa/xxx.log
2. Namenode 检查自己的元数据，看看元数据下有没有这个目录，假设满足条件(没有这个目录可以上传)
    响应客户端的请求[客户端可以上传]
3. 客户端RPC请求上传1个Block(0-128M),请返回DataNode(返回几个DataNode是客户端配置的,默认3个)
4. 返回(dn1,dn3,dn4)给客户端
    为什么要给客户端返回这三台机器呢？
        考虑因素： 空间/距离
            1. 第1个副本(dn1),要看DataNode的空间(剩余存储空间)和距离(同一个机架上的距离一样，而不是谁的网线长短)
            假如这几台机器的空间差不多，NameNode就随机返回一台；
            2. 第2个副本(dn3),考虑跨机架挑选1个DataNode，增加副本的可靠性
            3. 第3个副本就在第1个副本同机架另外挑选1台DataNode存放
        怎么能让NameNode知道哪个datanode在1个机架，哪个datanode在另1个机架，这个要配个文件，
        配机架感知。[就是在配置文件里写死了，哪个机架有哪几台DataNode（网咯拓扑）]
5. 客户端会找最近的(将要上传副本到的那台机器)一台DataNode[假如找的是dn1]，请求建立Block传输通道channel[本地流，以供写入数据]
    5.1 dn1接收到请求看到还有2台机器(dn3、dn4)，dn1就向dn3发送请求建立管道
    5.2 dn3接收到请求看到还有一台dn4，就向dn4发送请求建立管道
6.1 dn4建立管道成功后，给dn3发送响应，建立管道成功
6.2 dn3再给dn1发送响应，建立管道成功
    这样一来，管道 PipeLine就建立成功了
7. 客户端读取本地的1块数据(默认128M)，向dn1传输数据(数据是以1个1个packet[默认64k]的形式上传的)
    packet是以chunk(Byte)为单位校验
    7.1 每个DataNode写入磁盘之前都会有1个缓冲区(ByteBuf)，先写入缓冲区，再写入本地磁盘
         因为每个packet只有64k，所以dn写入之后，基本上后面的dn3和dn4也都复制上传完成了[几乎同步完成]
         每个DataNode上传成功packet 都会向前面的DataNode反馈说上传成功，第一个DataNode才给客户端说上传成功
    假如说是后面两台DataNode节点没有上传成功，但是只要是第1台上传成功了，客户端就认为上传成功了
    [因为NameNode最后会帮客户端异步去复制，省得客户端一直阻塞，说没成功再来，没成功再来]；
    假如第1台也失败了，客户端就会找NameNode，告诉他这台DataNode不行，再换，然后重新返回DataNode，重新上传；
8. 上传完了第1个Block，客户端会再次向NameNode发送请求说要上传第2块Block，然后重新走刚刚的流程
总结：
    这样一来，NameNode的元数据就记录了，上传的文件的 路径、文件有几个Block、Block副本在哪些机器
</code></pre><h2 id="客户端向HDFS读数据流程"><a href="#客户端向HDFS读数据流程" class="headerlink" title="客户端向HDFS读数据流程"></a>客户端向HDFS读数据流程</h2><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/*cdq.W37ZmIVEpun93YYQqysu6asV1HifPC7o7.VPys!/b/dDEBAAAAAAAA&amp;bo=ygTkAQAAAAADFxk!&amp;rf=viewer_4" alt=""></p>
<pre><code>1. 客户端向NameNode请求下载/aa/xx.log
2. NameNode的元数据上面查看/aa/xx.log，然后查询得到：3个Block(Block1,Block2,Block3)、{Block1:dn1,dn3,dn4 Block2:dn1,dn4,dn5...}
    并返回给客户端目标文件的元数据信息
3. 客户端要下载第1个Block，然后就去找副本所在节点距离它最近的(dn1)，建立管道，请求读取Block1
4. dn1建立本地流，fileInpuStream，dn1建立 NIO socket - socketOutPutStream
5. 客户端建立socketInputStream，还有fileOutPutStream 写入本地 如：c:/xxx.log
6. dn1 传输通过通道传输数据
Block1上传到客户端完成后，客户端会再去找第2块Block 跟找Block1是一样的，还有Block3
</code></pre><h2 id="MR-Yarn的提交流程"><a href="#MR-Yarn的提交流程" class="headerlink" title="MR Yarn的提交流程"></a>MR Yarn的提交流程</h2><pre><code>Yarn：
    只负责程序运行所需资源的分配回收等调度任务，与应用程序的内部运行工作机制完全无关，
    所以Yarn已经成为一个通用的资源调度平台，许许多多的运算框架都可以借助他来实现资源管理，
    比如：MR/SPARK/FLINK/TEZ...
NodeManager分配container基于：
Linux的资源隔离机制cgroup  比如：docker也是基于此
</code></pre><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/9zhVGQyCCNef1I8asBSqn83n.wL1HN35NFV9J.n7CUM!/b/dJEAAAAAAAAA&amp;bo=QAXgAgAAAAADB4U!&amp;rf=viewer_4" alt=""></p>
<pre><code>1. 客户端所在节点，运行jar包，main方法里，job.submit() 
  YarnRunner(Proxy[实现了ClientProtocol])找Yarn集群的老大ResourceManager申请提交1个Application
2. ResourceManager返回Application提交资源路径：hdfs://xxx/xx.staging 和 application_id
3. YarnRunner提交job运行所需的资源文件，提交到hdfs://xxx/xx.staging/application_id
                                                                                    /job.split
                                                                                    /job.xml
                                                                                    wc.jar
4. 客户端通过RPC调用 告诉ResourceManager 资源提交完毕，申请运行mrAppMaster [后面的事情就和客户端没关系了]
ResourceManager运行的时候不止是接收这1个程序，还可能接收其他的程序提交，所以ResourceManager也有不同的调度策略[三种策略]：
    4.1 FIFO[先进先出]：任务在队列里，要先运行完第1个，再运行第2个
        [这个时候是可以接收其他的任务提交的，只不过要等][这个在老版本里是默认的]
    4.2 Fair：每个job提交后，都会分配一点点资源给他们
    4.3 capacity：第1个job提交之后，把资源全部给他，当第2个job来的时候，第1个job的一些task可能就已经运行完了，
        这样的话空余出来的资源就可以给后来提交的job了，如果后来的job需要的资源很少，可能直接就运行完了，运行完了，            还可以把这些资源给之前的应用继续使用
        [新版本的默认就是capacity调度策略]
5. 将用户的请求初始化成一个task
6. 因为nodeManager和NamenodeManager一直保持着心跳，所以会去领取任务(task),假如nm1领取到任务
7. nm1就会生成1个container容器[分配了cpu + ram]，到HDFS上下载资源并启动mrAppMatser
8. mrAppMatser向NamenodeManager申请运行MapTask的容器
9. 假如这时候nm2和nm3领取到task任务，然后分别在各自的节点生成1个容器[这个容器就是1个进程叫做YarnChild,可以通过jps查看]
10. mrAppMatser发送程序启动脚本[java -cp...]给nm2和nm3来运行MapTask
        mrAppMatser会监管这些MapTask的运行情况，如果哪个MapTask运行失败，
        他还会去找NamenodeManager重新申请1个nodeManager来生成container来运行MapTask
        如果他发现有一个MapTask运行的特别慢，他还会去重新申请1个新的nodeManager生成container，
        两个同时处理一个切片，看哪个先执行完，就用哪个的结果文件作为reduce的输入 **[推测执行]**
11. 当MapTask运行完成之后，mrAppMatser知道MapTask的输出文件[分区且排序，不如有3个分区]在哪[container里的工作目录]
12. mrAppMatser向NamenodeManager申请3个容器，来运行reduceTask程序
13. 这3个容器运行的reduce task会向相对应map task的nodeManager拉取相对应分区的数据过来
14. reduce 对数据进行merge + 排序 ，调用reduce方法 ，输出结果
15. 当mrAppMatser执行完了之后就会去找ResourceManager申请注销自己
</code></pre><h4 id="总结hadoop1和hadoop2的比较："><a href="#总结hadoop1和hadoop2的比较：" class="headerlink" title="总结hadoop1和hadoop2的比较："></a>总结hadoop1和hadoop2的比较：</h4><pre><code>a) 1里面没有yarn，只有jobTracker，负责资源调度和应用运算流程管理，
如果后面还有任务提交，还是他来管理和调度资源，这样的话都在1个节点上，
很消耗性能，而且当jobTracker挂掉之后，任务都不能运行了。
b) 2里面有yarn，只要把任务提交给yarn ，然后yarn进行资源管理就可以了，
appMaster来负责应用运算流程管理，当一个appMaster挂了之后，也不会影响
其他应用的作业                                                 
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-JVM GC " class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/JVM GC /" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/JVM GC /">
        JVM总结相关笔记
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="JVM"><a href="#JVM" class="headerlink" title="JVM"></a>JVM</h2><h4 id="JVM的参数设置"><a href="#JVM的参数设置" class="headerlink" title="JVM的参数设置"></a>JVM的参数设置</h4><pre><code>1. -Xms 初始堆大小  -Xmx 最大堆大小  [一般这两个值设置的是一样的，防止GC后出现内存抖动]
2. -Xmn 年轻代大小
    a) 整个堆的大小 = 年轻代大小 + 老年代大小 + 持久代大小
    b) 持久代一般固定大小为64M
    c) 所以，增大年轻代后，将会较小老年代的大小。这个值对系统性能影响较大，
       Sun官方推荐配置为整个堆的3/8
3. -XX:NewSize 初始年轻代大小     -XX:MaxNewSize 最大年轻代大小
4. -XX:NewRatio 老年代和年轻代的比值
5. -XX：SurvivorRatio 设置年轻代中Eden区与Survivor区的大小比值 
   [默认是8:1:1,比如设置为6，那么就是 6:2:2]
6. -XX:MaxTenuringThreshold 设置年轻代的对象被回收多少次后才进入老年代，默认15次
    [控制对象在经过多少次minor GC后进入老年代，此参数只在Serial串行GC时有效]
7. -XX:PermSize 初始持久代大小  -XX:MaxPermSize 持久代最大值 
    [这个跟 -Xms 和 -Xmx 堆大小设置一样，都是相等，为了防止内存抖动]
</code></pre><h4 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h4><pre><code>JVM调优主要针对内存管理方面，包括控制各个代的大小，GC策略。
由于GC开始垃圾回收时会挂起应用线程，严重影响性能，调优的目的就是
为了尽量降低GC所导致的应用线程暂停时间、减少Full GC的次数。
</code></pre><p> 代调优：</p>
<pre><code> 1) 避免新生代大小设置过小
     当新生代设置过小时，会产生两种比较明显的现象:
         一是minor GC次数频繁
         二是可能导致minor GC对象直接进入老年代。当老年代内存不足时，会出发Full GC。
2) 避免新生代设置过大
    新生代设置过大，会带来两个问题：
        一是老年代变小，可能导致Full GC频繁执行；
        二是minor GC 执行回收的时间大幅度增加
那怎么选择年轻代的大小呢？[分不同的应用场景]
    a. 响应时间优先的应用：
        尽可能设大，知道接近系统的最低响应时间限制(根据实际情况选择)。
</code></pre><p><img src="/Users/Macx/Desktop/整理笔记/jvm-gc.png" alt=""><br>说明：新new的对象会首先会进入年轻代的Eden中（如果对象太大可能直接进入年老代），在GC之前对象是存在Eden和from中的，进行GC的时候Eden中的对象被拷贝到To这样一个survive空间（survive（幸存）空间：包括from和to，他们的空间大小是一样的，又叫s1和s2）中（有一个拷贝算法），From中的对象（算法会考虑经过GC幸存的次数）到一定次数 阈值（如果说每次GC之后这个对象依旧在Survive中存在，GC一次他的Age就会加1，默认15就会放到OldGeneration。但是实际情况比较复杂，有可能没有到阈值就从Survive区域直接到Old Generation区域。)</p>
<h4 id="1-哪些需要回收-—-gt-java堆内存、方法区内存"><a href="#1-哪些需要回收-—-gt-java堆内存、方法区内存" class="headerlink" title="1.哪些需要回收? —-&gt; java堆内存、方法区内存"></a>1.哪些需要回收? —-&gt; java堆内存、方法区内存</h4><h4 id="2-什么时候回收？-—–-gt"><a href="#2-什么时候回收？-—–-gt" class="headerlink" title="2.什么时候回收？ —–&gt;"></a>2.什么时候回收？ —–&gt;</h4><pre><code>2.1：引用计数法【引用count+1，引用失效时count-1，为0时不被引用】
【如果两个对象相互引用而且都没有被使用了，那么会造成内存泄漏】。

2.2：可达性分析【从根节点搜索，如果没有搜索到就是没有被使用，所以互相引用且搜索不到的也会被清除】
</code></pre><h4 id="怎么回收？—-gt-垃圾回收算法-3种"><a href="#怎么回收？—-gt-垃圾回收算法-3种" class="headerlink" title="怎么回收？—-&gt; 垃圾回收算法[3种]"></a>怎么回收？—-&gt; 垃圾回收算法[3种]</h4><h4 id="1-标记清除算法-Mark-Sweep"><a href="#1-标记清除算法-Mark-Sweep" class="headerlink" title="1.标记清除算法 [Mark-Sweep]"></a>1.标记清除算法 [Mark-Sweep]</h4><pre><code>遍历所有的GC Root，分别标记处可达的对象和不可达的对象，然后将不可达的对象回收。
**缺点**是：效率低、回收得到的空间不连续 【当比较大的对象被创建时由于被回收的是不连续的，
所以被回收的空间就存不下，造成了浪费】
</code></pre><h4 id="2-标记整理算法"><a href="#2-标记整理算法" class="headerlink" title="2.标记整理算法"></a>2.标记整理算法</h4><pre><code>将所有可用的对象往前移动[标记谁是活跃对象，整理，会把内存对象整理成一棵树一个连续的空间]，这样会很耗资源
</code></pre><h4 id="3-复制算法"><a href="#3-复制算法" class="headerlink" title="3.复制算法"></a>3.复制算法</h4><pre><code>将内存分为两块，每次只使用一块。当这一块内存满了，就将还存活的对象复制到另一块上，并且严格按照内存地址排列，然后把已使用的那块内存统一回收。
**优点**是：能够得到连续的内存空间 
**缺点**是：浪费了一半内存
年轻代使用的是复制整理算法
</code></pre><h5 id="有一点需要注意："><a href="#有一点需要注意：" class="headerlink" title="有一点需要注意："></a>有一点需要注意：</h5>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-Spark性能调优：" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/Spark性能调优：/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/Spark性能调优：/">
        Spark性能调优
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Spark性能调优："><a href="#Spark性能调优：" class="headerlink" title="Spark性能调优："></a>Spark性能调优：</h1><h2 id="1-分配更多的资源-–-性能调优的王道"><a href="#1-分配更多的资源-–-性能调优的王道" class="headerlink" title="1.分配更多的资源  – 性能调优的王道"></a>1.分配更多的资源  – 性能调优的王道</h2><pre><code>真实项目里的脚本：
    bin/spark-submit \
    --class com.xx.xx \
    --num-executors 80 \
    --driver-memory 6g \
    --executor-cores 3 \
    --master yarn-cluster \
    --queue root.default \
    --conf spark.yarn.executor.memoryOverhead=2048 \
    --conf spark.core.connection.ack.waite.timeout=300 \
    /usr/xx/xx.jar \
    args
</code></pre><h6 id="分配资源之前，要先看机器能够给我们提供多少资源，并且最大化的利用这些资源才是最好的；"><a href="#分配资源之前，要先看机器能够给我们提供多少资源，并且最大化的利用这些资源才是最好的；" class="headerlink" title="分配资源之前，要先看机器能够给我们提供多少资源，并且最大化的利用这些资源才是最好的；"></a>分配资源之前，要先看机器能够给我们提供多少资源，并且最大化的利用这些资源才是最好的；</h6><pre><code>1.standalone模式：
    根据实际情况分配spark作业资源，比如每台机器4G，2个cpu，20台机器
2.spark-on-yarn模式：
    要看spark作业要提交到的资源队列，大概有多少资源？
</code></pre><h6 id="SparkContext、DAGScheduler、taskScheduler，会将我们的算子切割成大量的task提交到Application的executor上去执行-比如分配给其中一个的executor100个task，他的cpu只有2个，那么并行只能执行2个task，如果cpu为5个，那么久先执行5个再执行5个"><a href="#SparkContext、DAGScheduler、taskScheduler，会将我们的算子切割成大量的task提交到Application的executor上去执行-比如分配给其中一个的executor100个task，他的cpu只有2个，那么并行只能执行2个task，如果cpu为5个，那么久先执行5个再执行5个" class="headerlink" title="SparkContext、DAGScheduler、taskScheduler，会将我们的算子切割成大量的task提交到Application的executor上去执行,比如分配给其中一个的executor100个task，他的cpu只有2个，那么并行只能执行2个task，如果cpu为5个，那么久先执行5个再执行5个"></a>SparkContext、DAGScheduler、taskScheduler，会将我们的算子切割成大量的task提交到Application的executor上去执行,比如分配给其中一个的executor100个task，他的cpu只有2个，那么并行只能执行2个task，如果cpu为5个，那么久先执行5个再执行5个</h6><pre><code>a.增加executor的数量：
    如果executor的数量比较少，那么意味着可以并行执行的task的数量就比较少，
    就意味着Application的并行执行能力比较弱；
        比如：
            有3个executor，每个executor有2个cup core，
            那么能够并行执行的task就是6个，6个执行完换下一批6个
    增加executor的个数后，并行执行的task就会变多，性能得到提升
b.增加每个executor的cpu core
        比如：
            之前：3个exexutor，每个executor的cpu core为2个，那么并执行的task是6个
                 把cpu core增加到5个，那么并行执行的task就是15个，提高了性能
c.增加每个executor的内存量：
    1.如果需要对RDD进行cache，那么更多的内存就能缓存更多的数据，
      将更少的数据写入磁盘，甚至不写入磁盘，减少了磁盘IO。
    2.对于shuffle操作，reduce端会需要内存来存储拉取过来的数据进行聚合，如果内存不够，
      也会写入磁盘，增加executor内存，就会有更少的数据写入磁盘，较少磁盘IO，提高性能。
    3.对于task的执行，需要创建很多对象，如果内存比较小，可能导致JVM堆内存满了，
      然后频繁的GC，垃圾回收，minorGC和fullGC，速度会很慢，如果加大内存，
      带来更少的GC，速度提升。
</code></pre><h2 id="2-调节并行度"><a href="#2-调节并行度" class="headerlink" title="2.调节并行度"></a>2.调节并行度</h2><pre><code>并行度：spark作业中，各个stage的task个数，也就代表了saprk作业中各个阶段[stage]的并行度。
[spark作业，Application，jobs，action会触发一个job，每个job会拆成多个stage，发生shuffle的时候，会拆出一个stage]
</code></pre><h5 id="如果不调节并行度，导致并行度过低，会怎么样？？？"><a href="#如果不调节并行度，导致并行度过低，会怎么样？？？" class="headerlink" title="如果不调节并行度，导致并行度过低，会怎么样？？？"></a>如果不调节并行度，导致并行度过低，会怎么样？？？</h5><pre><code>比如：
    1.我们通过上面的分配好资源后，有50个executor，每个executor10G内存，每个executor有3个cpu core
      基本已经达到了集群或者yarn的资源上限
    2. 50个executor * 3个cpu = 150个task,即可以并行执行150个task；
      而我们设置的时候只设置了100个并行度的task，这时候每个executor分配到2个task，同时运行task的数量只有100个，导致每个executor剩下的1个cpu core在那空转，浪费资源。
    资源虽然够了，但是并行度没有和资源相匹配，导致分配下去的资源浪费掉了！！！
    **合理的并行度设置，应该要设置的足够大，大到完全合理的利用集群资源！而且减少了每个task要处理的数据量[比如150g的数据分别分发给100个task处理就是每个task处理1.5G，但是如果是150个task的话，每个task就处理1G]**
总结：
    a. task数量，至少设置成与Spark application的总cpu数相同(理想状态下，比如150个cpu core，分配150个task，差不多同时运行完毕)
    b.官方推荐做法：task的数量设置成 Spark application的cpu core的个数的3~5倍！！
      比如总共150个cpu core，设置成300~500个task
      为什么这么设置呢？？？
           实际情况下和理想状态下是不一样的，因为有些task运行的快，有些运行的慢，
           比如有些运行需要50s，有些需要几分钟运行完毕，如果刚好设置task数量和cpu core的数量相同，可能会导致资源的浪费；
           比如150个task，10个运行完了，还有140个在运行，那么势必会导致10个cpu core的闲置，
           所以如果设置task的数量为cpu数量的2~3倍，一旦有task运行完，另一个task就会立刻补上来，
           尽量让cpu core不要空闲，提升了spark作业运行效率，提升性能。

    c.如何设置一个 Spark application的并行度？？？
        SparkConf sc = new SparkConf()
                       .set(&quot;spark.default.parallelism&quot;,&quot;500&quot;);
</code></pre><h2 id="3-重构RDD架构及RDD持久化"><a href="#3-重构RDD架构及RDD持久化" class="headerlink" title="3.重构RDD架构及RDD持久化:"></a>3.重构RDD架构及RDD持久化:</h2><pre><code>默认情况下 RDD出现的问题：             
                                              RDD4
                                            /
           hdfs --&gt; RDD1 --&gt; RDD2 --&gt;RDD3
                                            \
                                              RDD5
    以上情况： 执行RDD4和RDD5的时候都会从RDD1到RDD2然后到RDD3，执行期间的算子操作，
    而不会说到RDD3的算子操作后的结果给缓存起来，所以这样很麻烦，
    出现了RDD重复计算的情况，导致性能急剧下降！
结论：
    对于要多次计算和使用的公共RDD,一定要进行持久化！
    持久化也就是：BlockManager将RDD的数据缓存到内存或者磁盘上，后续无论对这个RDD进行多少次计算，都只需要到缓存中取就ok了。
    持久化策略：
        rdd.persist(StorageLevel.xx()) 或者 cache
        1.优先会把数据缓存到内存中 -- StorageLevel.MEMORY_ONLY
        2.如果纯内存空间无法支撑公共RDD的数据时，就会优先考虑使用序列化的机制在纯内存中存储，
        将RDD中的每个partition中的数据序列化成一个大的字节数组，也就是一个对象，
        序列化后，大大减少了内存空间的占用。-- StorageLevel.MEMORY_ONLY_SER
            序列化唯一的缺点：在获取数据的时候需要反序列化
        3.如果序列化纯内存的方式还是导致OOM，内存溢出的话，那就要考虑磁盘的方式。
          内存+磁盘的普通方式(无序列化) -- StorageLevel.MEMORY_AND_DISK
        4.如果上面的还是无法存下的话，就用 内存+磁盘+序列化 -- StorageLevel.MEMORY_AND_DISK_SER
另：在机器内存**极度充足**的情况下，可以使用双副本机制，来持久化，保证数据的高可靠性
    如果机器宕机了，那么还有一份副本数据，就不用再次进行算子计算了。[锦上添花--一般不要这么做] -- StorageLevel.MEMORY_ONLY_SER_2
</code></pre><h3 id="4-广播大变量-sc-broadcast-rdd-collect"><a href="#4-广播大变量-sc-broadcast-rdd-collect" class="headerlink" title="4.广播大变量 [sc.broadcast(rdd.collect)]"></a>4.广播大变量 [sc.broadcast(rdd.collect)]</h3><pre><code>问题情景：
    当我们在写程序用到外部的维度表的数据进行使用的时候，程序默认会给每个task都发送相同的这个数据，
    如果这个数据为100M，如果我们有1000个task，100G的数据，通过网络传输到task，
    集群瞬间因为这个原因消耗掉100G的内存，对spark作业运行速度造成极大的影响，性能想想都很可怕！！！
解决方案：
    sc.broadcast(rdd.collect)
    分析原理：
        [BlockManager:负责管理某个executor上的内存和磁盘上的数据]
        广播变量会在Driver上有一份副本，当一个task使用到广播变量的数据时，会在自己本地的executor的BlockManager去取数据，
        发现没有，BlockManager就会到Driver上远程去取数据，并保存在本地，
        然后第二个task需要的时候来找BlockManager直接就可以找到该数据，
        executor的Blockmanager除了可以到Driver远程的取数据，
        还可能到邻近节点的BlockManager上去拉取数据，越近越好!
    举例说明：
        50个executor，1000个task，外部数据10M，
        默认情况下，1000个task，1000个副本，10G数据，网络传输，集群中10G的内存消耗
        如果使用广播，50个executor，500M的数据，网络传输速率大大增加，
        10G=10000M 和 500M的对比 20倍啊。。。
**之前的一个测试[真实]：
        没有经过任何调优的spark作业，运行下来16个小时，
        合理分配资源+合理分配并行度+RDD持久化，作业下来5个小时,
        非常重要的一个调优Shuffle优化之后，2~3个小时，
        应用了其他的性能调优之后，JVM调参+广播等等，30分钟
        16个小时 和 30分钟对比，太可怕了！！！性能调优真的真的很重要！！！
</code></pre><h3 id="5-Kryo序列化机制："><a href="#5-Kryo序列化机制：" class="headerlink" title="5.Kryo序列化机制："></a>5.Kryo序列化机制：</h3><p>默认情况下，Spark内部使用java的序列化机制</p>
<pre><code>ObjectOutPutStream/ObjectInputStream,对象输入输出流机制来进行序列化
这种序列化机制的好处：
    处理方便，只是需要在算子里使用的变量是实现Serializable接口即可
缺点在于：
    默认序列化机制效率不高，序列化的速度比较慢，序列化后的数据占用内存空间还比较大
</code></pre><p> 解决：手动进行序列化格式的优化：Kryo [spark支持的]</p>
<pre><code>Kryo序列化机制比默认的java序列化机制速度快，
序列化后的数据更小，是java序列化后数据的 1/10 。
所以Kryo序列化后，会让网络传输的数据更少，在集群中耗费的资源大大减少。

Kryo序列化机制：[一旦启用，会生效的几个地方]
    a.算子函数中使用到的外部变量[比如广播的外部维度表数据]
        优化网络传输性能，较少集群的内存占用和消耗
    b.持久化RDD时进行序列化，StorageLevel.MEMORY_ONLY_SER
        将每个RDD partition序列化成一个大的字节数组时，就会使用Kryo进一步优化序列化的效率和性能。
        持久化RDD占用的内存越少，task执行的时候，创建的对象，不至于频繁的占满内存，频繁的GC
    c.shuffle
        在stage间的task的shuffle操作时，节点与节点之间的task会通过网络拉取和传输数据，
        此时这些数据也是可能需要序列化的，就会使用Kryo
</code></pre><p> 实现Kryo：</p>
<pre><code>step1. 在SparkConf里设置 new SparkConf()
                       .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KyroSerializer&quot;)
                       .registerKryoClasses(new Class[]{MyCategory.class})
    [Kryo之所以没有没有作为默认的序列化类库，就是因为Kryo要求，如果要达到它的最佳效果的话]
    [一定要注册我们自定义的类，不如：算子函数中使用到了外部自定义的对象变量，这时要求必须注册这个类，否则Kyro就达不到最佳性能]
step2. 注册使用到的，需要Kryo序列化的一些自定义类
</code></pre><h3 id="6-使用FastUtil优化数据格式："><a href="#6-使用FastUtil优化数据格式：" class="headerlink" title="6.使用FastUtil优化数据格式："></a>6.使用FastUtil优化数据格式：</h3><p>FastUtil是什么？？</p>
<pre><code>fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue；
fastutil能够提供更小的内存占用，更快的存取速度；我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，
fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。 
fastutil还提供了一些JDK标准类库中没有的额外功能（比如双向迭代器）。 
fastutil除了对象和原始类型为元素的集合，fastutil也提供引用类型的支持，但是对引用类型是使用等于号（=）进行比较的，而不是equals()方法。 
fastutil尽量提供了在任何场景下都是速度最快的集合类库。
</code></pre><p>Spark中FastUtil运用的场景？？</p>
<pre><code>1.如果算子函数使用了外部变量，
    第一可以使用broadcast广播变量优化；
    第二可以使用Kryo序列化类库，提升序列化性能和效率；
    第三如果外部变量是某种比较大的集合(Map、List等)，可以考虑fastutil来改写外部变量，
        首先从源头上就减少了内存的占用，通过广播变量进一步减少内存占用，
        再通过Kryo类库进一步减少内存占用
    避免了executor内存频繁占满，频繁唤起GC，导致性能下降的现象
</code></pre><p>使用步骤：</p>
<pre><code>step1:导入pom依赖
    &lt;dependency&gt;
        &lt;groupId&gt;fastutil&lt;/groupId&gt;
            &lt;artifactId&gt;fastutil&lt;/artifactId&gt;
        &lt;version&gt;5.0.9&lt;/version&gt;
    &lt;/dependency&gt;
step2:
    List&lt;Integer&gt; =&gt; IntList
     基本都是类似于IntList的格式，前缀就是集合的元素类型，
     特殊的就是Map，Int2IntMap，代表了Key-Value映射的元素类型
</code></pre><h3 id="7-调节数据本地化等待时长"><a href="#7-调节数据本地化等待时长" class="headerlink" title="7.调节数据本地化等待时长:"></a>7.调节数据本地化等待时长:</h3><p>问题发生的场景：</p>
<pre><code>spark在Driver上，对Application的每一个stage的task分配之前，
都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；
spark的分配算法：
    a.优先把每一个task正好分配到他要计算的数据所在的节点，这样的话不用在网络间传输数据
    b.但是，task没有机会分配到数据所在的节点上，为什么呢？？？
        因为那个节点上的计算资源和计算能力都满了，这个时候 spark会等待一段时间，
        默认情况下是3s钟，到最后，实在等不了了，就会选择一个较差的本地化级别，
        比如说会把task分配到靠他要计算的数据的节点最近的节点，然后进行计算
    c.对于b来说肯定要发生网络传输，task会通过其所在节点的executor的BlockManager来获取数据，
    BlockManager发现自己本地没有，就会用getRemote()的方法，通过TransferService(网络数据传输组件)
    从数据所在节点的BlockManager中获取数据，通过网络传输给task所在的节点
总结：
  我们肯定是希望看到 task和数据都在同一个节点上，直接从本地的executor中的BlockManager中去获取数据，
  纯内存或者带点IO，如果通过网络传输，那么大量的网络传输和磁盘IO都是性能的杀手
</code></pre><p>本地化的级别类型：</p>
<pre><code>1.PROCESS_LOCAL: 进程本地化,代码和数据都在同一个进程中，也就是在同一个executor进程中，
  task由executor来执行，数据在executor的BlockManager中，性能最好
2.NODE_LOCAL: 节点本地化，比如说一个节点上有两个executor，其中一个task需要第一个executor的数据，
  但是他被分配给了第二个executor，他会找第二个executor的BlockManager去取数据，但是没有，
  BlockManager会去第一个的executor的BlockManager去取数据，这是发生在进程中的
3.NOPREF: 数据从哪里获取都一样，没有好坏之分
4.RACK_LOCAL: 数据在同一个机架上的不同节点上，需要进行网络间的数据传输
5.ANY: 数据可能在集群中的任何地方，而且不在同一个机架，这种性能最差！！
</code></pre><p>开发时的流程：</p>
<pre><code>观察spark作业时的日志，先测试，先用client模式，在本地就可以看到比较全的日志。
日志里面会显示：starting task...,PROCESS_LOCAL,或者是NODE_LOCAL，观察大部分数据本地化的级别
如果发现大部分都是PROCESS_LOCAL的级别，那就不用调了，如果大部分都是NODE_LOCAL或者ANY，那就要调节一下等待时长了
要反复调节，反复观察本地化级别是否提升，查看spark作业运行的时间有没有缩短
不要本末倒置，如果是 本地化级别提升了，但是因为大量的等待时间，spark作业的运行时常变大了，这就不要调节了
spark.locality.waite
spark.locality.waite.process
spark.locality.waite.node
spark.locality.waite.rack
默认等待时长都是3s
设置方法：
    new SparkConf().set(&quot;spark.locality.waite&quot;,&quot;10&quot;)//不要带s
</code></pre><h3 id="8-JVM调优：1个executor对应1个JVM进程"><a href="#8-JVM调优：1个executor对应1个JVM进程" class="headerlink" title="8.JVM调优：1个executor对应1个JVM进程"></a>8.JVM调优：1个executor对应1个JVM进程</h3><p>A. 降低cache操作的内存占比</p>
<pre><code>JVM模块：
    每一次存放对象的时候都会放入eden区域，其中有一个survivor区域，另一个survivor区域是空闲的[新生代]，
    当eden区域和一个survivor区域放满了以后(spark运行产生的对象太多了)，
    就会触发minor gc，小型垃圾回收，把不再使用的对象从内存中清空，给后面新创建的对象腾出空间
    清理掉了不在使用的对象后，还有一部分存活的对象(还要继续使用的对象)，
    将存活的对象放入空闲的那个survivor区域里，这里默认eden:survivor1: survivor2 = 8:1:1,
    假如对象占了1.5放不下survivor区域了，那么就会放到[老年代]里；
    假如JVM的内存不够大的话，可能导致频繁的新生代内存满溢，频繁的进行minor gc，
    频繁的minor gc会导致短时间内，有些存活的对象，多次垃圾回收都没有回收掉，
    会导致这种短生命周期的对象(其实是不一定要长期使用的对象)年龄过大，
    垃圾回收次数太多，还没有回收到，就已经跑到了老年代；
    老年代中可能会因为内存不足，囤积一大堆短生命周期的对象(本来应该在年轻代中的)，
    可能马上就要回收掉的对象，此时可能造成老年代内存满溢，造成频繁的full gc(全局/全面垃圾回收)，full gc就会去老年代中回收对象；
    由于full gc算法的设计，是针对老年代中的对象，数量很少，满溢进行full gc的频率应该很少，
    因此采取了不太复杂的但是耗费性能和时间的垃圾回收算法。full gc 很慢很慢；
    full gc 和 minor gc，无论是快还是慢，都会导致JVM的工作线程停止工作，即 stop the world，
    简言之：gc的时候，spark停止工作，等待垃圾回收结束;
在spark中，堆内存被分为了两块：
    一块是专门用来给RDD cache和persist操作进行RDD数据缓存用的；
    一块是给spark算子函数的运行使用的，存放函数中自己创建的对象；
默认情况下，给RDD cache的内存占比是60%,但是在某些情况下，比如RDD cache不那么紧张，
而task算子函数中创建的对象过多，内存不太大，导致频繁的minor gc，甚至频繁的full gc，
导致spark频繁的暂停工作，性能影响会非常大，
解决办法：
    集群是spark-onyarn的话就可以通过spark ui来查看，spark的作业情况，
    可以看到每个stage的运行情况，包括每个task的运行时间，gc时间等等，
    如果发现gc太频繁，时间太长，此时可以适当调节这个比例；
总结：
    降低cache的内存占比，大不了用persist操作，选择将一部分的RDD数据存入磁盘，
    或者序列化方式Kryo，来减少RDD缓存的内存占比；
    对应的RDD算子函数的内存占比就增多了，就可以减少minor gc的频率，同时减少full gc的频率，提高性能
具体实现：0.6-&gt;0.5-&gt;0.4-&gt;0.2
    new SparkConf().set(&quot;spark.storage.memoryFraction&quot;,&quot;0.5&quot;)
</code></pre><p>B. executor堆外内存与连接时常</p>
<pre><code>1. executor堆外内存[off-heap memory]:
   场景：
        比如两个stage，第二个stage的executor的task需要第一个executor的数据，
        虽然可以通过Driver的MapOutputTracker可以找到自己数据的地址[也就是第一个executor的BlockManager]，
        但是第一个executor已经挂掉了，关联的BlockManager也没了，就没办法获取到数据；

    有时候，如果你的spark作业处理的数据量特别大，几亿数据量；
    spark作业一运行，是不是报错诸如：shuffle file cannot find,executor task lost,out of memory,
    这时候可能是executor的堆外内存不够用了，导致executor在运行的时候出现了内存溢出；
    导致后续的stage的task在运行的时候，可能从一些executor中拉取shuffle map output 文件，
    但是executor已经挂掉了，关联的BlockManager也没有了，所以可能会报shuffle output file not found，resubmitting task，executor lost，spark作业彻底失败；
  这个时候就可以考虑调节executor的堆外内存，堆外内存调节的比较大的话，也会提升性能；

    怎么调价堆外内存的大小？？
        在spark-submit 的脚本中添加 
                    --conf spark.yarn.executor.memoryOverhead=2048
        注意：这个设置是在spark-submit脚本中，不是在 new SparkConf()里设置的！！！
        这个是在spark-onyarn的集群中设置的，企业也是这么设置的！
        默认情况下,堆外内存是300多M，我们在项目中通常都会出现问题，导致spark作业反复崩溃，
        我们就可以调节这个参数 ，一般来说至少1G(1024M)，有时候也会2G、4G，
        来避免JVM oom的异常问题，提高整体spark作业的性能
2. 连接时常的等待：
          知识回顾：如果JVM处于垃圾回收过程，所有的工作线程将会停止，相当于一旦进行垃圾回收，
          spark/executor就会停止工作，无法提供响应
   场景：
          通常executor优先会从自己关联的BlockManager去取数据，如果本地没有，
          会通过TransferService，去远程连接其他节点上的executor的BlockManager去取；
          如果这个远程的executor正好创建的对象特别大，特别多，频繁的让JVM的内存满溢，进行垃圾回收，
          此时就没有反应，无法建立网络连接，会有卡住的现象。spark默认的网络连接超时时间是60s，
          如果卡住60秒都无法建立网络连接的话，就宣布失败；
          出现的现象：偶尔会出现，一串fileId诸如：hg3y4h5g4j5h5g5h3 not found，file lost，
          报错几次，几次都拉取不到数据的话，可能导致spark作业的崩溃！
          也可能会导致DAGScheduler多次提交stage，TaskScheduler反复提交多次task，
          大大延长了spark作业的运行时间
  解决办法：[注意是在shell脚本上不是在SparkConf上set！！]
          spark-submit 
                       --conf spark.core.connection.ack.waite.timeout=300
</code></pre><h2 id="9-shuffle调优"><a href="#9-shuffle调优" class="headerlink" title="9.shuffle调优"></a>9.shuffle调优</h2><pre><code>shuffle的概念以及场景
    什么情况下会发生shuffle？？
        在spark中，主要是这几个算子：groupByKey、reduceByKey、countByKey、join等
    什么是shuffle？
        a) groupByKey：把分布在集群中各个节点上的数据中同一个key，对应的values都集中到一块，
        集中到集群中的同一个节点上，更严密的说就是集中到一个节点上的一个executor的task中。
        集中一个key对应的values后才能交给我们处理，&lt;key,iterable&lt;value&gt;&gt;
          b) reduceByKey：算子函数对values集中进行reduce操作，最后变成一个value
          c) join  RDD&lt;key,value&gt;    RDD&lt;key,value&gt;,只要两个RDD中key相同的value都会到一个节点的executor的task中，供我们处理
      以reduceByKey为例：
</code></pre><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/62Boojghtc6iLB8KUOFgXQfCRbAwTZoOek9gr3ocl*o!/b/dDMBAAAAAAAA&amp;bo=YgrgAgAAAAADB6g!&amp;rf=viewer_4" alt=""></p>
<h4 id="9-1-shuffle调优之-map端合并输出文件"><a href="#9-1-shuffle调优之-map端合并输出文件" class="headerlink" title="9.1. shuffle调优之 map端合并输出文件"></a>9.1. shuffle调优之 map端合并输出文件</h4><pre><code>默认的shuffle对性能有什么影响？？
    实际生产环境的条件：
        100个节点，每个节点一个executor：100个executor，每个executor2个cpu core，
        总格1000个task，平均到每个executor是10个task；按照第二个stage的task个数和第一个stage的相同，
        那么每个节点map端输出的文件个数就是：10 * 1000 = 10000 个
        总共100个节点，总共map端输出的文件数：10000 * 100 = 100W 个
        100万个。。。太吓人了！！！
    shuffle中的写磁盘操作，基本上是shuffle中性能消耗最严重的部分，
    通过上面的分析可知，一个普通的生产环境的spark job的shuffle环节，会写入磁盘100万个文件，
    磁盘IO性能和对spark作业执行速度的影响，是极其惊人的！！
    基本上，spark作业的性能，都消耗在了shuffle中了，虽然不只是shuffle的map端输出文件这一部分，但是这也是非常大的一个性能消耗点。
怎么解决？
    开启map端输出文件合并机制：
        new SparkConf().set(&apos;spark.shuffle.consolidateFiles&apos;,&apos;true&apos;)
    实际开发中，开启了map端输出文件合并机制后，有什么变化？
        100个节点，100个executor，
        每个节点2个cpu core，
        总共1000个task，每个executor10个task，
        每个节点的输出文件个数：
            2*1000 = 2000 个文件
        总共输出文件个数：
            100 * 2000 = 20万 个文件
        相比开启合并之前的100万个，相差了5倍！！
合并map端输出文件，对spark的性能有哪些影响呢？
    1. map task写入磁盘文件的IO，减少：100万 -&gt; 20万个文件
    2. 第二个stage，原本要拉取第一个stage的task数量文件，1000个task，第二个stage的每个task都会拉取1000份文件，走网络传输；合并以后，100个节点，每个节点2个cpu，第二个stage的每个task只需要拉取 100 * 2 = 200 个文件，网络传输的性能大大增强
    实际生产中，使用了spark.shuffle.consolidateFiles后，实际的调优效果：
        对于上述的生产环境的配置，性能的提升还是相当可观的，从之前的5个小时 降到了 2~3个小时
总结：
    不要小看这个map端输出文件合并机制，实际上在数据量比较大的情况下，本身做了前面的优化，
    executor上去了 -&gt; cpu core 上去了 -&gt; 并行度（task的数量）上去了，但是shuffle没调优，
    这时候就很糟糕了，大量的map端输出文件的产生，会对性能有比较恶劣的影响 
</code></pre><h4 id="9-2-map端内存缓冲与reduce端内存占比"><a href="#9-2-map端内存缓冲与reduce端内存占比" class="headerlink" title="9.2. map端内存缓冲与reduce端内存占比"></a>9.2. map端内存缓冲与reduce端内存占比</h4><pre><code>spark.shuffle.file.buffer,默认32k
spark.shuffle.memoryFraction,占比默认0.2
调优的分量：
    map端内存缓冲和reduce端内存占比，网上对他俩说的是shuffle调优的不二之选，其实这是不对的，
    因为以实际的生产经验来说，这两个参数没那么重要，但是还是有一点效果的，
    就像是很多小的细节综合起来效果就很明显了，
</code></pre><p>原理：</p>
<pre><code>map：
    默认情况下，shuffle的map task输出到磁盘文件的时候，统一都会先写入每个task自己关联的一个内存缓冲区中，
    这个缓冲区默认大小是32k，每一次，当内存缓冲区满溢后，才会进行spill操作，溢写到磁盘文件中
reduce：
    reduce端task，在拉取数据之后，会用hashmap的数据格式来对每个key对应的values进行汇聚，
    针对每个key对应的value，执行我们自定义的聚合函数的代码，比如_+_,(把所有values相加)
    reduce task,在进行汇聚、聚合等操作的时候，实际上，使用的就是自己对应的executor的内存，
    executor(jvm进程，堆),默认executor内存中划分给reduce task进行聚合的比例是20%。
    问题来了，内存占比是20%，所以很有可能会出现，拉取过来的数据很多，那么在内存中，
    放不下，这个时候就会发生spill(溢写)到磁盘文件中取.
</code></pre><p>如果不调优会出现什么问题？？</p>
<pre><code>默认map端内存缓冲是32k，
默认reduce端聚合内存占比是20%
如果map端处理的数据比较大，而内存缓冲是固定的，会出现什么问题呢？
    每个task处理320k，32k的内存缓冲，总共向磁盘溢写10次，
    每个task处理32000k，32k的内存缓冲，总共向磁盘溢写1000次，
    这样就造成了多次的map端往磁盘文件的spill溢写操作，发生大量的磁盘IO，降低性能
map数据量比较大，reduce端拉取过来的数据很多，就会频繁的发生reduce端聚合内存不够用，
频繁发生spill操作，溢写到磁盘上去，这样一来，磁盘上溢写的数据量越大，
后面进行聚合操作的时候，很可能会多次读取磁盘中的数据进行聚合
默认情况下，在数据量比较大的时候，可能频繁的发生reduce端磁盘文件的读写；
这两点是很像的，而且有关联的，数据量变大，map端肯定出现问题，reduce也出现问题，
出的问题都是一样的，都是磁盘IO频繁，变多，影响性能
</code></pre><p>调优解决：</p>
<pre><code>我们要看spark UI，
    1. 如果公司用的是standalone模式，那么很简单，把spark跑起来，会显示sparkUI的地址，
    4040端口号，进去看，依次点击可以看到，每个stage的详情，有哪些executor，有哪些task，
    每个task的shuffle write 和 shuffle read的量，shuffle的磁盘和内存，读写的数据量
    2. 如果是yarn模式提交，从yarn的界面进去，点击对应的application，进入spark ui，查看详情
如果发现磁盘的read和write很大，就意味着要调节一下shuffle的参数，进行调优，
首先当然要考虑map端输出文件合并机制
   调节上面两个的参数，原则是：
      spark.shuffle.buffer，每次扩大一倍，然后看看效果，64k，128k
    spark.shuffle.memoryFraction,每次提高0.1，看看效果
不能调节的过大，因为你这边调节的很大，相对应的其他的就会变得很小，其他环节就会出问题
调节后的效果：
    map task内存缓冲变大了，减少了spill到磁盘文件的次数；
    reduce端聚合内存变大了，减少了spill到磁盘的次数，而且减少了后面聚合时读取磁盘的数量
    new SparkConf()
    .set(&quot;spark.shuffle.file.buffer&quot;,&quot;64&quot;)
    .set(&quot;spark.shuffle.file.memoryFraction&quot;,&quot;0.3&quot;)
</code></pre><h2 id="10-算子调优"><a href="#10-算子调优" class="headerlink" title="10.算子调优"></a>10.算子调优</h2><h5 id="1-算子调优之MapPartitons提升map的操作性能"><a href="#1-算子调优之MapPartitons提升map的操作性能" class="headerlink" title="1.算子调优之MapPartitons提升map的操作性能"></a>1.算子调优之MapPartitons提升map的操作性能</h5><pre><code>在spark中最近本的原则：每个task处理RDD中的每一个partition
优缺点对比：
    普通Map：
        优点：比如处理了一千条数据，内存不够了，那么就可以将已经处理的一千条数据从内存里面垃圾回收掉，
        或者用其他办法腾出空间；通常普通的map操作不会导致内存OOM异常；
        缺点：比如一个partition中有10000条数据，那么function会执行和计算一万次
    MapPartitions:
        优点：一个task仅仅会执行一次function，一次function接收partition中的所有数据
        只要执行一次就可以了，性能比较高
        缺点：对于大数据量来说，比如一个partition100万条数据，一次传入一个function后，
        可能一下子内存就不够了，但是又没办法腾出空间来，可能就OOM，内存溢出
那么什么时候使用MapPartitions呢？
    当数据量不太大的时候，都可以使用MapPartitions来操作，性能还是很不错的，
    不过也有经验表明用了MapPartitions后，内存直接溢出，
    所以在项目中自己先估算一下RDD的数据量，以每个partition的量，还有分配给executor的内存大小，
    可以试一下，如果直接OOM了，那就放弃吧，如果能够跑通，那就可以使用。
</code></pre><h5 id="2-算子调优之filter之后-filter-之后-用-coalesce来减少partition的数量"><a href="#2-算子调优之filter之后-filter-之后-用-coalesce来减少partition的数量" class="headerlink" title="2.算子调优之filter之后 filter 之后  用 coalesce来减少partition的数量"></a>2.算子调优之filter之后 filter 之后  用 coalesce来减少partition的数量</h5><pre><code>默认情况下，RDD经过filter之后，RDD中每个partition的数据量会不太一样，(原本partition里的数据量可能是差不多的)
问题：
    1.每一个partition的数据量变少了，但是在后面进行处理的时候，
    还是要和partition的数量一样的task数量去处理，有点浪费task计算资源
    2.每个partition的数据量不一样，后面会导致每个处理partition的task要处理的数据量不一样，
    这时候很容易出现**数据倾斜**
    比如说，有一个partition的数据量是100，而另一个partition的数据量是900，
    在task处理逻辑一样的情况下，不同task要处理的数据量可能差别就到了9倍，甚至10倍以上，
    同样导致速度差别在9倍或者10倍以上
    这样就是导致了有的task运行的速度很快，有的运行的很慢，这就是数据倾斜。
解决：
    针对以上问题，我们希望把partition压缩，因为数据量变小了，partition完全可以对应的变少，
    比如原来4个partition，现在可以变成2个partition，那么就只要用后面的2个task来处理，
    不会造成task资源的浪费(不必要针对只有一点点数据的partition来启动一个task进行计算)
    避免了数据倾斜的问题
</code></pre><h5 id="3-算子调优之使用foreachPartition优化写入数据库性能"><a href="#3-算子调优之使用foreachPartition优化写入数据库性能" class="headerlink" title="3.算子调优之使用foreachPartition优化写入数据库性能"></a>3.算子调优之使用foreachPartition优化写入数据库性能</h5><pre><code>默认的foreach有哪些缺点？
    首先和map一样，对于每条数据都要去调一次function，task为每个数据，都要去执行一次task；
    如果一个partition有100万条数据，就要调用100万次，性能极差！
    如果每条数据都要创建一个数据库连接，那么就要创建100万个数据库连接，
    但是数据库连接的创建和销毁都是非常耗性能的，虽然我们用了数据库连接池，只要创建固定数量的连接，
    还是得多次通过数据库连接，往数据库里(mysql)发送一条sql语句，mysql需要去执行这条sql语句，
    有100万条数据，那么就是要发送100万次sql语句；
用了foreachPartition以后，有哪些好处？
    1.对于我们写的函数就调用一次就行了，一次传入一个partition的所有数据
    2.主要创建或者获取一个数据库连接就可以了
    3.只要向数据里发送一条sql语句和一组参数就可以了
在实际开发中，我们都是清一色使用foreachPartition算子操作，
但是有个问题，跟mapPartitions操作一样，如果partition的数据量非常大，
比如真的是100万条，那几本就不行了！一下子进来可能会发生OOM，内存溢出的问题
一组数据的对比：
    生产环境中：
        一个partition中有1000条数据，用foreach，跟用foreachPartition，
        性能提高了2~3分；
数据库里是：
    for循环里preparestatement.addBatch
    外面是preparestatement.executeBatch
</code></pre><h5 id="4-算子调优之repartition解决SparkSQL低并行度的问题"><a href="#4-算子调优之repartition解决SparkSQL低并行度的问题" class="headerlink" title="4.算子调优之repartition解决SparkSQL低并行度的问题"></a>4.算子调优之repartition解决SparkSQL低并行度的问题</h5><pre><code>并行度： 我们是可以自己设置的
    1.spark.default.parallelism
    2.sc.textFile(),第二个参数传入指定的数量(这个方法用的非常少)
在生产环境中，我们是要自己手动设置一下并行度的，官网推荐就是在spark-submit脚本中，
指定你的application总共要启动多少个executor，100个，每个executor多少个cpu core，
2~3个，假设application的总cpu core有200个；
官方推荐设置并行度要是总共cpu core个数的2~3倍，一般最大值，所以是 600；
设置的这个并行度，在哪些情况下生效？哪些情况下不生效？
    1.如果没有使用SparkSQL(DataFrame)的话，那么整个spark应用的并行度就是我们设置的那个并行度
    2.如果第一个stage使用了SparkSQL从Hive表中查询了一些数据，然后做了一些transformatin的操作，
    接着做了一个shuffle操作(groupByKey)；下一个stage，在shuffle之后，做了一些transformation的操作
    如果Hive表对应了20个block，而我们自己设置的并行度是100，
    那么第一个stage的并行度是不受我们控制的，就只有20个task，第二个stage的才是我们设置的并行度100个
问题出在哪里了？
    SparkSQL 默认情况下，我们是没办法手动设置并行度的，所以可能造成问题，也可能不造成问题，
    SparkSQL后面的transformation算子操作，可能是很复杂的业务逻辑，甚至是很复杂的算法，
    如果SparkSQL默认的并行度设置的很少，20个，然后每个task要处理为数不少的数据量，
    还要执行很复杂的算法，这就导致第一个stage特别慢，第二个stage 1000个task，特别快！
解决办法：
    repartition：
        使用SparkSQL这一步的并行度和task的数量肯定是没办法改变了，但是可以将SparkSQL查出来的RDD，
        使用repartition算子进行重新分区，比如分多个partition，20 -&gt; 100个；
        然后从repartition以后的RDD，并行度和task数量，就会按照我们预期的来了，
        就可以避免在跟SparkSQL绑定在一起的stage中的算子，只能使用少量的task去处理大量数据以及复杂的算法逻辑
</code></pre><p>5.算子操作reduceByKey：</p>
<pre><code>reduceByKey相较于普通的shuffle操作(不如groupByKey)，他的一个特点就是会进行map端的本地聚合；
对map端给下个stage每个task创建的输出文件中，写数据之前，就会进行本地的combiner操作，也就是多每个key的value，都会执行算子函数(_+_)，减少了磁盘IO，较少了磁盘空间的占用,在reduce端的缓存也变少了
</code></pre><h2 id="11-troubleshooting之控制reduce端缓冲大小以避免内存溢出-OOM"><a href="#11-troubleshooting之控制reduce端缓冲大小以避免内存溢出-OOM" class="headerlink" title="11.troubleshooting之控制reduce端缓冲大小以避免内存溢出(OOM)"></a>11.troubleshooting之控制reduce端缓冲大小以避免内存溢出(OOM)</h2><pre><code>new SparkConf().set(&quot;spark.reducer.maxSizeInFlight&quot;,&quot;24&quot;) //默认是48M
Map端的task是不断地输出数据的，数据量可能是很大的，
    但是其reduce端的task，并不是等到Map端task将属于自己的那个分数据全部写入磁盘后，再去拉取的
    Map端写一点数据，reduce端task就会去拉取一小部分数据，立刻进行后面的聚合，算子函数的应用；
    每次reduce能够拉取多少数据，是由reduce端buffer来定，因为拉取过来的数据都是放入buffer中的，
    然后采用后面的executor分配的堆内存占比(0.2),去进行后续的聚合，函数操作
reduce端buffer 可能会出现什么问题？
    reduce端buffer默认是48M，也许大多时候，还没有拉取满48M，也许是10M，就计算掉了，
    但是有时候，Map端的数据量特别大，写出的速度特别快，reduce端拉取的时候，全部到达了自己缓冲的最大极限48M，全部填满，
    这个时候，再加上reduce端执行的聚合函数代码，可能会创建大量的对象，也许一下子内存就撑不住了，就会造成OOM，reduce端的内存就会造成内存泄漏
如何解决？
    这个时候，我们应该减少reduce端task缓冲的大小，我们宁愿多拉取几次，但是每次同时能拉取到reduce端每个task的数据量比较少，就不容易发生OOM，比如调成12M；
    在实际生产中，这种问题是很常见的，这是典型的以性能换执行的原理，
    reduce的缓冲小了，不容易造成OOM了，但是性能一定是有所下降的，你要拉取的次数多了，
    就会走更多的网络IO流，这时候只能走牺牲性能的方式了；
曾经一个经验：
    曾经写了一个特别复杂的spark作业，写完代码后，半个月就是跑步起来，里面各种各样的问题，
    需要进行troubleshooting，调节了十几个参数，其中里面就有reduce端缓冲的大小，最后，
    总算跑起来了！
</code></pre><h3 id="12-troubleshooting之解决JVM-GC导致的shuffle拉取文件失败："><a href="#12-troubleshooting之解决JVM-GC导致的shuffle拉取文件失败：" class="headerlink" title="12. troubleshooting之解决JVM GC导致的shuffle拉取文件失败："></a>12. troubleshooting之解决JVM GC导致的shuffle拉取文件失败：</h3><pre><code>过程：
    第一个stage的task输出文件的同时 ，会像Driver上记录这些数据信息，然后下一个stage的task想要得到上个stage的数据，
    就得像Driver所要元数据信息，然后去像上一个的stage的task生成的文件中拉取数据。
问题场景：
    在spark作业中，有时候经常出现一种情况，就是log日志报出：shuffle file not found..,
    有时候他会偶尔出现一次，有的时候出现一次后重新提交stage、task，重新执行一遍 就好了。
分析问题：
    executor在JVM进程中，可能内存不太够用，那么此时就很可能执行GC，minor gc 或者 full gc，
    总之一旦发生gc后，就会导致所有工作线程全部停止，比如BlockManager，基于netty的网络通信。
    第二个stage的task去拉取数据的时候，上一个executor正好在进行gc，就导致拉取了半天也没拉取到数据，
    那为什么第二次提交stage的时候，就又可以了呢？
        因为第二次提交的时候，上一个executor已经完成了gc。
解决：
    spark.shuffle.io.maxRetries 3[默认3次]
        shuffle 文件拉取时，如果没有拉取到，最多或者重试几次，默认3次
    spark.shuffle.io.retryWait 5s [默认5s]
        每一次重新拉取文件的时间间隔，默认5s
    默认情况下，第一个stage的executor正在漫长的full gc，第二个stage的executor尝试去拉取数据，
    结果没拉取到，这样会反复重试拉取3次，中间间隔时间5s，也就是总共15s，拉取不成功，就报 shuffle file not found
        我们可以增大上面两个参数的值：
            spark.shuffle.io.maxRetries 60次
            spark.shuffle.io.retryWait 60s
            最多可以忍受一个小时没有拉取到shuffle file，这只是一个设置最大的可能值，
            full gc 也不可能一个小时都没结束把，
            这样就解决了因为gc 而无法拉取到数据的问题
</code></pre><h3 id="13-troubleshooting之解决yarn-cluster模式的JVM栈内存溢出问题"><a href="#13-troubleshooting之解决yarn-cluster模式的JVM栈内存溢出问题" class="headerlink" title="13. troubleshooting之解决yarn-cluster模式的JVM栈内存溢出问题"></a>13. troubleshooting之解决yarn-cluster模式的JVM栈内存溢出问题</h3><pre><code>yarn-cluster运行流程：
    1.本地机器执行spark-submit脚本[yarn-cluster模式]，提交spark application给resourceManager
    2. resourceManager找到一个节点[nodeManager]启动applicationMaster[Driver进程]
    3. applicationMaster找resourceManager申请executor
    4. resourceManager分配container(内存+cpu)
    5. applicationMaster找到对应nodeManager申请启动executor
    6. nodeManager启动executor
    7. executor找applicationMaster进行反向注册
    到这里为止，applicationMaster(Driver)就知道自己有哪些资源可以用(executor)，
    然后就会去执行job，拆分stage，提交stage的task，进行task调度，
    分配到各个executor上面去执行。
yarn-client 和 yarn-cluster的区别：
    yarn-client模式Driver运行在本地机器上；yarn-cluster模式Driver是运行在yarn集群上的某个nodeManager节点上的；
    yarn-client模式会导致本地机器负责spark作业的调用，所以网卡流量会激增，yarn-cluster没有这个问题；
    yarnclient的Driver运行在本地，通常来说本地机器和yarn集群都不会在一个机房，所以性能不是特别好；
    yarn-cluster模式下，Driver是跟yarn集群运行在一个机房内，性能上也会好很好；
实践经验碰到的yarn-cluster的问题：
    有时候运行了包含spark sql的spark作业，可能会遇到 在yarn-client上运行好好地，在yarn-cluster模式下，
    可能无法提交运行，会报出JVM的PermGen(永久代)的内存溢出-OOM；
    Yarn-client模式下，Driver是运行在本地机器的，spark使用的JVM的PerGen的配置，是本地的spark-class文件，
    (spark客户端是默认有配置的),JVM的永久代大小默认是128M，这个是没问题的；
    但是在Yarn-cluster模式下，Driver是运行在yarn集群的某个节点上的，使用的是没有经过配置的默认设置82M(PerGen永久代大小)
    spark sql内部会进行很负责的sl语义解析、语法树的转换，特别复杂，在这种情况下，如果sql特别复杂，
    很可能会导致性能的消耗，内存的消耗，可能对PermGen永久代的内存占比就很大
    所以此时，如果对PermGen的内存占比需求多与82M，但是又小于128M，就会出现类似上面的情况，
    yarn-client可以正常运行因为他的默认permgen大小是128M，但是yarn-cluster的默认是82M，就会出现PermGen OOM -- PermGen out of memory
解决：
    spark-submit脚本中加入参数：
        --conf spark.driver.extraJavaOptions=&apos;-XX:PermSize=128M -XX:MxPermSize=256M&apos;
        这样就设置了永久代的大小默认128M，最大256M，那么这样的话，就可以保证spark作业不会出现上面的PermGen out of memory
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-ID-Mapping" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/ID-Mapping/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/ID-Mapping/">
        解密大数据ID-Mapping
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>集奥聚合带你解密大数据ID-Mapping<br>来源：数据观 时间：2016-06-27 14:46:24 作者：集奥聚合<br>　　谈到大数据，有一个非常基本但又关键的环节就是ID-Mapping（Identifier -Mapping）。ID-Mapping通俗的说就是把几份不同来源的数据，通过各种技术手段识别为同一个对象或主体，例如同一台设备（直接），同一个用户（间接），同一家企业（间接）等等，可以形象地理解为用户画像的“拼图”过程。一个用户的行为信息、属性数据是分散在很多不同的数据来源的，因此从单个数据来看，都相当于“盲人摸象”，看到的只是这个用户一个片面的画像，而ID-Mapping能把碎片化的数据全部串联起来，消除数据孤岛，提供一个用户的完整信息视图，同时让某一个领域的数据在另一个领域绽放出巨大的价值。<br>　　ID-Mapping有非常多的用处，比如跨屏跟踪和跨设备跟踪，将一个用户的手机、PC、平板等设备的上的行为信息串联到一起。再比如这两年非常热的程序化交易，它的一个重要环节就是要把当前广告请求的用户和第一方DMP平台里的用户历史兴趣数据匹配起来。可以说，没有ID-Mapping，程序化交易就变成了盲目投放，它的实时竞价，精准投放的优势也就不存在了。<br>　　ID-Mapping既然有这么大的作用，那么应该如何做好ID-Mapping呢？这个环节不是一个简单的按照Key匹配的过程，集奥聚合作为领先的第三方大数据公司，研发了多项ID-Mapping 的独家技术，用新的匹配技术和算法模型来重塑了ID-Mapping过程。据粗略评估，集奥聚合ID-Mapping系统有能力把十几个数据源的 56亿ID（Identifier，即标识符）匹配到一起，准确率达到 95%以上，有效用户总量提升了30%，平均每个用户的标签量提升200%以上。值得注意的是，这里的Identifier是指标识符，而并非Identity（身份信息），集奥聚合可在完全脱敏，不（也无需）识别、指出用户姓甚名谁的身份信息的情况下合法地将标识符对应至某匿名用户。<br>　　简单来说，集奥聚合ID-Mapping体系有三个层面。<br>　　第一个层面是物理Mapping<br>　　这是最单纯基本的层面，也就是如何精准地记录和标识一个用户，例如利用硬件设备码生成一个统一的设备码，利用一些强账号来标识用户等等。这个层面上主要的技术难度在于ID的稳定性、唯一性和持久性。<br>　　第二个层面是基于用户行为做迭代滚动Mapping<br>　　由于原始数据存在噪音，同一个用户的多份数据、多种ID之间是“多对多”的关系。那么哪些ID是可信的呢？<br>　　我们设计了一个置信度传播的机器学习图模型来帮助确定哪些身份ID是可信的。</p>
<p>　　算法示意图如上，每个节点是一个UID或QQ号或GID等标识的潜在的“用户”<br>　　 一开始节点之间关系的概率是随机的<br>　　 其中总有两个ID的关系是强置信的prior<br>　　 迭代收敛后，哪些ID是归属于同一个用户的标识符被识别出来<br>　　大体来说，这个算法的过程是给每一个ID，以及两个ID，如IMEI和邮箱之间的pair关系都有一个预设的置信度。而所有的ID根据两两关联构成了一张图，那么每个ID的置信度根据这张网的结构传播给相关联的ID，同时也从其他ID那边接收置信度，而pair关系的置信度不变。当算法迭代收敛时，高置信度的ID就是可信的。同一个子图内的ID就标识了同一个用户。用类似的算法，我们也可以评价每个数据源的质量等。<br>　　第三个层面是基于用户兴趣做相似用户的合并<br>　　如果说层面二主要还在判断标识一个用户的ID是否正确，那么层面三致力于把行为相似的用户给合并起来。<br>　　例如，某一个用户的设备多次连接同一个Wi-Fi网关，但是每次链接都会随机更换ID，那么相当于这个用户的数据“分裂”在多个不同ID下。那么如何把这些ID合并成同一个用户呢？<br>　　除了上述做法之外，集奥聚合开发了相似用户合并技术。基于用户的上网时间偏好、网址访问偏好、点击行为偏好、浏览行为偏好、APP偏好和社交账号偏好等，为每个用户提取了上千个特征之后，进行相似用户的聚类。<br>　　聚类中选择类中心附近的用户，再加上一些辅助准则判定，就可以把用户合并起来。<br>　　经实际测试，可以把用户ID总量减少80%，同时保持用户合并的准确率在91%以上。使用的历史数据时间窗越长就越精准。仅此一项就能让用户的标签密度提升 500%。最早出现于安卓<br>深入浅出理解 Cookie Mapping<br>Posted on 2014 年 11 月 9 日 by Abbo<br>在RTB（实时竞价广告，Real-Time-Bidding）广告领域（当然实际上不仅仅是这个领域），有一个常见的词汇叫 Cookie Mapping（Cookie 匹配），一会又是DSP（需求方供应平台）与DSP的Cookie Mapping，一会又是DSP与Ad Exchange的Cookie Mapping，一会还有DMP（数据管理平台）与DSP的Cookie Mapping，已经完全把大家搞浑了。许多互联网广告从业者都不清楚到底什么是 Cookie Mapping，到底又是为什么要 Cookie Mapping。今天就以小小的笔记，分享大家疑问的解答。<br>用户唯一标识体系<br>在互联网中，我们有着许多标识唯一用户的技术手段，其中，最为常见的就是 Cookie 了（什么是Cookie请参看网站分析中的Cookie）。简单的多，Cookie具备几个特征：<br>•    唯一性，一个Cookie是唯一存在于一个域名下的；<br>•    归属权，一个Cookie必须属于某一个域名，且相互不能访问使用；<br>•    持久性，一个Cookie可以持久的存在于一个浏览器中。<br>正因为Cookie具备上述几个特征，也就衍生出Cookie在使用上的一些特点了，我们以DSP.COM（广告购买平台），ADX.COM（广告交易平台），DMP.COM（数据管理平台）为例，存在以下结论：<br>•    DSP.COM，ADX.COM，DMP.COM都存在各自的用户标识体系（各自定义的唯一ID标识）；<br>•    用户Abbo在上述三个产品的ID分别是dsp-1，adx-a，dmp-①，且相互不能访问使用。<br>就这样，DSP.COM，ADX.COM，DMP.COM都可以唯一的标识出用户Abbo，但他们并不能互相读取标识信息。<br>共享用户特征<br>由于客户需求，广告主在DSP.COM，ADX.COM，DMP.COM均有业务存在：<br>•    广告主使用DSP.COM进行广告投放，并且用户Abbo点击了游戏广告；<br>•    用户Abbo主动使用了DMP.COM提供的浏览器购物比价插件服务；<br>•    用户Abbo点击过位于交易平台ADX.COM上的职业学习、求职类广告；<br>刚好，DSP.COM识别出了Abbo喜欢玩游戏特征，DMP.COM识别出了Abbo是男性用户，ADX.COM识别出了Abbo是个年轻人。此时问题来了，由于三方的数据并不共享，因此对于广告主而言，仅知道dsp-1喜欢玩游戏，adx-a是年轻人，dmp-①是男性用户。广告主并不能直接知道Abbo是个喜爱玩游戏的年轻男性。<br>最终目标，我们需要不同产品体系中的用户的特征，合并绑定到一个用户上来，这也就是本文主题的关键——Cookie Mapping。<br>常见 Mapping 方式<br>我们刚刚看到，不同厂商、产品对用户都使用了不同的标识体系，诸如dsp-a，adx-a，dmp-①此类。因此，我们在Cookie Mapping中的最为基础的信息表——ID映射关系，俗称Cookie Mapping表。它负责使dsp-1，adx-a，dmp-①关联起来。<br>要使同一个用户在不同体系中关联起来，只有一个做法，那就是当用户发生行为的时候，同时能够联通多家厂商、产品。也就是出现了以下最常见的几种Mapping方式生成ID映射关系表：<br>•    用户加载网页代码时候，同时加载DSP.COM，ADX.COM，DMP.COM的代码，互相调用Mapping接口传输ID信息；（客户端Mapping）<br>•    用户加载网页代码时候，由服务端转发携带ID的请求，由ADX.COM服务器告诉DSP.COM相关ID信息。（服务端Mapping）<br>这样一来，经过大量的Mapping匹配后，不同厂商、产品之间也就自然形成了一套对应ID映射关系表格了。<br>移动端的 Mapping<br>移动终端的发展趋势，Cookie的效果已经远不如PC端了——PC端的用户上网行为，往往发生在一两款Web浏览软件（浏览器）中，而移动端App较为分散，用户行为、特征体现在更多的应用程序（App）上。况且，移动终端的唯一性，存在着更多的ID体系标识唯一用户，诸如MAC地址、iOS IDFA、Android ID等等。这些ID往往是具备一定唯一性，并且能够在不同App中共享的标识信息。因此，移动终端有时候也不需要 Mapping，如果约定俗成的使用某一类ID也是可以进行唯一用户标识的。<br>斗胆小结<br>斗胆小结本文，观点并不一定全部正确，如有不足，还请点出：<br>•    唯一标识需求将长期存在；<br>•    Cookie标识在PC端短期内（10年）不会消失；<br>•    多终端的发展，将出现更多标识体系；<br>•    Mapping ID的需求将长期存在。<br>Open-ID是一个很好的想法，也是一个很好的应用，特别是第三方开源Open-ID产品，个人觉得还是值得一</p>
<p>一点做用户画像的人生经验（一）：ID强打通</p>
<ol>
<li>背景<br>在构建精准用户画像时，面临着这样一个问题：日志采集不能成功地收集用户的所有ID，且每条业务线有各自定义的UID用来标识用户，从而造成了用户ID的零碎化。因此，为了做用户标签的整合，用户ID之间的强打通（亦称为ID-Mapping）成了迫切的需求。大概三年前，在知乎上有这样一个与之相类似的问题：如何用MR实现并查集以对海量数据pair做聚合；目前为止还无人解答。本文将提供一个可能的解决方案——如何用MR计算框架来实现大数据下的ID强打通。<br>首先，简要地介绍下Android设备常见的ID：<br>•    IMEI（International Mobile Equipment Identity），即通常所说的手机序列号、手机“串号”，用于在移动电话网络中识别每一部独立的手机等行动通讯装置；序列号共有15位数字，前6位（TAC）是型号核准号码，代表手机类型。接着2位（FAC）是最后装配号，代表产地。后6位（SNR）是串号，代表生产顺序号。最后1位（SP）一般为0，是检验码，备用。<br>•    MAC(Media Access Control)一般代指MAC位址，为网卡的标识，用来定义网络设备的位置。<br>•    IMSI（International Mobile SubscriberIdentification Number），储存在SIM卡中，可用于区别移动用户的有效信息；其总长度不超过15位，同样使用0～9的数字。其中MCC是移动用户所属国家代号，占3位数字，中国的MCC规定为460；MNC是移动网号码，最多由两位数字组成，用于识别移动用户所归属的移动通信网;MSIN是移动用户识别码，用以识别某一移动通信网中的移动用户。<br>•    Android ID是系统随机生成的设备ID 为一串64位的编码（十六进制的字符串），通过它可以知道设备的寿命（在设备恢复出厂设置或刷机后，该值可能会改变）。</li>
<li>设计<br>从图论的角度出发，ID强打通更像是将小连通图合并成一个大连通图；比如，在日志中出现如下三条记录，分别表示三个ID集合（小连通图）：<br>A   B   C<pre><code>C   D
    D   E
</code></pre>通过将三个小连通图合并，便可得到一个大连通图——完整的ID集合列表A B C D E。淘宝明风介绍了如何用Spark GraphX通过outerJoinVertices等运算符来做大数据下的多图合并；针对ID强打通的场景，也可采用类似的思路：日志数据构建大的稀疏图，然后采用自join的方式做打通。但是，我并没有选用GraphX，理由如下：<br>•    GraphX只支持有向图，而不支持无向图，而ID之间的关联关系是一个无向连通图；<br>•    GraphX的join操作不完全可控，“不完全可控”是指在做图合并时我们需要做过滤山寨设备、一对多的ID等操作，而在GraphX封装好的join算子上实现过滤操作则成本过高。<br>因而，基于MR计算模型（Spark框架）我设计新的ID打通算法；算法流程如下：打通的map阶段将ID集合id_set中每一个Id做key然后进行打散（id_set.map(id -&gt; id_set))），Reduce阶段按key做id_set的合并。通过观察发现：仅需要两步MR便可完成上述打通的操作。以上面的例子做说明，第一步MR完成后，打通ID集合为：A B C D、 C D E，第二步MR完成后便得到完整的ID集合列表A B C D E。但是，在两步MR过程中，所有的key都会对应一个聚合结果，而其中一些聚合结果只是中间结果。故而引入了key_set用于保存聚合时的key值，加入了第三步MR，通过比较key_set与id_set来对中间聚合结果进行过滤。算法的伪代码如下：<br>MR step1:<br> Map: <pre><code>input: id_set
process: flatMap id_set;
output: id -&gt; (id_set, 1)
</code></pre> Rduce:<pre><code>process: reduceByKey
output: id -&gt; (id_set, empty key_set, int_value)
</code></pre></li>
</ol>
<p>MR step2:<br>    Map:<br>        input: id -&gt; (id_set, empty key_set, int_value)<br>        process: flatMap id_set, if have id_aggregation, then add key to key_set<br>        output: id -&gt; (id_set, key_set, int_value)<br>    Reduce:<br>        process: reduceByKey<br>        output: id -&gt; (id_set, key_set, int_value)</p>
<p>MR step3:<br>    Map:<br>        input: id -&gt; (id_set, empty key_set, int_value)<br>        process: flatMap id_set, if have id_aggregation, then add key to key_set<br>        output: id -&gt; (id_set, key_set, int_value)<br>    Reduce:<br>        process: reduceByKey<br>        output: id -&gt; (id_set, key_set, int_value)</p>
<p>Filters:<br>    process: if have id_aggregation, then add key to key_set<br>    filter: if no id_aggregation or key_set == id_set<br>    distinct</p>
<ol start="3">
<li>实现<br>针对上述ID强打通算法，Spark实现代码如下：<br>case class DvcId(id: String, value: String)<br>val log: RDD[mutable.Set[DvcId]]// MR1val rdd1: RDD[(DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))] = log<br>.flatMap { set =&gt;<br> set.map(t =&gt; (t, (set, 1)))<br>}.reduceByKey { (t1, t2) =&gt;<br> t1._1 ++= t2._1<br> val added = t1._2 + t2._2<br> (t1._1, added)<br>}.map { t =&gt;<br> (t._1, (t._2._1, mutable.Set.empty[DvcId], t._2._2))<br>}// MR2val rdd2: RDD[(DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))] = rdd1<br>.flatMap(flatIdSet).reduceByKey(tuple3Add)// MR3val rdd3: RDD[(DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))] = rdd2<br>.flatMap(flatIdSet).reduceByKey(tuple3Add)// filterval rdd4 = rdd3.filter { t =&gt;<br>t._2._2 += t._1<br>t._2._3 == 1 || (t._2._1 – t._2.<em>2).isEmpty<br>}.map(</em>._2._1).distinct()<br>// flat id_setdef flatIdSet(row: (DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))) = {<br>row._2._3 match {<br> case 1 =&gt;<br>   Array((row._1, (row._2._1, row._2._2, row._2._3)))<br> case _ =&gt;<br>   row._2._2 += row._1 // add key to keySet<br>   row._2._1.map(d =&gt; (d, (row._2._1, row._2._2, row._2._3))).toArray<br>}<br>}<br>def tuple3Add(t1: (mutable.Set[DvcId], mutable.Set[DvcId], Int),<pre><code>t2: (mutable.Set[DvcId], mutable.Set[DvcId], Int)) = {
</code></pre>t1._1 ++= t2._1<br>t1._2 ++= t2._2<br>val added = t1._3 + t2._3<br>(t1._1, t1._2, added)<br>}<br>其中，引入常量1是为了标记该条记录是否发生了ID聚合的情况。<br>ID强打通算法实现起来比较简单，但是在实际的应用时，日志数据往往是带噪声的：<br>•    有山寨设备；<br>•    ID之间存在着一对多的情况，比如，各业务线的UID的靠谱程度不一，有的UID会对应到多个设备。<br>另外，ID强打通后是HDFS的离线数据，为了提供线上服务、保证ID之间的一一对应关系，应选择何种分布式数据库、表应如何设计、如何做到数据更新时而不影响线上服务等等，则是另一个需要思考的问题。</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-sparkstreaming&amp;kafka" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/sparkstreaming&kafka/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/sparkstreaming&kafka/">
        将offsets存储在HBase中
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>将offsets存储在HBase中</p>
<p>HBase可以作为一个可靠的外部数据库来持久化offsets。通过将offsets存储在外部系统中，Spark Streaming应用功能能够重读或者回放任何仍然存储在Kafka中的数据。</p>
<p>根据HBase的设计模式，允许应用能够以rowkey和column的结构将多个Spark Streaming应用和多个Kafka topic存放在一张表格中。在这个例子中，表格以topic名称、消费者group id和Spark Streaming 的batchTime.milliSeconds作为rowkey以做唯一标识。尽管batchTime.milliSeconds不是必须的，但是它能够更好地展示历史的每批次的offsets。表格将存储30天的累积数据，如果超出30天则会被移除。下面是创建表格的DDL和结构</p>
<p>对每一个批次的消息，使用saveOffsets()将从指定topic中读取的offsets保存到HBase中</p>
<p>在执行streaming任务之前，首先会使用getLastCommittedOffsets()来从HBase中读取上一次任务结束时所保存的offsets。该方法将采用常用方案来返回kafka topic分区offsets。</p>
<p>情形1：Streaming任务第一次启动，从zookeeper中获取给定topic的分区数，然后将每个分区的offset都设置为0，并返回。</p>
<p>情形2：一个运行了很长时间的streaming任务停止并且给定的topic增加了新的分区，处理方式是从zookeeper中获取给定topic的分区数，对于所有老的分区，offset依然使用HBase中所保存，对于新的分区则将offset设置为0。</p>
<p>情形3：Streaming任务长时间运行后停止并且topic分区没有任何变化，在这个情形下，直接使用HBase中所保存的offset即可。</p>
<p>在Spark Streaming应用启动之后如果topic增加了新的分区，那么应用只能读取到老的分区中的数据，新的是读取不到的。所以如果想读取新的分区中的数据，那么就得重新启动Spark Streaming应用。</p>
<p>当我们获取到offsets之后我们就可以创建一个Kafka Direct DStream</p>
<p>在完成本批次的数据处理之后调用saveOffsets()保存offsets.<br> 你可以到HBase中去查看不同topic和消费者组的offset数据</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2018 rongyuewu
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>