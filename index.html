<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">rongyuewu</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="https://github.com/smackgg/hexo-theme-smackdown">smackdown</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">rongyuewu</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">rongyuewu</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-Markdown 编辑阅读器" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/02/Markdown 编辑阅读器/" class="article-date">
  	<time datetime="2018-05-02T13:29:30.000Z" itemprop="datePublished">2018-05-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/02/Markdown 编辑阅读器/">
        Markdown 编辑阅读器
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="欢迎使用-Cmd-Markdown-编辑阅读器"><a href="#欢迎使用-Cmd-Markdown-编辑阅读器" class="headerlink" title="欢迎使用 Cmd Markdown 编辑阅读器"></a>欢迎使用 Cmd Markdown 编辑阅读器</h1><hr>
<h1 id="nihao"><a href="#nihao" class="headerlink" title="nihao"></a>nihao</h1><h2 id="nihao-1"><a href="#nihao-1" class="headerlink" title="nihao"></a>nihao</h2><h3 id="nihao-2"><a href="#nihao-2" class="headerlink" title="nihao"></a>nihao</h3><h1>232</h1>

<blockquote>
<p>nidshfisdfjksdfjksdf<br>dsfjaksdfj df<br><strong>&amp;dfsdf</strong><br>nihao</p>
</blockquote>
<p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1525278572217&amp;di=902ca4f761f90828017346ac0a110f2d&amp;imgtype=0&amp;src=http%3A%2F%2Fr4.ykimg.com%2F0541040852660CC56A0A471F0F5B03E1" alt=""></p>
<p><a href="https://imgchr.com/i/CY82sH" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2018/05/02/CY82sH.md.png" alt="CY82sH.md.png"></a></p>
<p>我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，<strong>Cmd Markdown</strong> 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown：</p>
<blockquote>
<ul>
<li>整理知识，学习笔记</li>
<li>发布日记，杂文，所见所想</li>
<li>撰写发布技术文稿（代码支持）</li>
<li>撰写发布学术论文（LaTeX 公式支持）</li>
</ul>
</blockquote>
<p><img src="https://www.zybuluo.com/static/img/logo.png" alt="cmd-markdown-logo"></p>
<p>除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载：</p>
<h3 id="Windows-Mac-Linux-全平台客户端"><a href="#Windows-Mac-Linux-全平台客户端" class="headerlink" title="Windows/Mac/Linux 全平台客户端"></a><a href="https://www.zybuluo.com/cmd/" target="_blank" rel="noopener">Windows/Mac/Linux 全平台客户端</a></h3><blockquote>
<p>请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 <i class="icon-file"></i> <strong>新文稿</strong> 或者使用快捷键 <code>Ctrl+Alt+N</code>。</p>
</blockquote>
<hr>
<h2 id="什么是-Markdown"><a href="#什么是-Markdown" class="headerlink" title="什么是 Markdown"></a>什么是 Markdown</h2><p>Markdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，<strong>粗体</strong> 或者 <em>斜体</em> 某些文字，更棒的是，它还可以</p>
<h3 id="1-制作一份待办事宜-Todo-列表"><a href="#1-制作一份待办事宜-Todo-列表" class="headerlink" title="1. 制作一份待办事宜 Todo 列表"></a>1. 制作一份待办事宜 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#13-待办事宜-todo-列表" target="_blank" rel="noopener">Todo 列表</a></h3><ul>
<li style="list-style: none"><input type="checkbox"> 支持以 PDF 格式导出文稿</li>
<li style="list-style: none"><input type="checkbox"> 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率</li>
<li style="list-style: none"><input type="checkbox" checked> 新增 Todo 列表功能</li>
<li style="list-style: none"><input type="checkbox" checked> 修复 LaTex 公式渲染问题</li>
<li style="list-style: none"><input type="checkbox" checked> 新增 LaTex 公式编号功能</li>
</ul>
<h3 id="2-书写一个质能守恒公式-LaTeX"><a href="#2-书写一个质能守恒公式-LaTeX" class="headerlink" title="2. 书写一个质能守恒公式[^LaTeX]"></a>2. 书写一个质能守恒公式[^LaTeX]</h3><p>$$E=mc^2$$</p>
<h3 id="3-高亮一段代码-code"><a href="#3-高亮一段代码-code" class="headerlink" title="3. 高亮一段代码[^code]"></a>3. 高亮一段代码[^code]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@requires_authorization</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SomeClass</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># A comment</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'hello world'</span></span><br></pre></td></tr></table></figure>
<h3 id="4-高效绘制-流程图"><a href="#4-高效绘制-流程图" class="headerlink" title="4. 高效绘制 流程图"></a>4. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#7-流程图" target="_blank" rel="noopener">流程图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">st=&gt;start: Start</span><br><span class="line">op=&gt;operation: Your Operation</span><br><span class="line">cond=&gt;condition: Yes or No?</span><br><span class="line">e=&gt;end</span><br><span class="line"></span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br></pre></td></tr></table></figure>
<h3 id="5-高效绘制-序列图"><a href="#5-高效绘制-序列图" class="headerlink" title="5. 高效绘制 序列图"></a>5. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#8-序列图" target="_blank" rel="noopener">序列图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Alice-&gt;Bob: Hello Bob, how are you?</span><br><span class="line">Note right of Bob: Bob thinks</span><br><span class="line">Bob--&gt;Alice: I am good thanks!</span><br></pre></td></tr></table></figure>
<h3 id="6-高效绘制-甘特图"><a href="#6-高效绘制-甘特图" class="headerlink" title="6. 高效绘制 甘特图"></a>6. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#9-甘特图" target="_blank" rel="noopener">甘特图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">title 项目开发流程</span><br><span class="line">section 项目确定</span><br><span class="line">    需求分析       :a1, 2016-06-22, 3d</span><br><span class="line">    可行性报告     :after a1, 5d</span><br><span class="line">    概念验证       : 5d</span><br><span class="line">section 项目实施</span><br><span class="line">    概要设计      :2016-07-05  , 5d</span><br><span class="line">    详细设计      :2016-07-08, 10d</span><br><span class="line">    编码          :2016-07-15, 10d</span><br><span class="line">    测试          :2016-07-22, 5d</span><br><span class="line">section 发布验收</span><br><span class="line">    发布: 2d</span><br><span class="line">    验收: 3d</span><br></pre></td></tr></table></figure>
<h3 id="7-绘制表格"><a href="#7-绘制表格" class="headerlink" title="7. 绘制表格"></a>7. 绘制表格</h3><table>
<thead>
<tr>
<th>项目</th>
<th style="text-align:right">价格</th>
<th style="text-align:center">数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>计算机</td>
<td style="text-align:right">\$1600</td>
<td style="text-align:center">5</td>
</tr>
<tr>
<td>手机</td>
<td style="text-align:right">\$12</td>
<td style="text-align:center">12</td>
</tr>
<tr>
<td>管线</td>
<td style="text-align:right">\$1</td>
<td style="text-align:center">234</td>
</tr>
</tbody>
</table>
<h3 id="8-更详细语法说明"><a href="#8-更详细语法说明" class="headerlink" title="8. 更详细语法说明"></a>8. 更详细语法说明</h3><p>想要查看更详细的语法说明，可以参考我们准备的 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown" target="_blank" rel="noopener">Cmd Markdown 简明语法手册</a>，进阶用户可以参考 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#cmd-markdown-高阶语法手册" target="_blank" rel="noopener">Cmd Markdown 高阶语法手册</a> 了解更多高级功能。</p>
<p>总而言之，不同于其它 <em>所见即所得</em> 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。<strong>Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。</strong> 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。</p>
<hr>
<h2 id="什么是-Cmd-Markdown"><a href="#什么是-Cmd-Markdown" class="headerlink" title="什么是 Cmd Markdown"></a>什么是 Cmd Markdown</h2><p>您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 <strong>编辑/发布/阅读</strong> Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。</p>
<h3 id="1-实时同步预览"><a href="#1-实时同步预览" class="headerlink" title="1. 实时同步预览"></a>1. 实时同步预览</h3><p>我们将 Cmd Markdown 的主界面一分为二，左边为<strong>编辑区</strong>，右边为<strong>预览区</strong>，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！</p>
<h3 id="2-编辑工具栏"><a href="#2-编辑工具栏" class="headerlink" title="2. 编辑工具栏"></a>2. 编辑工具栏</h3><p>也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 <strong>编辑区</strong> 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。</p>
<p><img src="https://www.zybuluo.com/static/img/toolbar-editor.png" alt="tool-editor"></p>
<h3 id="3-编辑模式"><a href="#3-编辑模式" class="headerlink" title="3. 编辑模式"></a>3. 编辑模式</h3><p>完全心无旁骛的方式编辑文字：点击 <strong>编辑工具栏</strong> 最右侧的拉伸按钮或者按下 <code>Ctrl + M</code>，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！</p>
<h3 id="4-实时的云端文稿"><a href="#4-实时的云端文稿" class="headerlink" title="4. 实时的云端文稿"></a>4. 实时的云端文稿</h3><p>为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 <strong>编辑工具栏</strong> 的最右侧提示 <code>已保存</code> 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。</p>
<h3 id="5-离线模式"><a href="#5-离线模式" class="headerlink" title="5. 离线模式"></a>5. 离线模式</h3><p>在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。</p>
<h3 id="6-管理工具栏"><a href="#6-管理工具栏" class="headerlink" title="6. 管理工具栏"></a>6. 管理工具栏</h3><p>为了便于管理您的文稿，在 <strong>预览区</strong> 的顶部放置了如下所示的 <strong>管理工具栏</strong>：</p>
<p><img src="https://www.zybuluo.com/static/img/toolbar-manager.jpg" alt="tool-manager"></p>
<p>通过管理工具栏可以：</p>
<p><i class="icon-share"></i> 发布：将当前的文稿生成固定链接，在网络上发布，分享<br><i class="icon-file"></i> 新建：开始撰写一篇新的文稿<br><i class="icon-trash"></i> 删除：删除当前的文稿<br><i class="icon-cloud"></i> 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地<br><i class="icon-reorder"></i> 列表：所有新增和过往的文稿都可以在这里查看、操作<br><i class="icon-pencil"></i> 模式：切换 普通/Vim/Emacs 编辑模式</p>
<h3 id="7-阅读工具栏"><a href="#7-阅读工具栏" class="headerlink" title="7. 阅读工具栏"></a>7. 阅读工具栏</h3><p><img src="https://www.zybuluo.com/static/img/toolbar-reader.jpg" alt="tool-manager"></p>
<p>通过 <strong>预览区</strong> 右上角的 <strong>阅读工具栏</strong>，可以查看当前文稿的目录并增强阅读体验。</p>
<p>工具栏上的五个图标依次为：</p>
<p><i class="icon-list"></i> 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落<br><i class="icon-chevron-sign-left"></i> 视图：互换左边编辑区和右边预览区的位置<br><i class="icon-adjust"></i> 主题：内置了黑白两种模式的主题，试试 <strong>黑色主题</strong>，超炫！<br><i class="icon-desktop"></i> 阅读：心无旁骛的阅读模式提供超一流的阅读体验<br><i class="icon-fullscreen"></i> 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境</p>
<h3 id="8-阅读模式"><a href="#8-阅读模式" class="headerlink" title="8. 阅读模式"></a>8. 阅读模式</h3><p>在 <strong>阅读工具栏</strong> 点击 <i class="icon-desktop"></i> 或者按下 <code>Ctrl+Alt+M</code> 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。</p>
<h3 id="9-标签、分类和搜索"><a href="#9-标签、分类和搜索" class="headerlink" title="9. 标签、分类和搜索"></a>9. 标签、分类和搜索</h3><p>在编辑区任意行首位置输入以下格式的文字可以标签当前文档：</p>
<p>标签： 未分类</p>
<p>标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示：</p>
<p><img src="https://www.zybuluo.com/static/img/file-list.png" alt="file-list"></p>
<h3 id="10-文稿发布和分享"><a href="#10-文稿发布和分享" class="headerlink" title="10. 文稿发布和分享"></a>10. 文稿发布和分享</h3><p>在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 <i class="icon-share"></i> (Ctrl+Alt+P) 发布这份文档给好友吧！</p>
<hr>
<p>再一次感谢您花费时间阅读这份欢迎稿，点击 <i class="icon-file"></i> (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！</p>
<p>作者 <a href="http://weibo.com/ghosert" target="_blank" rel="noopener">@ghosert</a><br>2016 年 07月 07日    </p>
<p>[^LaTeX]: 支持 <strong>LaTeX</strong> 编辑显示支持，例如：$\sum_{i=1}^n a_i=0$， 访问 <a href="http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener">MathJax</a> 参考更多使用方法。</p>
<p>[^code]: 代码高亮功能支持包括 Java, Python, JavaScript 在内的，<strong>四十一</strong>种主流编程语言。</p>
<h1>nihao</h1>


      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-随笔" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/02/随笔/" class="article-date">
  	<time datetime="2018-05-02T13:29:30.000Z" itemprop="datePublished">2018-05-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/02/随笔/">
        随笔
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>复习知识点：<br>spar全部知识点，kafka，hdfs，hive及预处理，azkaban，hbase（会搭集群，roykey，会设计建表，有哪些特性）,redis，zookeeper java基础(罗海清脑图复习)</p>
<p>在hive中:<br>select * from 表名：<br>是查询该表名的所有字段记录</p>
<p>describe formatted 表名：<br>是查看该表的详细信息，而并查看不了表中的数据</p>
<p>十大经典数据挖掘算法之一:<br>kmeans<br>    7、cos余弦相似度和欧式距离的区</p>
<p>solid converter PDF软件下载破解<br>Name: SolidConverterPDFv9<br>E-mail: <a href="mailto:user@ru.ru" target="_blank" rel="noopener">user@ru.ru</a><br>Organization: any<br>Unlock Code: KTGK</p>
<p>HTTP 协议中 URI 和 URL 有什么区别？<br>统一资源标志符URI就是在某一规则下能把一个资源独一无二地标识出来<br>统一资源定位符URL同样标识出了唯一的一个人，起到了URI的作用，所以URL是URI的子集<br>可以用身份证号是uri(包含了url)可以确定一个人,而地址也可以确定一个人</p>
<p>本质而言Kylin麒麟系统就是Ubuntu 13.04</p>
<p>艾维奇电音大师:<br>代表之作:”wake me up” “levels” “x you” “a sky full of stars” “lay me down”</p>
<p>查看某个后台进程:<br>ps aux | grep redis</p>
<p>查看所有正在使用的端口：<br>netstat -ntlp</p>
<p>idea破解教程：<a href="http://blog.csdn.net/qq_38637558/article/details/78914772" target="_blank" rel="noopener">http://blog.csdn.net/qq_38637558/article/details/78914772</a></p>
<p><a href="http://tlias-stu.boxuegu.com/#/index" target="_blank" rel="noopener">http://tlias-stu.boxuegu.com/#/index</a><br>博学谷</p>
<p>mkdir -p /export/server<br>rm -rf jdk-8u65-linux-x64.tar.gz<br>mv zookeeper-3.4.5 zookeeper</p>
<p>js自调用匿名函数：<br>(function(){})();</p>
<p>shell命令：<br>-p 表示递归<br>-f 表示覆盖原有文件目录<br>-w 表示写的命令</p>
<p>有时间研究一下matlab<br>脱敏</p>
<p>有时间研究一下在简书，csdn,51cto,主要是github和脸书发表代码和作品的流程，还有有时间玩下阿里，腾讯的服务器，买个域名练练手熟悉一下。</p>
<p>菜鸟教程<br><a href="http://www.runoob.com/linux/linux-tutorial.html" target="_blank" rel="noopener">http://www.runoob.com/linux/linux-tutorial.html</a><br><a href="http://www.runoob.com/mysql/mysql-tutorial.html" target="_blank" rel="noopener">http://www.runoob.com/mysql/mysql-tutorial.html</a></p>
<p><a href="https://www.csdn.net/nav/cloud" target="_blank" rel="noopener">https://www.csdn.net/nav/cloud</a><br><a href="http://blog.csdn.net/superzyl" target="_blank" rel="noopener">http://blog.csdn.net/superzyl</a></p>
<p>在线json生成java实体类<br><a href="https://www.bejson.com/" target="_blank" rel="noopener">https://www.bejson.com/</a></p>
<p><a href="https://www.cnblogs.com/aipan/p/7770611.html" target="_blank" rel="noopener">https://www.cnblogs.com/aipan/p/7770611.html</a></p>
<p>启用hadoop本地模式：<br>在hive中设置（常用）<br>set hive.exec.mode.local.auto=true;</p>
<p>boss直聘、<br>拉钩</p>
<p>解决面试题中的脑筋急转弯<br>小袁搜题<br>作业帮</p>
<p>自古评论出奇才，<br>内涵佳句随口来。<br>若是生在隋唐代，<br>哪有诗仙李太白？</p>
<p>内事问百度，外事问谷歌，床事问天涯，绿事问虎扑”</p>
<p>C:\Windows\System32\drivers\etc  hosts目录</p>
<p>31773766<br>31773767</p>
<p><a href="http://blog.csdn.net/superzyl" target="_blank" rel="noopener">http://blog.csdn.net/superzyl</a><br>周老师的CSDN博客，里面有老师总结的50列sql面试题，面试前做一做</p>
<p>hourenren 13:52:23<br>在定义变量的时候是引用,如int &amp;a = b; a为b的一个引用<br>在表达式中为取地址如int *a = &b; a位指向b整型的一个指针<br>hourenren 14:15:41<br><a href="https://ideone.com/4okUHi" target="_blank" rel="noopener">https://ideone.com/4okUHi</a><br>hourenren 14:16:07<br>在线编译C++代码</p>
<p>八爪鱼爬虫</p>
<p>datagrip 智能sql编辑器</p>
<p><a href="https://www.cnblogs.com/zhangyinhua/p/8037599.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhangyinhua/p/8037599.html</a><br>jsoup文档讲解</p>
<p><a href="https://www.boxuegu.com/course/free/" target="_blank" rel="noopener">https://www.boxuegu.com/course/free/</a><br>redis免费学习面试热点</p>
<p><a href="https://www.cnblogs.com/lizichao1991/p/7809156.html" target="_blank" rel="noopener">https://www.cnblogs.com/lizichao1991/p/7809156.html</a> </p>
<p><a href="http://blog.csdn.net/yao970953039/article/details/62047755" target="_blank" rel="noopener">http://blog.csdn.net/yao970953039/article/details/62047755</a></p>
<p> Intellij IDEA 2017 debug断点调试技巧与总结详解篇<br><a href="http://blog.csdn.net/qq_27093465/article/details/64124330" target="_blank" rel="noopener">http://blog.csdn.net/qq_27093465/article/details/64124330</a></p>
<p>import scala.collection.mutable.Map<br>scala导包</p>
<p><a href="https://www.iteblog.com/archives/1542.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/1542.html</a><br>定时在线激活idea</p>
<p>在线pdf转word网站<br><a href="http://app.xunjiepdf.com/pdf2word" target="_blank" rel="noopener">http://app.xunjiepdf.com/pdf2word</a></p>
<p>乔布简历,一个挺不错的简历样式模板网站</p>
<p>快递单号：<br>0491 2969 6061</p>
<p>某个zookeeper挂掉了，解决方案：<br>cd /export/data/zkdata<br>rm -rf v<em> zoo</em><br>再重新启动zookeeper即可</p>
<p>linux中\转义空格<br>///两个//转义/，目录</p>
<p>农历2月12号 爸爸<br>农历4月初九 妈妈<br>农历四月一号  姐姐</p>
<p>13787692647</p>
<p><a href="http://my.tv.sohu.com/us/254995980/79868928.shtml" target="_blank" rel="noopener">http://my.tv.sohu.com/us/254995980/79868928.shtml</a><br>hadoop年薪23万学员分享面试经验</p>
<p>万里面试<br>kafka 和spark 都会问的很多<br>数据库ods 层数据怎么清楚？</p>
<p>面试常见问题：<br>链接：<a href="https://pan.baidu.com/s/1hIWUx01oOcilf2O_p_Fg3Q?密码：7z7s" target="_blank" rel="noopener">https://pan.baidu.com/s/1hIWUx01oOcilf2O_p_Fg3Q?密码：7z7s</a></p>
<p>搭建外网服务器网站：<br><a href="https://www.vultr.com/?ref=7323908" target="_blank" rel="noopener">https://www.vultr.com/?ref=7323908</a><br>用crt连上后，依次输入以下三条命令：</p>
<p>wget –no-check-certificate -O shadowsocks-all.sh <a href="https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh" target="_blank" rel="noopener">https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh</a></p>
<p>chmod +x shadowsocks-all.sh</p>
<p>./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log</p>
<p>图像识别物体<br><a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet</a></p>
<p>leetcode 刷题网站</p>
<p>l kafka删除 topic<br>bin/kafka-topics.sh –delete –zookeeper zk01:2181 –topic test<br>需要 server.properties 中设置 delete.topic.enable=true 否则只是标记删除或者直接重启。</p>
<p>你可以通过命令：./bin/kafka-topics –zookeeper 【zookeeper server】  –list 来查看所有topic</p>
<p> C:\Users\RongYue.jupyter\jupyter_notebook_config.py</p>
<p> kill和kill -9 和区别（原理）<br> 有时候我们使用kill无法杀掉一个进程，但是用kill -9却可以，kill的作用是向进程发送一个信号（并没有说是杀掉进程哈）。具体发送什么信号由后面接的参数决定。<br> kill默认参数是TERM。也就是说，如果没指定具体的信号作为参数，则默认使用kill TERM pid。因此kill pid是可以杀掉一个进程<br> 大多数信号可以被捕获的。而TERM信号就是在这个大多数里的，一些进程可能为了特殊的用途捕获了TERM信号，导致了你使用kill pid时无法杀掉进程。 另外《APUE》中也强调了，有两个信号不能被捕获，SIGKILL 和SIGSTOP<br> 没错，kill -9 就是向进程发送SIGKILL信号</p>
<p>spark 的几种运行模式<br>spark的yarn管理资源不够用了怎么办<br>azkaban任务提交我只修改任务里的一部分，如何避免每次都上传zip包<br>shuffle的排序算法 归并排序<br>azkaban调度失败了该怎么检查和恢复，dug的思路<br>spark运行内存不够会发生什么，如何解决<br>你们集群中hdfs和yarn的使用率是多少<br>hive的调优，数据量大的情况和不仅仅数据倾斜</p>
<p>有没有做过cdh的升级和改造，cdh的版本<br>集群部署规划情况，内存，大小，磁盘，<br>工作中接触的数据量大小，我答500–1g</p>
<p>思维题，1亿个用户每个用户都有一个动态的打分值(1–100)，不用排序，如果快速知道排名前十的用户是谁</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-数据倾斜解决方案" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/02/数据倾斜解决方案/" class="article-date">
  	<time datetime="2018-05-02T13:29:30.000Z" itemprop="datePublished">2018-05-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/02/数据倾斜解决方案/">
        Hive 数据倾斜解决方案
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Hive-数据倾斜解决方案："><a href="#Hive-数据倾斜解决方案：" class="headerlink" title="Hive 数据倾斜解决方案："></a>Hive 数据倾斜解决方案：</h4><pre><code>1.调节参数
    hive.map.aggr=true  Map端部分聚合，相当于combiner
    hive.groupby.skewindata=true
        有数据倾斜的时候进行负载均衡，当选项设定为true，生成的查询计划会有两个mr job，
        第一个job中，map的输出结果集合会随机分不到reduce中，每个reduce做部分聚合操作，
        并输出结果，这样的处理结果是相同的groupByKey会分到不同的reduce中，从而达到敷在君更的目的；
        第二个job再根据预处理的数据结果按照GroupByKey分不到redcue，最终完成聚合
2. 小表和大表进行join操作
    使用map join让小的维度表(1000条以下的记录数)先进内存[distributedCache],在map端完成reduce

3.1 空值产生的数据倾斜
    赋予空值新的key值
        select *
        from log a
        left join users b
        on
        case when 
                a.user_id is null 
                then 
                concat(&apos;hive&apos;,rand())
                else a.user_id
                end = b.user_id
    好处：这个方法只有一个job，把空值的key变成了字符串加上随机数，就能把倾斜的数据分到不同的reduce上，
    解决数据倾斜问题。
3.2 不同数据类型关联产生数据倾斜
    比如 用户表user_id为int类型，log表中的user_id 字段既有int也有string类型，
    当按照user_id进行两个表的join操作时，默认的hash操作会按照int类型的id来进行分配，
    这样就会导致所有string类型的id记录都分配到一个reducer中
    解决：把数字类型转换成字符串类型
        select * 
        from 
            users a
        left outer join logs b
        on
        a.user_id = cast(b.user_id as string)
3.3 users表有600w+的记录，把users分发到所有的map也是不小的开销，而且map join不支持这么大的小表，
如果用普通的join，又会碰到数据倾斜的问题
    解决：
        select * from log a
            left outer join 
                (
                    select d.* from (select distinct user_id from log) c
                    join users d
                    on c.user_id = d.user_id
                ) x
            on a.user_id = x.user_id;
</code></pre><h4 id="spark解决数据倾斜："><a href="#spark解决数据倾斜：" class="headerlink" title="spark解决数据倾斜："></a>spark解决数据倾斜：</h4><pre><code>1.增加并行度，也就是增加task的个数，可以缓解数据倾斜 这样可以将分配到统一task上的key散开，
2.自定义 partition 默认是 HashPartition，这样可以将不同的key分配到多个task上，
但是也只是缓解，而且也不灵活，不能解决同一个key数据量很好的场景
3.将reduce端join变成map端join -- broadcast 
    优势：
        避免了shuffle，彻底解除了数据倾斜
    劣势：
        要求join的一侧数据集合足够小，适用于join，不适用于聚合
4.在数据倾斜的key前面加前缀，让这个key分不到不同的task中
    将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，
    另外一个RDD每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N    倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。
    最后将两次Join的结果集通过union合并，即可得到全部Join结果。
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kafka - scala 语言写的 版本 1.0.0 scala 2.11 官方推荐" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/kafka - scala 语言写的 版本 1.0.0 scala 2.11 官方推荐/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/kafka - scala 语言写的 版本 1.0.0 scala 2.11 官方推荐/">
        kafka总结相关笔记
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="kafka-scala-语言写的-版本-1-0-0-scala-2-11-官方推荐"><a href="#kafka-scala-语言写的-版本-1-0-0-scala-2-11-官方推荐" class="headerlink" title="kafka - scala 语言写的 版本 1.0.0 scala 2.11 官方推荐"></a>kafka - scala 语言写的 版本 1.0.0 scala 2.11 官方推荐</h1><p>kafka 是什么？</p>
<pre><code>1.kafka是一个消息队列(生产者消费者模式)
2.目标：构建企业中统一的、高通量、低延时的消息平台
3.大多的是消息队列（消息中间件）都是基于JMS标准实现的，Kafka类似于JMS的实现
</code></pre><p>kafka 有什么用？(消息队列有什么用？)</p>
<pre><code>作为缓冲，来异构、解耦系统
    a. 用户注册需要多个步骤，每个步骤执行都需要很长时间，代表用户等待时间是所有步骤的累计时间
    b. 为了减少用户等待的时间，使用并行执行，有多少步骤，就开启多少个线程来执行
        代表用户等待时间是所有步骤中耗时最多的那个步骤时间
    c. 问题：开启多个线程执行每个步骤，如果以一个步骤执行异常，或者严重超时，
        用户的等待时间就不可控了
使用消息队列来保证
    1.注册时，立刻返回成功
    2.发送注册成功的消息到消息平台
    3.对注册信息感兴趣的程序，可以消费消息
</code></pre><h3 id="kafka的基本架构"><a href="#kafka的基本架构" class="headerlink" title="kafka的基本架构"></a>kafka的基本架构</h3><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/x.psMta.aW44s9upJo9IrOikdwcFORL9vPWx0WhaZgA!/b/dFYBAAAAAAAA&amp;bo=CgMQAgAAAAARBys!&amp;rf=viewer_4" alt=""><br>    kafka cluster：由多个服务器组成，每个服务器单独的名字broker(server)<br>    kafka producer：生产者、负责生产数据<br>    kafka consumer：消费者、负责消费数据<br>    kafka topic：主题，一类消息的名称。存储数据时将一类数据存放在某个topic下，消费数据也是消费一类数据<br>        订单系统：创建一个topic，叫做order<br>        用户系统：创建一个topic，叫做user<br>        商品系统：创建一个topic，叫做product<br><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/lmVbiS8e*xd5wkNzFNMQTFPDCKYrpGIGlMDPkcySlww!/b/dDEBAAAAAAAA&amp;bo=PgPcAAAAAAARB9E!&amp;rf=viewer_4" alt=""></p>
<h5 id="配置kafka需要修改配置文件的三个地方："><a href="#配置kafka需要修改配置文件的三个地方：" class="headerlink" title="配置kafka需要修改配置文件的三个地方："></a>配置kafka需要修改配置文件的三个地方：</h5><pre><code>1.broker.id
2.数据存放的目录，注意：目录如果不存在，需要新建
3.zookeeper的地址信息
</code></pre><h5 id="查看kafka集群"><a href="#查看kafka集群" class="headerlink" title="查看kafka集群"></a>查看kafka集群</h5><pre><code>由于kafka集群没有UI界面，需要借助外部工具，来查看kafka的集群
这个工具是一个java程序，必须要安装好jdk --- ZooInspector
</code></pre><h5 id="1-创建一个订单的topic。"><a href="#1-创建一个订单的topic。" class="headerlink" title="1) 创建一个订单的topic。"></a>1) 创建一个订单的topic。</h5><pre><code>bin/kafka-topics.sh --create --zookeeper zk01:2181 --replication-factor 1 --partitions 1 --topic order
</code></pre><h5 id="2）编写代码启动一个生产者，生产数据"><a href="#2）编写代码启动一个生产者，生产数据" class="headerlink" title="2）编写代码启动一个生产者，生产数据"></a>2）编写代码启动一个生产者，生产数据</h5><pre><code>bin/kafka-console-producer.sh --broker-list kafka01:9092 --topic order
</code></pre><h5 id="3）-编写代码启动给一个消费者，消费数据"><a href="#3）-编写代码启动给一个消费者，消费数据" class="headerlink" title="3）    编写代码启动给一个消费者，消费数据"></a>3）    编写代码启动给一个消费者，消费数据</h5><pre><code>bin/kafka-console-consumer.sh --zookeeper zk01:2181 --from-beginning --topic order
</code></pre><h3 id="kafka原理"><a href="#kafka原理" class="headerlink" title="kafka原理"></a>kafka原理</h3><h4 id="1-分片与副本机制"><a href="#1-分片与副本机制" class="headerlink" title="1.分片与副本机制"></a>1.分片与副本机制</h4><pre><code>分片：
    当数据量非常大的时候，一个服务器存放不了，就将数据分成两个或者多个部分，
    存放在多台服务器上。每个服务器上的数据，叫做一个分片。
        问题：
            如果一个partition中有10T数据，如何存放？是放在一个文件还是多个文件？
                kafka的解决方案是多个文件！
                这里说的多个文件就是segment段
                    [我们在kafka配置文件中配置的数据存储目录：/export/data/kafka]
                    里面有topicname-index [如：order-0,意思是order这个topic的第0个副本]
                    这个order-0目录下就是存放着诸如：
                        00000000000000000.index
                        00000000000000000.log
                        segment段包含了这两个文件，segment段默认是1G大小
                        segment段中有两个核心的文件.log和.index，当log文件等于1G的时候，
                        新的数据会写入到下个segment中，同时我们也可以看到segment段中有.timeindex文件生成,
                        而且通过查看，可以看到一个segment段差不多会存储70万条数据。
</code></pre><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/hn5PDC6zOwPd68flbGfMrNXsek.8DkiFksRzxPIWngA!/b/dEEBAAAAAAAA&amp;bo=hAMCAgAAAAARB7c!&amp;rf=viewer_4" alt=""><br>                    如上图所述：</p>
<pre><code>                *Segment文件命名规则：
                    partition全局的第一个segment从0开始，后续每个sgment文件名为上一个segment文件最后一条消息的offset值。
                    数值最大的为64位long大小，19位数字字符长度，没有数字用0填充。
                *索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中
                message的物理偏移地址
                结论：
                    kafka查找segment file 只需要两步
                        1.先找到这个数据对应的segment[通过查看segment的区间，offset在哪个segment段中]
                        2.根据某个segment段中的.index 索引文件中查找该条数据所在.log文件中的位置
        kafka为什么要对文件进行切分，保存多个文件中？
            kafka作为消息中间件，只负责消息的临时存储，并不是永久存储，
            需要删除过期的数据。
            如果将所有的数据都存放在一个文件中，要删除过期数据的时候，就麻烦了。
            因为文件有日期属性，删除过期数据，只需要根据文件的日属性删除就好了

副本：
    当数据只保存一份的时候，有丢失的风险，为了更好的容错和容灾，
    将数据拷贝几份，保存到不同的机器上。
</code></pre><h4 id="kafka生产数据的分发策略"><a href="#kafka生产数据的分发策略" class="headerlink" title="kafka生产数据的分发策略"></a>kafka生产数据的分发策略</h4><pre><code>kafka在生产数据的实惠，有一个数据分发策略。默认的情况使用DefaultPartitoner.class类
这个类就定义数据分发的策略。
    1.如果用指定partition，生产就不会条用DefaultPartitoner.partition()方法，直接发到指定的分区
    [这种不常用！]
    public ProducerRecord(String topic, Integer partition, K key, V value) {
        this(topic, partition, null, key, value, null);
    }

    2.当用户指定key，使用hash算法。如果key一直不变，同一个key算出来的hash值是一个固定值。
    如果是固定值，这种hash取模就没意义。
    public ProducerRecord(String topic, K key, V value) {
        this(topic, null, null, key, value, null);
    }

    3.当用户既没有指定partition也没有指定key时，使用轮询[round-robin]的方式发送数据
    public ProducerRecord(String topic, V value) {
        this(topic, null, null, null, value, null);
    }
</code></pre><h3 id="kafka-消费者的负载均衡"><a href="#kafka-消费者的负载均衡" class="headerlink" title="kafka-消费者的负载均衡"></a>kafka-消费者的负载均衡</h3><pre><code>举例说明：[问题重现]
    当每秒钟有400条数据过来，分了3个partition来存储，但是只有1个消费者并且每秒能消费100条，这样的话，生产者的速度很快，但是消费者跟不上，怎么办？
    造成了数据大量滞后和延时！
解决：
    多几个消费者，共同来消费数据
        比如3个消费者来共同消费数据这样就解决了。
新的问题：
    消费组中消费者的数量和partition的数量一致，但是消费者消费的熟读还是跟不上[比如每个消费者只能消费100条]，怎么办？
        再加个消费者吗？ 答案是 no！！！！
        因为根据kafka负载均衡策略规定，多出来的笑着是处于空闲状态的！
    也就是1个partition只能被1个消费者消费
真是解决办法：
    要么修改topic的partition数量；
    要么减少消费者处理时间，提高处理速度；
</code></pre><h4 id="kafka消息不丢失机制："><a href="#kafka消息不丢失机制：" class="headerlink" title="kafka消息不丢失机制："></a>kafka消息不丢失机制：</h4><p> 1.producer端消息不丢失机制：</p>
<pre><code>如果有多个副本，就需要选择一个leader出来，负责消息的读写请求。
比如：
    有一条数据经过partitioner.class计算把数据发送给了broker2[producerRecord ---&gt; 2]
    关于ack的响应有3个状态值：
        0：生产者只管发数据，并不关心数据是否丢失
        1：partition的leader收到数据后，就返回响应码状态
        -1：所有的从节点和leader都收到数据后，才返回响应码状态
    问题：
        如果broker端一直不给ack状态码，producer永远不知道是否成功。
            producer可以设置一个超时时间  10s，超过时间就认为失败。
    问题又来了：
        如果一条消息发送一次，得到一次ack相应，在大量数据情况下会占用很多带宽怎么办？
    解决：
        生产者将数据线缓存到producer端，达到一定的数量阈值或者时间阈值之后发送
        [比如：设置缓冲池中可以放2万条数据，或者等待时间设置成500ms]
    问题：
        如果设置buffer，按照500条每个批次发送数据到broker，但是broker迟迟不给相应，buffer中的数据如何处理？
        [而且producer端还源源不断的生产数据，这时候就造成了阻塞情况]
    解决：
        可以对buffer进行设置，如果满了，并不确定是否发送，
        如果需要继续生产数据，就可以选择buffer清空，或者不清空 [消息不丢失，一般会设置不清空]
    同步模式和异步模式：[异步就是没有设置缓冲池 buffer]
        在同步模式下：
            1. 生产者等待10s，如果broker没有给出ack响应，就认为失败。
            2. 生产者重置3次，如果还没响应，就报错。
        在异步模式下：[认为设置]
            1. 现将数据保存在生产者端的buffer中。buffer大小是2万条
            2. 满足数据阈值或者数量阈值其中的一个条件就可以发送数据
            3. 发送一批数据的大小是500条
        如果broker迟迟不给ack，而buffer又满了，开发者自己设置是否直接清空buffer中的数据。
</code></pre><p>2.broker端消息不丢失机制：</p>
<pre><code>broker端的消息不丢失，其实就是用partition副本机制来保证的。
</code></pre><p>3.consumer端消息不丢失机制：</p>
<pre><code>partition中有个segment段，log和index文件，log文件存放的是消息本身。
index文件存放的是消息offset值和存放在log文件的哪儿文件。
问题：[在kafka 0.8版本之前consumer消费数据的offset值是保存在zookeeper上的，但是
        这样会导致一种现象，就是consumer已经消费完了，但是等还没把offset值保存到zookeeper
        上的时候，consumer挂了，再次重启后，那么就会出现**重复**消费的问题]
解决：
    kafka从0.8版本以后，offset的值是保存在了kafka的内置topic上，
    这样就不会造成重复消费的问题了
</code></pre><h4 id="补充："><a href="#补充：" class="headerlink" title="补充："></a>补充：</h4><pre><code>Consumer Group [CG]：
    kafka用来实现一个topic消息的广播(发给所有的consumer)和单播(发给任意一个consumer)的手段
    一个topic可以有多个consumer group。topic的消息会复制(不是真的复制，只是概念上的)到所有的CG
    但每个partition只会把消息发给该CG中的一个consumer。
    用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic
broker：
    一台kafka服务器就是一个broker，一个集群由多个broker组成。
    一个broker可以容纳多个topic。
Partition：
    为了负债均衡
    kafka只保证按照一个partition中的顺序将消息发给consumer，
    不保证一个topic的整体（多个partition）的顺序。
leader：
    每一个replication集合中的partition都会选出一个唯一的leader，所有的读写请求都是由leader处理，
    其他的replicas从leader处把数据更新同步到本地。每个cluster当中会选举出一个broker来担任controller，
    负责处理partition的leader选举，协调partition迁移等工作。
ISR（In-Sync-Replica）：
    是Replicas的一个子集，表示目前Alive且与leader能够‘catch-up’的replicas集合。
    由于读写都是首先落到leader上，
    所以一般来说通过同步机制从leader上拉取数据的replica都会和leader有一些延迟
    [包括延迟时间和延迟条数2个维度]
    任意一个超过阈值都会把该replica提出ISR。每个Partition都有他自己独立的ISR。
</code></pre><h5 id="配置文件当中配置的kafka多久删除数据"><a href="#配置文件当中配置的kafka多久删除数据" class="headerlink" title="配置文件当中配置的kafka多久删除数据"></a>配置文件当中配置的kafka多久删除数据</h5><pre><code>The minimum age of a log file to be eligible for deletion
log.retention.hours=168 
定时检查周期，发现数据存了超过上面配置的时间，就干数据
log.retention.check.interval.ms=30000
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-JVM GC " class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/JVM GC /" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/JVM GC /">
        JVM总结相关笔记
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="JVM"><a href="#JVM" class="headerlink" title="JVM"></a>JVM</h2><h4 id="JVM的参数设置"><a href="#JVM的参数设置" class="headerlink" title="JVM的参数设置"></a>JVM的参数设置</h4><pre><code>1. -Xms 初始堆大小  -Xmx 最大堆大小  [一般这两个值设置的是一样的，防止GC后出现内存抖动]
2. -Xmn 年轻代大小
    a) 整个堆的大小 = 年轻代大小 + 老年代大小 + 持久代大小
    b) 持久代一般固定大小为64M
    c) 所以，增大年轻代后，将会较小老年代的大小。这个值对系统性能影响较大，
       Sun官方推荐配置为整个堆的3/8
3. -XX:NewSize 初始年轻代大小     -XX:MaxNewSize 最大年轻代大小
4. -XX:NewRatio 老年代和年轻代的比值
5. -XX：SurvivorRatio 设置年轻代中Eden区与Survivor区的大小比值 
   [默认是8:1:1,比如设置为6，那么就是 6:2:2]
6. -XX:MaxTenuringThreshold 设置年轻代的对象被回收多少次后才进入老年代，默认15次
    [控制对象在经过多少次minor GC后进入老年代，此参数只在Serial串行GC时有效]
7. -XX:PermSize 初始持久代大小  -XX:MaxPermSize 持久代最大值 
    [这个跟 -Xms 和 -Xmx 堆大小设置一样，都是相等，为了防止内存抖动]
</code></pre><h4 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h4><pre><code>JVM调优主要针对内存管理方面，包括控制各个代的大小，GC策略。
由于GC开始垃圾回收时会挂起应用线程，严重影响性能，调优的目的就是
为了尽量降低GC所导致的应用线程暂停时间、减少Full GC的次数。
</code></pre><p> 代调优：</p>
<pre><code> 1) 避免新生代大小设置过小
     当新生代设置过小时，会产生两种比较明显的现象:
         一是minor GC次数频繁
         二是可能导致minor GC对象直接进入老年代。当老年代内存不足时，会出发Full GC。
2) 避免新生代设置过大
    新生代设置过大，会带来两个问题：
        一是老年代变小，可能导致Full GC频繁执行；
        二是minor GC 执行回收的时间大幅度增加
那怎么选择年轻代的大小呢？[分不同的应用场景]
    a. 响应时间优先的应用：
        尽可能设大，知道接近系统的最低响应时间限制(根据实际情况选择)。
</code></pre><p><img src="/Users/Macx/Desktop/整理笔记/jvm-gc.png" alt=""><br>说明：新new的对象会首先会进入年轻代的Eden中（如果对象太大可能直接进入年老代），在GC之前对象是存在Eden和from中的，进行GC的时候Eden中的对象被拷贝到To这样一个survive空间（survive（幸存）空间：包括from和to，他们的空间大小是一样的，又叫s1和s2）中（有一个拷贝算法），From中的对象（算法会考虑经过GC幸存的次数）到一定次数 阈值（如果说每次GC之后这个对象依旧在Survive中存在，GC一次他的Age就会加1，默认15就会放到OldGeneration。但是实际情况比较复杂，有可能没有到阈值就从Survive区域直接到Old Generation区域。)</p>
<h4 id="1-哪些需要回收-—-gt-java堆内存、方法区内存"><a href="#1-哪些需要回收-—-gt-java堆内存、方法区内存" class="headerlink" title="1.哪些需要回收? —-&gt; java堆内存、方法区内存"></a>1.哪些需要回收? —-&gt; java堆内存、方法区内存</h4><h4 id="2-什么时候回收？-—–-gt"><a href="#2-什么时候回收？-—–-gt" class="headerlink" title="2.什么时候回收？ —–&gt;"></a>2.什么时候回收？ —–&gt;</h4><pre><code>2.1：引用计数法【引用count+1，引用失效时count-1，为0时不被引用】
【如果两个对象相互引用而且都没有被使用了，那么会造成内存泄漏】。

2.2：可达性分析【从根节点搜索，如果没有搜索到就是没有被使用，所以互相引用且搜索不到的也会被清除】
</code></pre><h4 id="怎么回收？—-gt-垃圾回收算法-3种"><a href="#怎么回收？—-gt-垃圾回收算法-3种" class="headerlink" title="怎么回收？—-&gt; 垃圾回收算法[3种]"></a>怎么回收？—-&gt; 垃圾回收算法[3种]</h4><h4 id="1-标记清除算法-Mark-Sweep"><a href="#1-标记清除算法-Mark-Sweep" class="headerlink" title="1.标记清除算法 [Mark-Sweep]"></a>1.标记清除算法 [Mark-Sweep]</h4><pre><code>遍历所有的GC Root，分别标记处可达的对象和不可达的对象，然后将不可达的对象回收。
**缺点**是：效率低、回收得到的空间不连续 【当比较大的对象被创建时由于被回收的是不连续的，
所以被回收的空间就存不下，造成了浪费】
</code></pre><h4 id="2-标记整理算法"><a href="#2-标记整理算法" class="headerlink" title="2.标记整理算法"></a>2.标记整理算法</h4><pre><code>将所有可用的对象往前移动[标记谁是活跃对象，整理，会把内存对象整理成一棵树一个连续的空间]，这样会很耗资源
</code></pre><h4 id="3-复制算法"><a href="#3-复制算法" class="headerlink" title="3.复制算法"></a>3.复制算法</h4><pre><code>将内存分为两块，每次只使用一块。当这一块内存满了，就将还存活的对象复制到另一块上，并且严格按照内存地址排列，然后把已使用的那块内存统一回收。
**优点**是：能够得到连续的内存空间 
**缺点**是：浪费了一半内存
年轻代使用的是复制整理算法
</code></pre><h5 id="有一点需要注意："><a href="#有一点需要注意：" class="headerlink" title="有一点需要注意："></a>有一点需要注意：</h5>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-HDFS读写流程&amp;Yarn执行流程" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/HDFS读写流程&Yarn执行流程/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/HDFS读写流程&Yarn执行流程/">
        客户端向HDFS读写数据流程
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="客户端向HDFS写数据流程"><a href="#客户端向HDFS写数据流程" class="headerlink" title="客户端向HDFS写数据流程"></a>客户端向HDFS写数据流程</h2><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/KYeqSTrKzIYBJCYrfZ6gqk7Ztvhkl3Ax*HFAUXpr1fM!/b/dGgBAAAAAAAA&amp;bo=SgVRAgAAAAADBz4!&amp;rf=viewer_4" alt=""></p>
<pre><code>1. 客户端向Namenode请求上传文件 /aaa/xxx.log
2. Namenode 检查自己的元数据，看看元数据下有没有这个目录，假设满足条件(没有这个目录可以上传)
    响应客户端的请求[客户端可以上传]
3. 客户端RPC请求上传1个Block(0-128M),请返回DataNode(返回几个DataNode是客户端配置的,默认3个)
4. 返回(dn1,dn3,dn4)给客户端
    为什么要给客户端返回这三台机器呢？
        考虑因素： 空间/距离
            1. 第1个副本(dn1),要看DataNode的空间(剩余存储空间)和距离(同一个机架上的距离一样，而不是谁的网线长短)
            假如这几台机器的空间差不多，NameNode就随机返回一台；
            2. 第2个副本(dn3),考虑跨机架挑选1个DataNode，增加副本的可靠性
            3. 第3个副本就在第1个副本同机架另外挑选1台DataNode存放
        怎么能让NameNode知道哪个datanode在1个机架，哪个datanode在另1个机架，这个要配个文件，
        配机架感知。[就是在配置文件里写死了，哪个机架有哪几台DataNode（网咯拓扑）]
5. 客户端会找最近的(将要上传副本到的那台机器)一台DataNode[假如找的是dn1]，请求建立Block传输通道channel[本地流，以供写入数据]
    5.1 dn1接收到请求看到还有2台机器(dn3、dn4)，dn1就向dn3发送请求建立管道
    5.2 dn3接收到请求看到还有一台dn4，就向dn4发送请求建立管道
6.1 dn4建立管道成功后，给dn3发送响应，建立管道成功
6.2 dn3再给dn1发送响应，建立管道成功
    这样一来，管道 PipeLine就建立成功了
7. 客户端读取本地的1块数据(默认128M)，向dn1传输数据(数据是以1个1个packet[默认64k]的形式上传的)
    packet是以chunk(Byte)为单位校验
    7.1 每个DataNode写入磁盘之前都会有1个缓冲区(ByteBuf)，先写入缓冲区，再写入本地磁盘
         因为每个packet只有64k，所以dn写入之后，基本上后面的dn3和dn4也都复制上传完成了[几乎同步完成]
         每个DataNode上传成功packet 都会向前面的DataNode反馈说上传成功，第一个DataNode才给客户端说上传成功
    假如说是后面两台DataNode节点没有上传成功，但是只要是第1台上传成功了，客户端就认为上传成功了
    [因为NameNode最后会帮客户端异步去复制，省得客户端一直阻塞，说没成功再来，没成功再来]；
    假如第1台也失败了，客户端就会找NameNode，告诉他这台DataNode不行，再换，然后重新返回DataNode，重新上传；
8. 上传完了第1个Block，客户端会再次向NameNode发送请求说要上传第2块Block，然后重新走刚刚的流程
总结：
    这样一来，NameNode的元数据就记录了，上传的文件的 路径、文件有几个Block、Block副本在哪些机器
</code></pre><h2 id="客户端向HDFS读数据流程"><a href="#客户端向HDFS读数据流程" class="headerlink" title="客户端向HDFS读数据流程"></a>客户端向HDFS读数据流程</h2><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/*cdq.W37ZmIVEpun93YYQqysu6asV1HifPC7o7.VPys!/b/dDEBAAAAAAAA&amp;bo=ygTkAQAAAAADFxk!&amp;rf=viewer_4" alt=""></p>
<pre><code>1. 客户端向NameNode请求下载/aa/xx.log
2. NameNode的元数据上面查看/aa/xx.log，然后查询得到：3个Block(Block1,Block2,Block3)、{Block1:dn1,dn3,dn4 Block2:dn1,dn4,dn5...}
    并返回给客户端目标文件的元数据信息
3. 客户端要下载第1个Block，然后就去找副本所在节点距离它最近的(dn1)，建立管道，请求读取Block1
4. dn1建立本地流，fileInpuStream，dn1建立 NIO socket - socketOutPutStream
5. 客户端建立socketInputStream，还有fileOutPutStream 写入本地 如：c:/xxx.log
6. dn1 传输通过通道传输数据
Block1上传到客户端完成后，客户端会再去找第2块Block 跟找Block1是一样的，还有Block3
</code></pre><h2 id="MR-Yarn的提交流程"><a href="#MR-Yarn的提交流程" class="headerlink" title="MR Yarn的提交流程"></a>MR Yarn的提交流程</h2><pre><code>Yarn：
    只负责程序运行所需资源的分配回收等调度任务，与应用程序的内部运行工作机制完全无关，
    所以Yarn已经成为一个通用的资源调度平台，许许多多的运算框架都可以借助他来实现资源管理，
    比如：MR/SPARK/FLINK/TEZ...
NodeManager分配container基于：
Linux的资源隔离机制cgroup  比如：docker也是基于此
</code></pre><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/9zhVGQyCCNef1I8asBSqn83n.wL1HN35NFV9J.n7CUM!/b/dJEAAAAAAAAA&amp;bo=QAXgAgAAAAADB4U!&amp;rf=viewer_4" alt=""></p>
<pre><code>1. 客户端所在节点，运行jar包，main方法里，job.submit() 
  YarnRunner(Proxy[实现了ClientProtocol])找Yarn集群的老大ResourceManager申请提交1个Application
2. ResourceManager返回Application提交资源路径：hdfs://xxx/xx.staging 和 application_id
3. YarnRunner提交job运行所需的资源文件，提交到hdfs://xxx/xx.staging/application_id
                                                                                    /job.split
                                                                                    /job.xml
                                                                                    wc.jar
4. 客户端通过RPC调用 告诉ResourceManager 资源提交完毕，申请运行mrAppMaster [后面的事情就和客户端没关系了]
ResourceManager运行的时候不止是接收这1个程序，还可能接收其他的程序提交，所以ResourceManager也有不同的调度策略[三种策略]：
    4.1 FIFO[先进先出]：任务在队列里，要先运行完第1个，再运行第2个
        [这个时候是可以接收其他的任务提交的，只不过要等][这个在老版本里是默认的]
    4.2 Fair：每个job提交后，都会分配一点点资源给他们
    4.3 capacity：第1个job提交之后，把资源全部给他，当第2个job来的时候，第1个job的一些task可能就已经运行完了，
        这样的话空余出来的资源就可以给后来提交的job了，如果后来的job需要的资源很少，可能直接就运行完了，运行完了，            还可以把这些资源给之前的应用继续使用
        [新版本的默认就是capacity调度策略]
5. 将用户的请求初始化成一个task
6. 因为nodeManager和NamenodeManager一直保持着心跳，所以会去领取任务(task),假如nm1领取到任务
7. nm1就会生成1个container容器[分配了cpu + ram]，到HDFS上下载资源并启动mrAppMatser
8. mrAppMatser向NamenodeManager申请运行MapTask的容器
9. 假如这时候nm2和nm3领取到task任务，然后分别在各自的节点生成1个容器[这个容器就是1个进程叫做YarnChild,可以通过jps查看]
10. mrAppMatser发送程序启动脚本[java -cp...]给nm2和nm3来运行MapTask
        mrAppMatser会监管这些MapTask的运行情况，如果哪个MapTask运行失败，
        他还会去找NamenodeManager重新申请1个nodeManager来生成container来运行MapTask
        如果他发现有一个MapTask运行的特别慢，他还会去重新申请1个新的nodeManager生成container，
        两个同时处理一个切片，看哪个先执行完，就用哪个的结果文件作为reduce的输入 **[推测执行]**
11. 当MapTask运行完成之后，mrAppMatser知道MapTask的输出文件[分区且排序，不如有3个分区]在哪[container里的工作目录]
12. mrAppMatser向NamenodeManager申请3个容器，来运行reduceTask程序
13. 这3个容器运行的reduce task会向相对应map task的nodeManager拉取相对应分区的数据过来
14. reduce 对数据进行merge + 排序 ，调用reduce方法 ，输出结果
15. 当mrAppMatser执行完了之后就会去找ResourceManager申请注销自己
</code></pre><h4 id="总结hadoop1和hadoop2的比较："><a href="#总结hadoop1和hadoop2的比较：" class="headerlink" title="总结hadoop1和hadoop2的比较："></a>总结hadoop1和hadoop2的比较：</h4><pre><code>a) 1里面没有yarn，只有jobTracker，负责资源调度和应用运算流程管理，
如果后面还有任务提交，还是他来管理和调度资源，这样的话都在1个节点上，
很消耗性能，而且当jobTracker挂掉之后，任务都不能运行了。
b) 2里面有yarn，只要把任务提交给yarn ，然后yarn进行资源管理就可以了，
appMaster来负责应用运算流程管理，当一个appMaster挂了之后，也不会影响
其他应用的作业                                                 
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-点击流日志分析流程" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/点击流日志分析流程/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/点击流日志分析流程/">
        点击流日志分析流程
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="点击流日志分析流程"><a href="#点击流日志分析流程" class="headerlink" title="点击流日志分析流程"></a>点击流日志分析流程</h1><p>流程图如下：<br>    <img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/UbKrmZeyW3FpirSZBEMFrDGawneMp6lIpLWKfxekmLw!/b/dJUAAAAAAAAA&amp;bo=QgauAgAAAAARB9g!&amp;rf=viewer_4" alt=""><br>    <img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/FimQ39x6PIwqFTl56xYF*n.qFv3LMDsUx5Bk7YsN*vs!/b/dGoBAAAAAAAA&amp;bo=kgU4BAAAAAARB5s!&amp;rf=viewer_4" alt=""></p>
<h6 id="原始数据："><a href="#原始数据：" class="headerlink" title="原始数据："></a>原始数据：</h6><pre><code>194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] &quot;GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/4.0 (compatible;)&quot;
字段解析:
1、访客 ip 地址: 58.215.204.118
2、访客用户信息: - -
3、请求时间:[18/Sep/2013:06:51:35 +0000]
4、请求方式:GET
5、请求的 url:/wp-includes/js/jquery/jquery.js?ver=1.10.2 6、请求所用协议:HTTP/1.1
7、响应码:304
8、返回的数据流量:0
9、访客的来源 url:http://blog.fens.me/nodejs-socketio-chat/
10 、 访 客 所 用 浏 览 器 : Mozilla/5.0 (Windows NT 5.1; rv:23.0) Firefox/23.0
Gecko/20100101
</code></pre><p>1.根据原始数据生成点击流模型 PageViews 和 Visits</p>
<pre><code>PageViews:
    根据IP判断是否是同一用户，根据前后两条日志时间相差是否在30分钟内，
    判断访问日志是否是属于同一个session[会话]，按照时间顺序标上步骤，
    这样就构成了一条访问轨迹线;
Visits：
    侧重于体现用户在一次session中的进入离开时间、进入离开页面，
    还有统计出在本次session中用户总共访问了几个页面
</code></pre><p>2.漏斗模型</p>
<pre><code>逐层递减
</code></pre><p>3.常见指标：</p>
<pre><code>骨灰级指标：
    IP：1天内访问网站的不重复IP总数
    PV[PageView]:用户每打开1次网页，记录1个PV
    UV[Unique Pageview]:1天以内，访问网站不重复的用户数据(以cookie为依据)，1天内同1访客多次访问网站只被计算1次
基础级指标：
    访问次数：
        访客从进入网站到离开网站一系列活动极为一次访问，也就是session
    网站停留时间：
        访问者在网站上花费的时间
    页面停留时间：
        访问者在某个特定页面或某组网页上所花费的时间
复合级指标:
    人均浏览页面：
        浏览次数/独立访客数  --体现网站对访客的吸引程度
    二跳率：
        二跳率的概念是当网站页面展开后，用户在页面上产生的首次点击被称为“二跳”，二跳的次数即为“二跳量”。二跳量与到达量（进入网站的人）的比值称为页面的二跳率。
    跳出率：
        跳出率是指在只访问了入口页面（例如网站首页）就离开的访问量与所产生总访问量的百分比。跳出率计算公式：跳出率=访问一个页面后离开网站的次数/总访问次数。
    二跳率越高越好，跳出率越低越好。
4.基础分析(PV,IP,UV)
5.来源分析
6.受访分析
7.访客分析
    终端详情[PC,移动端]新老访客、忠诚度、活跃度
8.转化路径分析
</code></pre><blockquote>
<blockquote>
<blockquote>
<p>数据处理流程：</p>
</blockquote>
</blockquote>
</blockquote>
<pre><code>&gt; 数据采集
    Flume采集需要在配置文件里配置source、channel、sink：
        source:
            1.spoolDir的作用是：监控文件夹，如果有新的文件产生，采集开始
            2.exec tail -f access.log 只能监听文件追加的内容
    以上1和2都没办法满足我们的log日志采集，因为既要监控文件也要监控文件夹，
    **好在Flume1.7的稳定版本提供了TAILDIR类型的source，
    可以监控一个目录，并且使用正则表达式匹配目录中的文件名进行实时收集，具体配置详情如下：
        a1.sources.r1.type = TAILDIR
        a1.sources.r1.positionFile = /var/log/flume/taildir_position.json
        a1.sources.r1.filegroups = f1 f2
        a1.sources.r1.filegroups.f1 = /var/log/test1/example.log 
        a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*
    解释：
        filegroups:指定 filegroups，可以有多个[每个还可以使用正则表达式来匹配]，
        以空格分隔;(TailSource 可以同时监控 tail 多个目录中的文件)
      **positionFile:解决了机器重启后无法**断点续传**的问题[检查点文件会以 json 格式保存已经 tail 文件的位置]

&gt; 数据预处理
    通过MapReduce程序对采集到的原始日志数据进行预处理，
    比如清洗，格式整理，滤除脏数据等，并且梳理成点击流模型数据
        1.一般来说，开发中针对不合法的数据，我们不是直接删除，而是打个标签 比如true或者false，
        因为这些数据可能对这个场景是无用的，但是对其他场景是有用的
        2.编写MapReduce程序，只有map没有reduce，因为输入一条数据，处理完后直接输出不需要聚合,setReduceNum = 0 ,输出的结果文件就是part-m
        3.编写相对应的Javabean的时候要实现Writable接口，
        重写toString方法的时候是按照Hive的默认分隔符&apos;\001&apos;进行分割的，
        导入hive表的时候直接按照默认的分隔符就ok了,
        注意：readFields 和 write 的方法写得时候要一致对应
        4.业务要求状态码为400以上的设置valid为false，还有时间不合法[为null或者双引号]
        5.过滤掉静态资源[图片/css/js],这个标准是根据业务来定的，一般会在mapper的set up 
        初始化方法里定义hashSet来存这些准则，然后进行标记清除
        6.PageViews数据生成： (session[UUID] + stayTime + step)
        ---------------------------------------------------------------------------------------
                    Session + IP + 地址 + 时间 + 访问页面 + URL + 停留时长 + 第几步
        ---------------------------------------------------------------------------------------
            a) 编写MapReduce程序，map端以Ip为key，Javabean为value，发送到reduce，
            reduce端进行values的排序，这里需要遍历values，每一次都new
            一个新的Javabean然后进行赋值[因为Javabean是引用类型，然后添加入新的ArrayList中，
            如果不重新new 一个新的对象的话，那么到最后ArrayList里面的对象就是同一个，因为他们指向的都是同一块堆内存！！！]，
            再把这个Javabean添加到新的ArrayList中
            b) 按照时间排序,然后对新的ArrayList进行排序，Collections.sort(beansList,new Comparabtor(javabean){中间获取时间来进行升序排序})
            c) 从有序的beans中分辨出歌词visit，并对一次visit中所访问的page按顺序标号step
                核心思想：比较相邻两条记录中的时间差，如果时间差&lt;30分钟，则该两条记录属于同一个session[生成UUID]，否则属于不同的session
                只有1条的和大于1条的，他们的默认时间都是60秒
        7.Visits模型：数据来自PageViews模型
        ---------------------------------------------------------------------------------------
        Session + 起始时间 + 结束时间 + 进入页面URL + 离开页面URL + 访问页面数 + 停留时长 + IP + referer
        ---------------------------------------------------------------------------------------
             编写MapReduce程序：
                 mapper端：k为session，value：javabean
                 reduce端：
                     以step进行排序


&gt; 数据入库
    将预处理之后的数据导入到Hive仓库中相应的库和表中
&gt; 数据分析
    项目的核心内容，即根据需求开发ETL分析语句，得出各种统计结果
&gt; 工作流调度：
    简单的任务调度:
        可以使用Linux的crontab -e 来设置调度，但是其缺点是无法设置依赖
    复杂的任务调度：
        推荐使用 : azkaban [Java语言实现的，他有管理页面，配置起来比较简单]
            azkaban是由LinkedIn公司推出的一个批量工作流任务调度器，
            用于在一个工作流内以一个特定的顺序运行一组工作和流程。
            使用job配置文件简历任务之间的依赖关系，并提供了一个已使用的web用户界面维护和跟踪工作流。
            支持command、Java、Hive、pig、Hadoop，而且是基于java开发，代码结构清晰，抑郁二次开发
                azkaban的组成：
                    1.mysql服务器
                        用于存储项目、日志或者执行计划[执行周期等]之类的信息
                    2.web服务器：
                        使用jetty[开源的serverlet容器]对外提供web服务，使用户可以通过wen页面方便管理
                    3.executor服务器：
                        负责具体的工作流的提交、执行
           配置azkaban步骤：[cluster模式]
                   1.生成keystore证书文件，mv到webserver下
                   2.配置为年修改一下时区：Asia/Shanghai
                   3.配置数据库mysql
                   4.配置user admin的登录
          使用azkaban的步骤：
                  1.创建a.job文件并且已经配置b.job ，里面的type为command，中间可以配置的dependencies=b，然后command=xxxx
                    期间要把这些job打成zip包，通过web提交上去配置立刻执行还是scheduler定期执行
                      # a.job
                    type=command
                    dependencies=b
                    command=echo hahaha
                这样的话a的job任务就会等待b结束后再执行
                2.hdfs操作任务
                    command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop fs -mkdir /azaz
                  3.MapReduce操作任务
                      command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop jar hadoop-mapreduce- examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout
                  4.hive操作任务
                      执行一个命令是 command=/xx/hive -e &apos;show tables&apos;
                      执行一个文件，里面是hive sql语句， commmand=/xx/hive -f &apos;test.sql&apos;
                      Hive 脚本: test.sql
                        type=command
                        command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop jar hadoop-mapreduce- examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout
                        use default;
                        drop table aztest;
                        create table aztest(id int,name string) row format delimited fields terminated by &apos;,&apos;;
                        load data inpath &apos;/aztest/hiveinput&apos; into table aztest;
                        create table azres as select * from aztest;
                        insert overwrite directory &apos;/aztest/hiveoutput&apos; select count(1) from aztest;
        不推荐使用: ooize[虽然是Apache旗下的，但是工作流的过程是编写大量的XML文件配置，而且代码复杂度比较高，不易于二次开发]
&gt; 数据展现
    将分析所得到的数据进行数据可视化，一般通过图表[百度的echarts]进行展示
</code></pre><h2 id="模块开发-数据仓库的设计"><a href="#模块开发-数据仓库的设计" class="headerlink" title="模块开发-数据仓库的设计"></a>模块开发-数据仓库的设计</h2><pre><code>1.纬度建模
    纬度表(demension)
        通常指 按照类别、区域或者时间等等来分析，维度表数据比较固定，数据量小
     事实表
        事实表的设计是以能够正确记录历史信息为准则也就是一条一条的数据，就像是消费记录里面有product_id
        维度表的设计是以能够以合适的角度来聚合主题内容为准则  这边有product_id对应的产品信息
2.纬度建模三种模式：
    2.1 星型模式 [像星星一样]
            由一个事实表和一组维度表组成    
                比如：
                    事实表里有地域主键、时间键、部门键、产品键 对应有4个维度表相关联
    2.2 雪花模式[不常用，因为不容易维护！！！]
            在星型模式基础上，维度表还有维度表
    2.3 星座模式 [开发常用！！！]
        基于多张事实表，而且共享纬度信息
</code></pre><p>本项目数据仓库的设计：</p>
<pre><code>1.事实表的设计 ods_weblog_orgin =&gt; 对应mr清洗完之后的数据 【窄表】和【宽表或者明细表】
    窄表：对应原始数据表，字段跟数据中一一对应，但是不利于分析
---------------------------------------------------------------------------------------------------------------
        valid  remote_addr remote_user time_local  request status  body_bytes_sent http_referer  http_user_agent
        是否有效 访客IP         访客用户信息  请求时间     请求url  响应码    相应字节数       来源url         访客终端信息
---------------------------------------------------------------------------------------------------------------
    宽表：把某些融合各种信息的字段 提取出不同的信息作为新的字段
        相对于之前的窄表 字段增加了，所以叫宽表，
        比如时间戳，如果是之前的话 &apos;2018-09-09 18:09:09&apos;这种时间不利于分析，
        如果分成年，月，日，那么分析时直接group by day 或者 year 或者day 就ok了
        还有referer_url也是如此，可以拆分为host或者参数之类的

2.维度表的设计如：
    时间维度 t_dim_time: date_key year month day hour 
    访客地域纬度t_dim_area: area_ID 北京 上海 广州 深圳
    终端类型维度 t_dim_termination: uc firefox chrome safari ios android
    网站栏目纬度 t_dim_section: 进口食品、生鲜日配、时令果蔬、奶制品、
                                休闲保健、酒饮冲调茶叶、粮油副食、母婴玩具、个护清洁、家具家电
    维度表的数据一般要结合业务情况自己写脚本按照规则生成，也可以用工具来生成，方便后续关联分析
    比如事先生成时间维度表中的数据，跨度从业务需求的日期到当前的日期即可，具体根据分析粒度，
    库生成年，季，月，周，天，时等相关信息，用于分析
</code></pre><p>数据仓库三层架构：</p>
<pre><code>ods层：数据就是通过mr清洗过的数据，带有标签valid或者标识的数据
    1.创建ODS层数据表
        1.1. 原始日志数据表 :创建按照时间来分区的hive 分区表
            drop table if exists ods_weblog_origin;
            create table ods_weblog_origin
            (
            valid string,remote_addr string,remote_user string, time_local string,request string,status string, body_bytes_sent string, http_referer string, http_user_agent string
            )
            partitioned by (datestr string)
            row format delimited 
            fields terminated by &apos;\001&apos;;
        1.2. 点击流模型 PageViews表
            drop table if exists ods_click_pageviews;
            create table ods_click_pageviews
            (
            session string,remote_addr string,remote_user string, time_local string,request string,visit_step string, page_staylong string, http_referer string, http_user_agent string, body_bytes_sent string, status string
            )
            partitioned by (datestr string)
            row format delimited 
            fields terminated by &apos;\001&apos;;
        1.3. 点击流模型 Visits
            drop table if exists ods_click_visits;
            create table ods_click_visits
            (
            )
            partitioned by (datestr string)
            row format delimited 
            fields terminated by &apos;\001&apos;;
        1.4. 维度表创建(这里举例：时间，年、月、日、时)
            drop table if exists t_dim_time;
            create table t_dim_time 
            (
            date_key int,...
            )
            row format delimited 
            fields terminated by &apos;,&apos;;
        1.5 创建明细宽表 ods_weblog_detail 时间可以明细为 年月日时分秒，
            referer_url 可以明细为 host、path、query、queryid
            从预清洗后的表中得到这些数据，如果是referer_url 需要使用Hive里定义的函数：
                lateral view parse_url_tuple(正则表达式)这个方法，自动把url转换为host、path等
            如果是时间拓宽明细表的话 就是 substring
dw层：ods通过ETL处理之后得到dw层
        多维度统计PV总量：
            b) 与时间维度表关联查询
                insert into table dw_pvs_everyday select count(*) as pvs,a.month as month,a.day as day 
                from 
                (select distinct month, day from t_dim_time) a 
                join 
                ods_weblog_detail b 
                on a.month=b.month and a.day=b.day group by a.month,a.day;
            c) 按照referer维度进行统计每小时各来访 url 产生的 PV 量
                insert into table dw_pvs_referer_everyhour partition(datestr=&apos;20130918&apos;)
                select http_referer,ref_host,month,day,hour,count(1) as pv_referer_cnt
                from 
                ods_weblog_detail
                group by http_referer,ref_host,month,day,hour
                having ref_host is not null
                order by hour asc,day asc,month asc,pv_referer_cnt desc;
            d) 人均浏览量
                统计今日所有来访者平均请求的页面数。
                    insert into table dw_avgpv_user_everyday
                    select 
                    &apos;20130918&apos;,sum(b.pvs)/count(b.remote_addr) 
                    from
                    (select remote_addr,count(1) as pvs from ods_weblog_detail where datestr=&apos;20130918&apos; group by remote_addr) b; 
            e)特别重要：分组求TopN ************非常重要**********
                row_number()函数
                    row_number() over (partition by xxx order by xxx) rank
                insert into table dw_pvs_refhost_topn_everyhour partition(datestr=&apos;20130918&apos;) 
                select t.hour,t.od,t.ref_host,t.ref_host_cnts 
                from(
                select ref_host,ref_host_cnts,concat(month,day,hour) as hour,row_number() over (partition by concat(month,day,hour) order by ref_host_cnts desc
                ) as od 
                from 
                dw_pvs_refererhost_everyhour) t 
                where od&lt;=3;
            f) 受访分析(从页面的角度分析)
                热门页面统计
                    统计每日最热门的页面 top10
                        insert into table dw_hotpages_everydayselect &apos;20130918&apos;,a.request,a.request_counts 
                        from
                        (select request as request,count(request) as request_counts 
                        from 
                        ods_weblog_detail 
                        where datestr=&apos;20130918&apos; 
                        group by request having request is not null
                        ) a 
                        order by a.request_counts desc 
                        limit 10;
            g) 每小时独立访客及其产生的 pv
                insert into table dw_user_dstc_ip_h 
                select remote_addr,count(1) as pvs,concat(month,day,hour) as hour 
                from 
                ods_weblog_detail Where datestr=&apos;20130918&apos; 
                group by concat(month,day,hour),remote_addr;
                    在以上的结果基础上，统计每小时独立访客总数
                        select count(1) as dstc_ip_cnts,hour from dw_user_dstc_ip_h group by hour;
                    统计每日独立访客总数
                        select remote_addr,count(1) as counts,concat(month,day) as day 
                        from 
                        ods_weblog_detail Where datestr=&apos;20130918&apos; 
                        group by concat(month,day),remote_addr;
                    统计每月独立访客总数
                        select 
                        remote_addr,count(1) as counts,month 
                        from 
                        ods_weblog_detail 
                        group by month,remote_addr;
            h) 每日新访访客 today left join old ***************非常重要*************
                insert into table dw_user_new_d partition(datestr=&apos;20130918&apos;) 
                select tmp.day as day,tmp.today_addr as new_ip 
                from 
                ( select today.day as day,today.remote_addr as today_addr,old.ip as old_addr from (select distinct remote_addr as remote_addr,&quot;20130918&quot; as day from ods_weblog_detail where datestr=&quot;20130918&quot;) today left outer join dw_user_dsct_history old on today.remote_addr=old.ip ) tmp 
                where tmp.old_addr is null;
            注意：每日新用户追加到累计表
            i) 访客 Visit 分析(点击流模型)
                查询今日所有回头访客及其访问次数。
                    insert overwrite table dw_user_returning partition(datestr=&apos;20130918&apos;) 
                    select tmp.day,tmp.remote_addr,tmp.acc_cnt 
                    from 
                    (select &apos;20130918&apos; as day,remote_addr,count(session) as acc_cnt from ods_click_stream_visit group by remote_addr) tmp 
                    where tmp.acc_cnt&gt;1;
            j) 人均访问频次
                统计出每天所有用户访问网站的平均次数(visit)
                    select sum(pagevisits)/count(distinct remote_addr) from ods_click_stream_visit where datestr=&apos;20130918&apos;;
            k) 关键路径转化率分析(漏斗模型) -- 基于PageViews模型
                定义好业务流程中的页面标识，下例中的步骤为[模型设计]: Step1、 /item
                                                          Step2、 /category
                                                         Step3、 /index
                                                        Step4、 /order
                --查询每一步人数存入 dw_oute_numbs
                create table dw_oute_numbs as
                select &apos;step1&apos; as step,count(distinct remote_addr) and request like &apos;/item%&apos;
                union
                select &apos;step2&apos; as step,count(distinct remote_addr) and request like &apos;/category%&apos;
                union
                select &apos;step3&apos; as step,count(distinct remote_addr) and request like &apos;/order%&apos;
                union
                select &apos;step4&apos; as step,count(distinct remote_addr) and request like &apos;/index%&apos;;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                注:UNION 将多个 SELECT 语句的结果集合并为一个独立的结果集。
                ***利用级联求和自己和自己join ******************非常重要********************
                inner join
                select abs.step,abs.numbs,abs.rate as abs_ratio,rel.rate as leakage_rate
                from
                (
                select tmp.rnstep as step,tmp.rnnumbs as numbs,tmp.rnnumbs/tmp.rrnumbs as rate
                from
                (
                select rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs inner join
                dw_oute_numbs rr) tmp
                where tmp.rrstep=&apos;step1&apos;
                ) abs
                left outer join
                (
                select tmp.rrstep as step,tmp.rrnumbs/tmp.rnnumbs as rate
                from
                (
                select rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs inner join
                dw_oute_numbs rr) tmp
                where cast(substr(tmp.rnstep,5,1) as int)=cast(substr(tmp.rrstep,5,1) as int)-1
                ) rel
                on abs.step=rel.step;
                from dw_oute_numbs rn
                其中 cast(substr(tmp.rnstep,5,1) as int) 是 把字符串截取字符 然后强制转化为int
            ) 还可以按照栏目纬度和UA（user agent）纬度来分析PV,
                为了说明PV是可以从各个纬度去分析的

app层：应用层来拿数据展示
</code></pre><p>Sqoop：是Hadoop和关系数据库服务器之间传送数据的一种工具-sql到Hadoop和Hadoop到sql</p>
<pre><code>sqoop工作机制是将导入或导出命令翻译成MapReduce程序来实现，
在翻译出的MapReduce中主要是对inputformat和outputformat进行定制
1.从关系型数据库(mysql)导入到hadoop 是 DBIputformat，import命令
    bin/sqoop import \
    --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
    --password hadoop \
    --target-dir /sqoopresult \ //--target-dir 可以用来指定导出数据存放至 HDFS 的目录;
    --table emp --m 1   //m 1 表示一个map来跑
2.导入 mysql 表数据到 HIVE
    2.1 将关系型数据的表结构复制到 hive 中
    bin/sqoop create-hive-table \
    --connect jdbc:mysql://node-21:3306/sqoopdb \ --table emp_add \
    --username root \
    --password hadoop \
    --hive-table test.emp_add_sp
    2.2 以上只是复制表的结构，并没有将数据导进去，将数据导入Hive表中
        bin/sqoop import \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --table emp_add \
        --hive-table test.emp_add_sp \
        --hive-import \   ****
        --m 1
    2.3 复杂查询条件:如果不指定分隔符是默认逗号
        bin/sqoop import \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --target-dir /wherequery12 \
        --query &apos;select id,name,deg from emp WHERE --split-by id \
        --fields-terminated-by &apos;\t&apos; \
        --m 1
    2.4 下面的命令用于在 EMP 表执行增量导入:
        bin/sqoop import \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --table emp --m 1 \
        --incremental append \  ****
        --check-column id \  ****
        --last-value 1205    ****
    3. Sqoop 导出
        将数据从 HDFS 导出到 RDBMS 数据库导出前，目标表必须存在于目标数据库中。
        bin/sqoop export \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --table employee \
        --export-dir /emp/emp_data
        还可以用下面命令指定输入文件的分隔符
        --input-fields-terminated-by &apos;\t&apos;
</code></pre><h4 id="工作流调度："><a href="#工作流调度：" class="headerlink" title="工作流调度："></a>工作流调度：</h4><pre><code>整个项目的数据按照处理过程，从数据采集到数据分析，再到结果数据的到处，
一系列的任务可以分割成若干个azkaban的job单元，然后由工作流调度器调度执行。
调度脚本的编写难点在于shell脚本
shell脚本大体框架如下：
    #!/bin/bash
    #set java env
    #set hadoop env
    #设置一些主类、目录等常量
    #获取时间信息
    #shell 主程序、结合流程控制(if....else)去分别执行 shell 命令。 更多工作流及 hql 脚本定义见参考资料。
    hive -e执行sql语句
</code></pre><h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><pre><code>Echarts：
    百度前端技术部开发的，基于JavaScript的数据可视化图标库，
    可以构建折线图(区域图)、柱状 图(条状图)、散点图(气泡图)、饼图(环形图)、
    K 线图、地图、力导向布局图以及和弦图， 同时支持任意维度的堆积和多图表混合展现。
javaEE中web.xml 的&lt;url-pattern&gt;/&lt;/url-pattern&gt; 是拦截所有，jsp除外

1.Mybatis example 排序问题 example.setOrderByClause(&quot;`dateStr` ASC&quot;);
查询结果便可以根据 dataStr 字段正序排列(从小到大)
如何区分不同数据仓库层的表：
2.Echarts 前端数据格式问题
注意，当异步加载数据的时候，前端一般需要的是数据格式是数组。一定要对应上。在 这里我们可以使用 Java Bean 封装数据，然后转换成 json 扔到前端，对应    上相应的字段即 可。
ObjectMapper om = new ObjectMapper(); beanJson = om.writeValueAsString(bean);
3.Controller 返回的 json @RequestMapping(value=&quot;/xxxx&quot;,produces=&quot;application/json;charset=UTF-8&quot;)
@ResponseBody    

一般使用第一种[业内默认的]
1.表之前加前缀 ods_T_access.log
                dw_T_access.log
2.针对不同的数据仓库层 建立对应的数据库 database
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-Hive的优化：" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/Hive的优化：/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/Hive的优化：/">
        HIVE 总结
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="HIVE-总结："><a href="#HIVE-总结：" class="headerlink" title="HIVE 总结："></a>HIVE 总结：</h3><h4 id="内部表-管理表"><a href="#内部表-管理表" class="headerlink" title="内部表(管理表)"></a>内部表(管理表)</h4><pre><code>内部表也称之为 MANAGED_TABLE；默认存储在/user/hive/warehouse下，也可以通过location指定;
**重点**：删除表时也会删除元数据和数据本身
</code></pre><h4 id="外部表-托管表"><a href="#外部表-托管表" class="headerlink" title="外部表(托管表)"></a>外部表(托管表)</h4><pre><code>外部表也称之为 EXTERNAL_TABLE;在创建表时可以自己指定目录位置(location);
**重点**：删除表时只会删除元数据不会删除数据本身
</code></pre><h3 id="分区表："><a href="#分区表：" class="headerlink" title="分区表："></a>分区表：</h3><pre><code>分区表实际上就是对应一个hdfs文件系统上的独立的文件夹，该文件夹下是该分区的所有数据文件。
hive中的分区就是分目录，把大的数据集根据业务分割成更小的数据集。
**作用**：在查询时通过where子句中的表达式来选择查询所需要的指定分区，这样的查询效率会提高很高。
</code></pre><h5 id="分区表的修复："><a href="#分区表的修复：" class="headerlink" title="分区表的修复："></a>分区表的修复：</h5><pre><code>面试题：
    方法一[msck repair table emp]：加入创建了一个hive分区表，然后用hdfs的方式去-mkdir分区字段的目录，然后向这个目录下-put数据文件，然后去select,会显示表的内容为空，但是数据在的，这个时候查看元数据的partitions里面是没有这个分区字段的，此时可以用 **msck repair table emp** 命令来进行修复后就可以查到内容
    方法二[alter table emp add partition(day = 15)]
</code></pre><h3 id="hive中的高级查询："><a href="#hive中的高级查询：" class="headerlink" title="hive中的高级查询："></a>hive中的高级查询：</h3><pre><code>1. order by  ---&gt; 对全局数据的排序，仅仅只有一个reduce ***数据量比较大的时候慎用！！！
2. sort by ---&gt;对每一个reduce内部数据进行排序，对于全局结果集来说不是排序 使用前先设置reduce的个数：set mapreduce.job.reduces = 3
3. distribute by ---&gt; 作用就是分区(partition) ,类似于MapReduce中分区partition，对数据进行分区，结合sort by进行使用 如：select * from emp distribute by depno sort by empno asc;
    注意：distribute by 要放在 sort by 前面[先分区再排序]
4.cluster by ---&gt; 当distribute by 和 sort by字段相同时可以使用cluster by
</code></pre><h3 id="UDF【user-defined-function】"><a href="#UDF【user-defined-function】" class="headerlink" title="UDF【user defined function】:"></a>UDF【user defined function】:</h3><h6 id="hive自带了很多UDF，如：max、min、split，但是往往在开发中不能满足我们的也无需求"><a href="#hive自带了很多UDF，如：max、min、split，但是往往在开发中不能满足我们的也无需求" class="headerlink" title="hive自带了很多UDF，如：max、min、split，但是往往在开发中不能满足我们的也无需求"></a>hive自带了很多UDF，如：max、min、split，但是往往在开发中不能满足我们的也无需求</h6><pre><code>UDF 一进一出;         
UDAF(aggregation) 多进一出;  
UDTF(table-generating) 一进多出
</code></pre><p>UDF开发步骤：</p>
<pre><code>1.继承hive.ql.UDF类
2.实现一个或多个evaluate方法 即：重载
3.把程序打成jar包 
4. [jar包在本地]: 
   4.11 add jar /xxx/xxx.jar 
   4.12 create temporary function funcname as &quot;类的全路径&quot; 
   [jar包在hdfs] 
   4.2 create function funcname as &quot;类的全路径&quot; using jar &apos;hdfs://xxx/xx.jar&apos;
</code></pre><p>UDF开发注意事项：</p>
<pre><code>1.UDF必须要有返回值，不能为void，且可以返回null
2.UDF中常用Text/LongWritable等类型，不推荐使用java类型，因为hive底层是用MapReduce，而mr的实现里面都是他自己独有的可序列化类型
</code></pre><h2 id="Hive的优化："><a href="#Hive的优化：" class="headerlink" title="Hive的优化："></a>Hive的优化：</h2><h4 id="优化1-数据压缩-使用snappy格式-snappy是谷歌开源的"><a href="#优化1-数据压缩-使用snappy格式-snappy是谷歌开源的" class="headerlink" title="优化1.数据压缩 [使用snappy格式]    **snappy是谷歌开源的"></a>优化1.数据压缩 [使用snappy格式]    **snappy是谷歌开源的</h4><pre><code>作用：数据量小，减少网络IO流
hive底层还是走的MapReduce，也就是压缩mr阶段的数据；
具体--&gt; 1.1 CompressedInput上传之前进行压缩;  然后客户端进行InputSplit分片，这个阶段不需要配置，因为如果是压缩后的snappy文件，map会自动进行解压
       1.2map接收数据后进行DecompressInput[解压缩]对数据进行处理
       1.3 map阶段SpillToDisk的时候进行压缩 
               配置：mapreduce.map.output.compress=true;
    mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.Defaul tCodec
       1.4 reduce 来map的disk取数据后进行解压缩
       1.5 reduce 进行数据汇总 然后 进行压缩 输出[CompressReduceOutput]
       配置：mapreduce.output.fileoutputformat.compress=true;
    mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.Defaul tCodec
</code></pre><h4 id="优化2-数据的存储格式【storage-format】：列式存储的Parquet是Apache的顶级项目"><a href="#优化2-数据的存储格式【storage-format】：列式存储的Parquet是Apache的顶级项目" class="headerlink" title="优化2.数据的存储格式【storage format】：列式存储的Parquet是Apache的顶级项目"></a>优化2.数据的存储格式【storage format】：列式存储的Parquet是Apache的顶级项目</h4><h6 id="我们一般select-name-age-from-table-group-by-xx-都是查询的列，如果读取文件时按照列来读取，那么效率会高很多"><a href="#我们一般select-name-age-from-table-group-by-xx-都是查询的列，如果读取文件时按照列来读取，那么效率会高很多" class="headerlink" title="我们一般select name,age from table group by xx..都是查询的列，如果读取文件时按照列来读取，那么效率会高很多"></a>我们一般select name,age from table group by xx..都是查询的列，如果读取文件时按照列来读取，那么效率会高很多</h6><pre><code>SequenceFile、TextFile：按行存储数据
ParquetFile、ORCFile ：按列存储数据 【ORC里面是 strip index，索引查找】
**重要**测试显示存储在hdfs上的相同文件 create table name(...) stored as orc;和 create...stored as textfile;文件大小比例：orc : text = 1:7 大大的减少了存储的数据大小
小结：1.orc和parquet的存储格式使得存储相同的数据占得空间变小，尤其是orc
     2.而列式存储相对于text、sequence的行式存储查询也是很占优势的
</code></pre><h5 id="以上两点：压缩-数据存储格式-两者结合效果更好"><a href="#以上两点：压缩-数据存储格式-两者结合效果更好" class="headerlink" title="以上两点：压缩+数据存储格式 两者结合效果更好"></a>以上两点：压缩+数据存储格式 两者结合效果更好</h5><pre><code>create table page_views_orc_snappy(...) row format ... stored as orc tblproperties (&apos;orc.compress&apos;=&apos;SNAPPY&apos;);
insert into table page_views_orc_snappy select * from page_views;
//注释：这个插入语句会执行三个job: 查询page_views + 转化为orc格式 + snappy compress[压缩]
</code></pre><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><pre><code>在实际的项目开发中，hive表的数据
    *存储格式
        orcfile / parquet
    *数据压缩
        snappy
</code></pre><h4 id="优化3-Fetch-Task-值改为-more"><a href="#优化3-Fetch-Task-值改为-more" class="headerlink" title="优化3.Fetch-Task     值改为 more"></a>优化3.Fetch-Task     值改为 more</h4><pre><code>存在的问题分析：select * from t 不会走mr，select name from t 就会走mr，为什么？
因为：
    在hive-default.xml里面有个&lt;property&gt;是hive.fetch.task.conversion = minimal
[默认的--只有三种情况下不会走MapReduce：1.select * from t 2.select * from t where partitionname = xx 3.select * from t limit n]
我们可以在hive-site.xml里配置这个属性的值为 more 
</code></pre><h3 id="优化4-hive高级优化"><a href="#优化4-hive高级优化" class="headerlink" title="优化4.hive高级优化"></a>优化4.hive高级优化</h3><pre><code>1.大表拆成子表 
    create table table1... AS select * from table2
    比如订单表，都是几百列，当我们不同需求的时候只用一些字段的时候，就到大表创建的字表里取查询，这样就比较快
2.外部表 + 分区表   [因为有可能多个部门同时在用这些数据] + [一般二级分区表较多--月/日]
  create internal table name ... partitioned by (month string,day string) row format delimited...
3.数据存储格式 + 压缩   [orcfile/parquetfile   +   snappy]

    set parquet.compress = snappy;
    create table order_parquet_snappy(...) 
    row format ... 
    stored as parquet 
    as select * from order;
4.SQL语句的优化
</code></pre><h3 id="优化5-数据倾斜问题："><a href="#优化5-数据倾斜问题：" class="headerlink" title="优化5.数据倾斜问题："></a>优化5.数据倾斜问题：</h3><pre><code>Join:
    1.Shuffle/Reduce/Commen Join
        连接发生的阶段是 Reduce Task
        一般是大表对大表
        每个表的数据都是从文件中读取的
    2.Map Join
        连接发生在 Map Task
        一般是小表对大表
        大表的数据从文件中读取，小表的数据从内存中读取
        用到了 DistributedCache，把小表缓存到了个个节点的内存中
    3.SMB Join   [Sort + Merge + Bucket]
        一般在大公司里会用到，其实就是对reduce join的优化，因为两个大表都特别大，那么会吃不消的
        set hive.enforce.bucketing = true;
        set mapreduce.job.reduces = 4  [举例]
        create table tablename(...) clustered by (cid) into 4 buckets row format ...
        load data inpath &apos;xxx&apos; into ...实质上是 hdfs dfs -put xxx 不会走MapReduce的
        所以插入分桶表bucket需要用 insert into table table_bucket select * from xxx cluster by (cid),这样才会走mr
    4.group by 和 count(distinct)容易造成数据倾斜 //todo:待解决
</code></pre><p>hdfs dfs -du -h /root/xxx 是看目录或者文件有多大</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-sparkstreaming&amp;kafka" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/sparkstreaming&kafka/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/sparkstreaming&kafka/">
        将offsets存储在HBase中
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>将offsets存储在HBase中</p>
<p>HBase可以作为一个可靠的外部数据库来持久化offsets。通过将offsets存储在外部系统中，Spark Streaming应用功能能够重读或者回放任何仍然存储在Kafka中的数据。</p>
<p>根据HBase的设计模式，允许应用能够以rowkey和column的结构将多个Spark Streaming应用和多个Kafka topic存放在一张表格中。在这个例子中，表格以topic名称、消费者group id和Spark Streaming 的batchTime.milliSeconds作为rowkey以做唯一标识。尽管batchTime.milliSeconds不是必须的，但是它能够更好地展示历史的每批次的offsets。表格将存储30天的累积数据，如果超出30天则会被移除。下面是创建表格的DDL和结构</p>
<p>对每一个批次的消息，使用saveOffsets()将从指定topic中读取的offsets保存到HBase中</p>
<p>在执行streaming任务之前，首先会使用getLastCommittedOffsets()来从HBase中读取上一次任务结束时所保存的offsets。该方法将采用常用方案来返回kafka topic分区offsets。</p>
<p>情形1：Streaming任务第一次启动，从zookeeper中获取给定topic的分区数，然后将每个分区的offset都设置为0，并返回。</p>
<p>情形2：一个运行了很长时间的streaming任务停止并且给定的topic增加了新的分区，处理方式是从zookeeper中获取给定topic的分区数，对于所有老的分区，offset依然使用HBase中所保存，对于新的分区则将offset设置为0。</p>
<p>情形3：Streaming任务长时间运行后停止并且topic分区没有任何变化，在这个情形下，直接使用HBase中所保存的offset即可。</p>
<p>在Spark Streaming应用启动之后如果topic增加了新的分区，那么应用只能读取到老的分区中的数据，新的是读取不到的。所以如果想读取新的分区中的数据，那么就得重新启动Spark Streaming应用。</p>
<p>当我们获取到offsets之后我们就可以创建一个Kafka Direct DStream</p>
<p>在完成本批次的数据处理之后调用saveOffsets()保存offsets.<br> 你可以到HBase中去查看不同topic和消费者组的offset数据</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-ID-Mapping" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/ID-Mapping/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/ID-Mapping/">
        解密大数据ID-Mapping
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>集奥聚合带你解密大数据ID-Mapping<br>来源：数据观 时间：2016-06-27 14:46:24 作者：集奥聚合<br>　　谈到大数据，有一个非常基本但又关键的环节就是ID-Mapping（Identifier -Mapping）。ID-Mapping通俗的说就是把几份不同来源的数据，通过各种技术手段识别为同一个对象或主体，例如同一台设备（直接），同一个用户（间接），同一家企业（间接）等等，可以形象地理解为用户画像的“拼图”过程。一个用户的行为信息、属性数据是分散在很多不同的数据来源的，因此从单个数据来看，都相当于“盲人摸象”，看到的只是这个用户一个片面的画像，而ID-Mapping能把碎片化的数据全部串联起来，消除数据孤岛，提供一个用户的完整信息视图，同时让某一个领域的数据在另一个领域绽放出巨大的价值。<br>　　ID-Mapping有非常多的用处，比如跨屏跟踪和跨设备跟踪，将一个用户的手机、PC、平板等设备的上的行为信息串联到一起。再比如这两年非常热的程序化交易，它的一个重要环节就是要把当前广告请求的用户和第一方DMP平台里的用户历史兴趣数据匹配起来。可以说，没有ID-Mapping，程序化交易就变成了盲目投放，它的实时竞价，精准投放的优势也就不存在了。<br>　　ID-Mapping既然有这么大的作用，那么应该如何做好ID-Mapping呢？这个环节不是一个简单的按照Key匹配的过程，集奥聚合作为领先的第三方大数据公司，研发了多项ID-Mapping 的独家技术，用新的匹配技术和算法模型来重塑了ID-Mapping过程。据粗略评估，集奥聚合ID-Mapping系统有能力把十几个数据源的 56亿ID（Identifier，即标识符）匹配到一起，准确率达到 95%以上，有效用户总量提升了30%，平均每个用户的标签量提升200%以上。值得注意的是，这里的Identifier是指标识符，而并非Identity（身份信息），集奥聚合可在完全脱敏，不（也无需）识别、指出用户姓甚名谁的身份信息的情况下合法地将标识符对应至某匿名用户。<br>　　简单来说，集奥聚合ID-Mapping体系有三个层面。<br>　　第一个层面是物理Mapping<br>　　这是最单纯基本的层面，也就是如何精准地记录和标识一个用户，例如利用硬件设备码生成一个统一的设备码，利用一些强账号来标识用户等等。这个层面上主要的技术难度在于ID的稳定性、唯一性和持久性。<br>　　第二个层面是基于用户行为做迭代滚动Mapping<br>　　由于原始数据存在噪音，同一个用户的多份数据、多种ID之间是“多对多”的关系。那么哪些ID是可信的呢？<br>　　我们设计了一个置信度传播的机器学习图模型来帮助确定哪些身份ID是可信的。</p>
<p>　　算法示意图如上，每个节点是一个UID或QQ号或GID等标识的潜在的“用户”<br>　　 一开始节点之间关系的概率是随机的<br>　　 其中总有两个ID的关系是强置信的prior<br>　　 迭代收敛后，哪些ID是归属于同一个用户的标识符被识别出来<br>　　大体来说，这个算法的过程是给每一个ID，以及两个ID，如IMEI和邮箱之间的pair关系都有一个预设的置信度。而所有的ID根据两两关联构成了一张图，那么每个ID的置信度根据这张网的结构传播给相关联的ID，同时也从其他ID那边接收置信度，而pair关系的置信度不变。当算法迭代收敛时，高置信度的ID就是可信的。同一个子图内的ID就标识了同一个用户。用类似的算法，我们也可以评价每个数据源的质量等。<br>　　第三个层面是基于用户兴趣做相似用户的合并<br>　　如果说层面二主要还在判断标识一个用户的ID是否正确，那么层面三致力于把行为相似的用户给合并起来。<br>　　例如，某一个用户的设备多次连接同一个Wi-Fi网关，但是每次链接都会随机更换ID，那么相当于这个用户的数据“分裂”在多个不同ID下。那么如何把这些ID合并成同一个用户呢？<br>　　除了上述做法之外，集奥聚合开发了相似用户合并技术。基于用户的上网时间偏好、网址访问偏好、点击行为偏好、浏览行为偏好、APP偏好和社交账号偏好等，为每个用户提取了上千个特征之后，进行相似用户的聚类。<br>　　聚类中选择类中心附近的用户，再加上一些辅助准则判定，就可以把用户合并起来。<br>　　经实际测试，可以把用户ID总量减少80%，同时保持用户合并的准确率在91%以上。使用的历史数据时间窗越长就越精准。仅此一项就能让用户的标签密度提升 500%。最早出现于安卓<br>深入浅出理解 Cookie Mapping<br>Posted on 2014 年 11 月 9 日 by Abbo<br>在RTB（实时竞价广告，Real-Time-Bidding）广告领域（当然实际上不仅仅是这个领域），有一个常见的词汇叫 Cookie Mapping（Cookie 匹配），一会又是DSP（需求方供应平台）与DSP的Cookie Mapping，一会又是DSP与Ad Exchange的Cookie Mapping，一会还有DMP（数据管理平台）与DSP的Cookie Mapping，已经完全把大家搞浑了。许多互联网广告从业者都不清楚到底什么是 Cookie Mapping，到底又是为什么要 Cookie Mapping。今天就以小小的笔记，分享大家疑问的解答。<br>用户唯一标识体系<br>在互联网中，我们有着许多标识唯一用户的技术手段，其中，最为常见的就是 Cookie 了（什么是Cookie请参看网站分析中的Cookie）。简单的多，Cookie具备几个特征：<br>•    唯一性，一个Cookie是唯一存在于一个域名下的；<br>•    归属权，一个Cookie必须属于某一个域名，且相互不能访问使用；<br>•    持久性，一个Cookie可以持久的存在于一个浏览器中。<br>正因为Cookie具备上述几个特征，也就衍生出Cookie在使用上的一些特点了，我们以DSP.COM（广告购买平台），ADX.COM（广告交易平台），DMP.COM（数据管理平台）为例，存在以下结论：<br>•    DSP.COM，ADX.COM，DMP.COM都存在各自的用户标识体系（各自定义的唯一ID标识）；<br>•    用户Abbo在上述三个产品的ID分别是dsp-1，adx-a，dmp-①，且相互不能访问使用。<br>就这样，DSP.COM，ADX.COM，DMP.COM都可以唯一的标识出用户Abbo，但他们并不能互相读取标识信息。<br>共享用户特征<br>由于客户需求，广告主在DSP.COM，ADX.COM，DMP.COM均有业务存在：<br>•    广告主使用DSP.COM进行广告投放，并且用户Abbo点击了游戏广告；<br>•    用户Abbo主动使用了DMP.COM提供的浏览器购物比价插件服务；<br>•    用户Abbo点击过位于交易平台ADX.COM上的职业学习、求职类广告；<br>刚好，DSP.COM识别出了Abbo喜欢玩游戏特征，DMP.COM识别出了Abbo是男性用户，ADX.COM识别出了Abbo是个年轻人。此时问题来了，由于三方的数据并不共享，因此对于广告主而言，仅知道dsp-1喜欢玩游戏，adx-a是年轻人，dmp-①是男性用户。广告主并不能直接知道Abbo是个喜爱玩游戏的年轻男性。<br>最终目标，我们需要不同产品体系中的用户的特征，合并绑定到一个用户上来，这也就是本文主题的关键——Cookie Mapping。<br>常见 Mapping 方式<br>我们刚刚看到，不同厂商、产品对用户都使用了不同的标识体系，诸如dsp-a，adx-a，dmp-①此类。因此，我们在Cookie Mapping中的最为基础的信息表——ID映射关系，俗称Cookie Mapping表。它负责使dsp-1，adx-a，dmp-①关联起来。<br>要使同一个用户在不同体系中关联起来，只有一个做法，那就是当用户发生行为的时候，同时能够联通多家厂商、产品。也就是出现了以下最常见的几种Mapping方式生成ID映射关系表：<br>•    用户加载网页代码时候，同时加载DSP.COM，ADX.COM，DMP.COM的代码，互相调用Mapping接口传输ID信息；（客户端Mapping）<br>•    用户加载网页代码时候，由服务端转发携带ID的请求，由ADX.COM服务器告诉DSP.COM相关ID信息。（服务端Mapping）<br>这样一来，经过大量的Mapping匹配后，不同厂商、产品之间也就自然形成了一套对应ID映射关系表格了。<br>移动端的 Mapping<br>移动终端的发展趋势，Cookie的效果已经远不如PC端了——PC端的用户上网行为，往往发生在一两款Web浏览软件（浏览器）中，而移动端App较为分散，用户行为、特征体现在更多的应用程序（App）上。况且，移动终端的唯一性，存在着更多的ID体系标识唯一用户，诸如MAC地址、iOS IDFA、Android ID等等。这些ID往往是具备一定唯一性，并且能够在不同App中共享的标识信息。因此，移动终端有时候也不需要 Mapping，如果约定俗成的使用某一类ID也是可以进行唯一用户标识的。<br>斗胆小结<br>斗胆小结本文，观点并不一定全部正确，如有不足，还请点出：<br>•    唯一标识需求将长期存在；<br>•    Cookie标识在PC端短期内（10年）不会消失；<br>•    多终端的发展，将出现更多标识体系；<br>•    Mapping ID的需求将长期存在。<br>Open-ID是一个很好的想法，也是一个很好的应用，特别是第三方开源Open-ID产品，个人觉得还是值得一</p>
<p>一点做用户画像的人生经验（一）：ID强打通</p>
<ol>
<li>背景<br>在构建精准用户画像时，面临着这样一个问题：日志采集不能成功地收集用户的所有ID，且每条业务线有各自定义的UID用来标识用户，从而造成了用户ID的零碎化。因此，为了做用户标签的整合，用户ID之间的强打通（亦称为ID-Mapping）成了迫切的需求。大概三年前，在知乎上有这样一个与之相类似的问题：如何用MR实现并查集以对海量数据pair做聚合；目前为止还无人解答。本文将提供一个可能的解决方案——如何用MR计算框架来实现大数据下的ID强打通。<br>首先，简要地介绍下Android设备常见的ID：<br>•    IMEI（International Mobile Equipment Identity），即通常所说的手机序列号、手机“串号”，用于在移动电话网络中识别每一部独立的手机等行动通讯装置；序列号共有15位数字，前6位（TAC）是型号核准号码，代表手机类型。接着2位（FAC）是最后装配号，代表产地。后6位（SNR）是串号，代表生产顺序号。最后1位（SP）一般为0，是检验码，备用。<br>•    MAC(Media Access Control)一般代指MAC位址，为网卡的标识，用来定义网络设备的位置。<br>•    IMSI（International Mobile SubscriberIdentification Number），储存在SIM卡中，可用于区别移动用户的有效信息；其总长度不超过15位，同样使用0～9的数字。其中MCC是移动用户所属国家代号，占3位数字，中国的MCC规定为460；MNC是移动网号码，最多由两位数字组成，用于识别移动用户所归属的移动通信网;MSIN是移动用户识别码，用以识别某一移动通信网中的移动用户。<br>•    Android ID是系统随机生成的设备ID 为一串64位的编码（十六进制的字符串），通过它可以知道设备的寿命（在设备恢复出厂设置或刷机后，该值可能会改变）。</li>
<li>设计<br>从图论的角度出发，ID强打通更像是将小连通图合并成一个大连通图；比如，在日志中出现如下三条记录，分别表示三个ID集合（小连通图）：<br>A   B   C<pre><code>C   D
    D   E
</code></pre>通过将三个小连通图合并，便可得到一个大连通图——完整的ID集合列表A B C D E。淘宝明风介绍了如何用Spark GraphX通过outerJoinVertices等运算符来做大数据下的多图合并；针对ID强打通的场景，也可采用类似的思路：日志数据构建大的稀疏图，然后采用自join的方式做打通。但是，我并没有选用GraphX，理由如下：<br>•    GraphX只支持有向图，而不支持无向图，而ID之间的关联关系是一个无向连通图；<br>•    GraphX的join操作不完全可控，“不完全可控”是指在做图合并时我们需要做过滤山寨设备、一对多的ID等操作，而在GraphX封装好的join算子上实现过滤操作则成本过高。<br>因而，基于MR计算模型（Spark框架）我设计新的ID打通算法；算法流程如下：打通的map阶段将ID集合id_set中每一个Id做key然后进行打散（id_set.map(id -&gt; id_set))），Reduce阶段按key做id_set的合并。通过观察发现：仅需要两步MR便可完成上述打通的操作。以上面的例子做说明，第一步MR完成后，打通ID集合为：A B C D、 C D E，第二步MR完成后便得到完整的ID集合列表A B C D E。但是，在两步MR过程中，所有的key都会对应一个聚合结果，而其中一些聚合结果只是中间结果。故而引入了key_set用于保存聚合时的key值，加入了第三步MR，通过比较key_set与id_set来对中间聚合结果进行过滤。算法的伪代码如下：<br>MR step1:<br> Map: <pre><code>input: id_set
process: flatMap id_set;
output: id -&gt; (id_set, 1)
</code></pre> Rduce:<pre><code>process: reduceByKey
output: id -&gt; (id_set, empty key_set, int_value)
</code></pre></li>
</ol>
<p>MR step2:<br>    Map:<br>        input: id -&gt; (id_set, empty key_set, int_value)<br>        process: flatMap id_set, if have id_aggregation, then add key to key_set<br>        output: id -&gt; (id_set, key_set, int_value)<br>    Reduce:<br>        process: reduceByKey<br>        output: id -&gt; (id_set, key_set, int_value)</p>
<p>MR step3:<br>    Map:<br>        input: id -&gt; (id_set, empty key_set, int_value)<br>        process: flatMap id_set, if have id_aggregation, then add key to key_set<br>        output: id -&gt; (id_set, key_set, int_value)<br>    Reduce:<br>        process: reduceByKey<br>        output: id -&gt; (id_set, key_set, int_value)</p>
<p>Filters:<br>    process: if have id_aggregation, then add key to key_set<br>    filter: if no id_aggregation or key_set == id_set<br>    distinct</p>
<ol start="3">
<li>实现<br>针对上述ID强打通算法，Spark实现代码如下：<br>case class DvcId(id: String, value: String)<br>val log: RDD[mutable.Set[DvcId]]// MR1val rdd1: RDD[(DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))] = log<br>.flatMap { set =&gt;<br> set.map(t =&gt; (t, (set, 1)))<br>}.reduceByKey { (t1, t2) =&gt;<br> t1._1 ++= t2._1<br> val added = t1._2 + t2._2<br> (t1._1, added)<br>}.map { t =&gt;<br> (t._1, (t._2._1, mutable.Set.empty[DvcId], t._2._2))<br>}// MR2val rdd2: RDD[(DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))] = rdd1<br>.flatMap(flatIdSet).reduceByKey(tuple3Add)// MR3val rdd3: RDD[(DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))] = rdd2<br>.flatMap(flatIdSet).reduceByKey(tuple3Add)// filterval rdd4 = rdd3.filter { t =&gt;<br>t._2._2 += t._1<br>t._2._3 == 1 || (t._2._1 – t._2.<em>2).isEmpty<br>}.map(</em>._2._1).distinct()<br>// flat id_setdef flatIdSet(row: (DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))) = {<br>row._2._3 match {<br> case 1 =&gt;<br>   Array((row._1, (row._2._1, row._2._2, row._2._3)))<br> case _ =&gt;<br>   row._2._2 += row._1 // add key to keySet<br>   row._2._1.map(d =&gt; (d, (row._2._1, row._2._2, row._2._3))).toArray<br>}<br>}<br>def tuple3Add(t1: (mutable.Set[DvcId], mutable.Set[DvcId], Int),<pre><code>t2: (mutable.Set[DvcId], mutable.Set[DvcId], Int)) = {
</code></pre>t1._1 ++= t2._1<br>t1._2 ++= t2._2<br>val added = t1._3 + t2._3<br>(t1._1, t1._2, added)<br>}<br>其中，引入常量1是为了标记该条记录是否发生了ID聚合的情况。<br>ID强打通算法实现起来比较简单，但是在实际的应用时，日志数据往往是带噪声的：<br>•    有山寨设备；<br>•    ID之间存在着一对多的情况，比如，各业务线的UID的靠谱程度不一，有的UID会对应到多个设备。<br>另外，ID强打通后是HDFS的离线数据，为了提供线上服务、保证ID之间的一一对应关系，应选择何种分布式数据库、表应如何设计、如何做到数据更新时而不影响线上服务等等，则是另一个需要思考的问题。</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2018 rongyuewu
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>