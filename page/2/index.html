<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">rongyuewu</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="https://github.com/smackgg/hexo-theme-smackdown">smackdown</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">rongyuewu</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">rongyuewu</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-storm的架构" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/storm的架构/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/storm的架构/">
        storm
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="storm"><a href="#storm" class="headerlink" title="storm"></a>storm</h1><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/t7Rbr8aYgIO4*9TbdsjJMgxTPY8JHwAmijtuJfP2rbo!/b/dEMBAAAAAAAA&amp;bo=wAY4BAAAAAADN.g!&amp;rf=viewer_4" alt=""></p>
<h3 id="Storm架构"><a href="#Storm架构" class="headerlink" title="Storm架构"></a>Storm架构</h3><pre><code>类似于Hadoop的架构，主从(Master/Slave)
Nimbus: 主
    集群的主节点，负责任务(task)的指派和分发、资源的分配
Supervisor: 从
    可以启动多个Worker，具体几个呢？可以通过配置来指定
    一个Topo可以运行在多个Worker之上，也可以通过配置来指定
    集群的从节点，(负责干活的)，负责执行任务的具体部分
    启动和停止自己管理的Worker进程
无状态，在他们上面的信息(元数据)会存储在ZK中
Worker: 运行具体组件逻辑(Spout/Bolt)的进程
=====================分割线===================
task： 
    Spout和Bolt
    Worker中每一个Spout和Bolt的线程称为一个Task
executor： spout和bolt可能会共享一个线程
</code></pre><h5 id="提交代码到集群："><a href="#提交代码到集群：" class="headerlink" title="提交代码到集群："></a>提交代码到集群：</h5><pre><code>storm jar /home/hadoop/lib/storm-1.0.jar com.bigdata.ClusterSumStormTopology
</code></pre><h5 id="storm-其他命令的使用"><a href="#storm-其他命令的使用" class="headerlink" title="storm 其他命令的使用"></a>storm 其他命令的使用</h5><pre><code>list
    Syntax: storm list
    List the running topologies and their statuses.
如何停止作业
    kill
        Syntax: storm kill topology-name [-w wait-time-secs]
</code></pre><p>并行度</p>
<pre><code>一个worker进程执行的是一个topo的子集
一个worker进程会启动1..n个executor线程来执行一个topo的component
一个运行的topo就是由集群中多台物理机上的多个worker进程组成

executor是一个被worker进程启动的单独线程，每个executor只会运行1个topo的一个component
task是最终运行spout或者bolt代码的最小执行单元

默认：
    一个supervisor节点最多启动4个worker进程  
    每一个topo默认占用一个worker进程         
    每个worker进程会启动一个executor        
    每个executor启动一个task   

Total slots:4  
Executors: 3   但是stormUI上是 spout + bolt = 2  why 3?
隐藏的acker 导致的
</code></pre><h5 id="通过代码对并行度的理解："><a href="#通过代码对并行度的理解：" class="headerlink" title="通过代码对并行度的理解："></a>通过代码对并行度的理解：</h5><pre><code>Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes
topologyBuilder.setSpout(&quot;blue-spout&quot;, new BlueSpout(), 2); // set parallelism hint to 2
topologyBuilder.setBolt(&quot;green-bolt&quot;, new GreenBolt(), 2)
           .setNumTasks(4)
           .shuffleGrouping(&quot;blue-spout&quot;);topologyBuilder.setBolt(&quot;yellow-bolt&quot;, new YellowBolt(), 6)
           .shuffleGrouping(&quot;green-bolt&quot;);StormSubmitter.submitTopology(
    &quot;mytopology&quot;,
    conf,
    topologyBuilder.createTopology()
);

##解释：
conf.setNumWorkers(2) ---&gt;两个worker
setSpout(&quot;blue-spout&quot;, new BlueSpout(), 2)---&gt; 2个spout executor 对应默认的2个task
setBolt(&quot;green-bolt&quot;, new GreenBolt(), 2).setNumTasks(4) ---&gt; 2个executor 4个task(因为它指定了setNumTasks个数)

结果：
1个topology
2个workers  
2+2+2[2个worker 就有2个acker] = 6个executors  
2+4+2[2个worker 就有2个acker默认2个task] = 8个task
</code></pre><h5 id="修改正在运行的topology的并行度："><a href="#修改正在运行的topology的并行度：" class="headerlink" title="修改正在运行的topology的并行度："></a>修改正在运行的topology的并行度：</h5><pre><code>#Reconfigure the topology &quot;mytopology&quot; to use 5 worker processes,
#the spout &quot;blue-spout&quot; to use 3 executors 
#the bolt &quot;yellow-bolt&quot; to use 10 executors.

1.可以使用StormUI来rebalance the topology.
2.也可以使用命令行来修改：$ storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10
</code></pre><p>stream grouping分组策略内置[build-in]有8种：</p>
<pre><code>常用的有两种：
1.shuffle grouping 随机分配也就是轮询RoundRobin 这样不会造成数据倾斜
2.fileds grouping 比如按照字段hash取模分组 
如果想自定义分组策略：--&gt; 自定义分组策略需要实现CustomStreamGroup接口
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-点击流日志分析流程" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/点击流日志分析流程/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/点击流日志分析流程/">
        点击流日志分析流程
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="点击流日志分析流程"><a href="#点击流日志分析流程" class="headerlink" title="点击流日志分析流程"></a>点击流日志分析流程</h1><p>流程图如下：<br>    <img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/UbKrmZeyW3FpirSZBEMFrDGawneMp6lIpLWKfxekmLw!/b/dJUAAAAAAAAA&amp;bo=QgauAgAAAAARB9g!&amp;rf=viewer_4" alt=""><br>    <img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/FimQ39x6PIwqFTl56xYF*n.qFv3LMDsUx5Bk7YsN*vs!/b/dGoBAAAAAAAA&amp;bo=kgU4BAAAAAARB5s!&amp;rf=viewer_4" alt=""></p>
<h6 id="原始数据："><a href="#原始数据：" class="headerlink" title="原始数据："></a>原始数据：</h6><pre><code>194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] &quot;GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/4.0 (compatible;)&quot;
字段解析:
1、访客 ip 地址: 58.215.204.118
2、访客用户信息: - -
3、请求时间:[18/Sep/2013:06:51:35 +0000]
4、请求方式:GET
5、请求的 url:/wp-includes/js/jquery/jquery.js?ver=1.10.2 6、请求所用协议:HTTP/1.1
7、响应码:304
8、返回的数据流量:0
9、访客的来源 url:http://blog.fens.me/nodejs-socketio-chat/
10 、 访 客 所 用 浏 览 器 : Mozilla/5.0 (Windows NT 5.1; rv:23.0) Firefox/23.0
Gecko/20100101
</code></pre><p>1.根据原始数据生成点击流模型 PageViews 和 Visits</p>
<pre><code>PageViews:
    根据IP判断是否是同一用户，根据前后两条日志时间相差是否在30分钟内，
    判断访问日志是否是属于同一个session[会话]，按照时间顺序标上步骤，
    这样就构成了一条访问轨迹线;
Visits：
    侧重于体现用户在一次session中的进入离开时间、进入离开页面，
    还有统计出在本次session中用户总共访问了几个页面
</code></pre><p>2.漏斗模型</p>
<pre><code>逐层递减
</code></pre><p>3.常见指标：</p>
<pre><code>骨灰级指标：
    IP：1天内访问网站的不重复IP总数
    PV[PageView]:用户每打开1次网页，记录1个PV
    UV[Unique Pageview]:1天以内，访问网站不重复的用户数据(以cookie为依据)，1天内同1访客多次访问网站只被计算1次
基础级指标：
    访问次数：
        访客从进入网站到离开网站一系列活动极为一次访问，也就是session
    网站停留时间：
        访问者在网站上花费的时间
    页面停留时间：
        访问者在某个特定页面或某组网页上所花费的时间
复合级指标:
    人均浏览页面：
        浏览次数/独立访客数  --体现网站对访客的吸引程度
    二跳率：
        二跳率的概念是当网站页面展开后，用户在页面上产生的首次点击被称为“二跳”，二跳的次数即为“二跳量”。二跳量与到达量（进入网站的人）的比值称为页面的二跳率。
    跳出率：
        跳出率是指在只访问了入口页面（例如网站首页）就离开的访问量与所产生总访问量的百分比。跳出率计算公式：跳出率=访问一个页面后离开网站的次数/总访问次数。
    二跳率越高越好，跳出率越低越好。
4.基础分析(PV,IP,UV)
5.来源分析
6.受访分析
7.访客分析
    终端详情[PC,移动端]新老访客、忠诚度、活跃度
8.转化路径分析
</code></pre><blockquote>
<blockquote>
<blockquote>
<p>数据处理流程：</p>
</blockquote>
</blockquote>
</blockquote>
<pre><code>&gt; 数据采集
    Flume采集需要在配置文件里配置source、channel、sink：
        source:
            1.spoolDir的作用是：监控文件夹，如果有新的文件产生，采集开始
            2.exec tail -f access.log 只能监听文件追加的内容
    以上1和2都没办法满足我们的log日志采集，因为既要监控文件也要监控文件夹，
    **好在Flume1.7的稳定版本提供了TAILDIR类型的source，
    可以监控一个目录，并且使用正则表达式匹配目录中的文件名进行实时收集，具体配置详情如下：
        a1.sources.r1.type = TAILDIR
        a1.sources.r1.positionFile = /var/log/flume/taildir_position.json
        a1.sources.r1.filegroups = f1 f2
        a1.sources.r1.filegroups.f1 = /var/log/test1/example.log 
        a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*
    解释：
        filegroups:指定 filegroups，可以有多个[每个还可以使用正则表达式来匹配]，
        以空格分隔;(TailSource 可以同时监控 tail 多个目录中的文件)
      **positionFile:解决了机器重启后无法**断点续传**的问题[检查点文件会以 json 格式保存已经 tail 文件的位置]

&gt; 数据预处理
    通过MapReduce程序对采集到的原始日志数据进行预处理，
    比如清洗，格式整理，滤除脏数据等，并且梳理成点击流模型数据
        1.一般来说，开发中针对不合法的数据，我们不是直接删除，而是打个标签 比如true或者false，
        因为这些数据可能对这个场景是无用的，但是对其他场景是有用的
        2.编写MapReduce程序，只有map没有reduce，因为输入一条数据，处理完后直接输出不需要聚合,setReduceNum = 0 ,输出的结果文件就是part-m
        3.编写相对应的Javabean的时候要实现Writable接口，
        重写toString方法的时候是按照Hive的默认分隔符&apos;\001&apos;进行分割的，
        导入hive表的时候直接按照默认的分隔符就ok了,
        注意：readFields 和 write 的方法写得时候要一致对应
        4.业务要求状态码为400以上的设置valid为false，还有时间不合法[为null或者双引号]
        5.过滤掉静态资源[图片/css/js],这个标准是根据业务来定的，一般会在mapper的set up 
        初始化方法里定义hashSet来存这些准则，然后进行标记清除
        6.PageViews数据生成： (session[UUID] + stayTime + step)
        ---------------------------------------------------------------------------------------
                    Session + IP + 地址 + 时间 + 访问页面 + URL + 停留时长 + 第几步
        ---------------------------------------------------------------------------------------
            a) 编写MapReduce程序，map端以Ip为key，Javabean为value，发送到reduce，
            reduce端进行values的排序，这里需要遍历values，每一次都new
            一个新的Javabean然后进行赋值[因为Javabean是引用类型，然后添加入新的ArrayList中，
            如果不重新new 一个新的对象的话，那么到最后ArrayList里面的对象就是同一个，因为他们指向的都是同一块堆内存！！！]，
            再把这个Javabean添加到新的ArrayList中
            b) 按照时间排序,然后对新的ArrayList进行排序，Collections.sort(beansList,new Comparabtor(javabean){中间获取时间来进行升序排序})
            c) 从有序的beans中分辨出歌词visit，并对一次visit中所访问的page按顺序标号step
                核心思想：比较相邻两条记录中的时间差，如果时间差&lt;30分钟，则该两条记录属于同一个session[生成UUID]，否则属于不同的session
                只有1条的和大于1条的，他们的默认时间都是60秒
        7.Visits模型：数据来自PageViews模型
        ---------------------------------------------------------------------------------------
        Session + 起始时间 + 结束时间 + 进入页面URL + 离开页面URL + 访问页面数 + 停留时长 + IP + referer
        ---------------------------------------------------------------------------------------
             编写MapReduce程序：
                 mapper端：k为session，value：javabean
                 reduce端：
                     以step进行排序


&gt; 数据入库
    将预处理之后的数据导入到Hive仓库中相应的库和表中
&gt; 数据分析
    项目的核心内容，即根据需求开发ETL分析语句，得出各种统计结果
&gt; 工作流调度：
    简单的任务调度:
        可以使用Linux的crontab -e 来设置调度，但是其缺点是无法设置依赖
    复杂的任务调度：
        推荐使用 : azkaban [Java语言实现的，他有管理页面，配置起来比较简单]
            azkaban是由LinkedIn公司推出的一个批量工作流任务调度器，
            用于在一个工作流内以一个特定的顺序运行一组工作和流程。
            使用job配置文件简历任务之间的依赖关系，并提供了一个已使用的web用户界面维护和跟踪工作流。
            支持command、Java、Hive、pig、Hadoop，而且是基于java开发，代码结构清晰，抑郁二次开发
                azkaban的组成：
                    1.mysql服务器
                        用于存储项目、日志或者执行计划[执行周期等]之类的信息
                    2.web服务器：
                        使用jetty[开源的serverlet容器]对外提供web服务，使用户可以通过wen页面方便管理
                    3.executor服务器：
                        负责具体的工作流的提交、执行
           配置azkaban步骤：[cluster模式]
                   1.生成keystore证书文件，mv到webserver下
                   2.配置为年修改一下时区：Asia/Shanghai
                   3.配置数据库mysql
                   4.配置user admin的登录
          使用azkaban的步骤：
                  1.创建a.job文件并且已经配置b.job ，里面的type为command，中间可以配置的dependencies=b，然后command=xxxx
                    期间要把这些job打成zip包，通过web提交上去配置立刻执行还是scheduler定期执行
                      # a.job
                    type=command
                    dependencies=b
                    command=echo hahaha
                这样的话a的job任务就会等待b结束后再执行
                2.hdfs操作任务
                    command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop fs -mkdir /azaz
                  3.MapReduce操作任务
                      command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop jar hadoop-mapreduce- examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout
                  4.hive操作任务
                      执行一个命令是 command=/xx/hive -e &apos;show tables&apos;
                      执行一个文件，里面是hive sql语句， commmand=/xx/hive -f &apos;test.sql&apos;
                      Hive 脚本: test.sql
                        type=command
                        command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop jar hadoop-mapreduce- examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout
                        use default;
                        drop table aztest;
                        create table aztest(id int,name string) row format delimited fields terminated by &apos;,&apos;;
                        load data inpath &apos;/aztest/hiveinput&apos; into table aztest;
                        create table azres as select * from aztest;
                        insert overwrite directory &apos;/aztest/hiveoutput&apos; select count(1) from aztest;
        不推荐使用: ooize[虽然是Apache旗下的，但是工作流的过程是编写大量的XML文件配置，而且代码复杂度比较高，不易于二次开发]
&gt; 数据展现
    将分析所得到的数据进行数据可视化，一般通过图表[百度的echarts]进行展示
</code></pre><h2 id="模块开发-数据仓库的设计"><a href="#模块开发-数据仓库的设计" class="headerlink" title="模块开发-数据仓库的设计"></a>模块开发-数据仓库的设计</h2><pre><code>1.纬度建模
    纬度表(demension)
        通常指 按照类别、区域或者时间等等来分析，维度表数据比较固定，数据量小
     事实表
        事实表的设计是以能够正确记录历史信息为准则也就是一条一条的数据，就像是消费记录里面有product_id
        维度表的设计是以能够以合适的角度来聚合主题内容为准则  这边有product_id对应的产品信息
2.纬度建模三种模式：
    2.1 星型模式 [像星星一样]
            由一个事实表和一组维度表组成    
                比如：
                    事实表里有地域主键、时间键、部门键、产品键 对应有4个维度表相关联
    2.2 雪花模式[不常用，因为不容易维护！！！]
            在星型模式基础上，维度表还有维度表
    2.3 星座模式 [开发常用！！！]
        基于多张事实表，而且共享纬度信息
</code></pre><p>本项目数据仓库的设计：</p>
<pre><code>1.事实表的设计 ods_weblog_orgin =&gt; 对应mr清洗完之后的数据 【窄表】和【宽表或者明细表】
    窄表：对应原始数据表，字段跟数据中一一对应，但是不利于分析
---------------------------------------------------------------------------------------------------------------
        valid  remote_addr remote_user time_local  request status  body_bytes_sent http_referer  http_user_agent
        是否有效 访客IP         访客用户信息  请求时间     请求url  响应码    相应字节数       来源url         访客终端信息
---------------------------------------------------------------------------------------------------------------
    宽表：把某些融合各种信息的字段 提取出不同的信息作为新的字段
        相对于之前的窄表 字段增加了，所以叫宽表，
        比如时间戳，如果是之前的话 &apos;2018-09-09 18:09:09&apos;这种时间不利于分析，
        如果分成年，月，日，那么分析时直接group by day 或者 year 或者day 就ok了
        还有referer_url也是如此，可以拆分为host或者参数之类的

2.维度表的设计如：
    时间维度 t_dim_time: date_key year month day hour 
    访客地域纬度t_dim_area: area_ID 北京 上海 广州 深圳
    终端类型维度 t_dim_termination: uc firefox chrome safari ios android
    网站栏目纬度 t_dim_section: 进口食品、生鲜日配、时令果蔬、奶制品、
                                休闲保健、酒饮冲调茶叶、粮油副食、母婴玩具、个护清洁、家具家电
    维度表的数据一般要结合业务情况自己写脚本按照规则生成，也可以用工具来生成，方便后续关联分析
    比如事先生成时间维度表中的数据，跨度从业务需求的日期到当前的日期即可，具体根据分析粒度，
    库生成年，季，月，周，天，时等相关信息，用于分析
</code></pre><p>数据仓库三层架构：</p>
<pre><code>ods层：数据就是通过mr清洗过的数据，带有标签valid或者标识的数据
    1.创建ODS层数据表
        1.1. 原始日志数据表 :创建按照时间来分区的hive 分区表
            drop table if exists ods_weblog_origin;
            create table ods_weblog_origin
            (
            valid string,remote_addr string,remote_user string, time_local string,request string,status string, body_bytes_sent string, http_referer string, http_user_agent string
            )
            partitioned by (datestr string)
            row format delimited 
            fields terminated by &apos;\001&apos;;
        1.2. 点击流模型 PageViews表
            drop table if exists ods_click_pageviews;
            create table ods_click_pageviews
            (
            session string,remote_addr string,remote_user string, time_local string,request string,visit_step string, page_staylong string, http_referer string, http_user_agent string, body_bytes_sent string, status string
            )
            partitioned by (datestr string)
            row format delimited 
            fields terminated by &apos;\001&apos;;
        1.3. 点击流模型 Visits
            drop table if exists ods_click_visits;
            create table ods_click_visits
            (
            )
            partitioned by (datestr string)
            row format delimited 
            fields terminated by &apos;\001&apos;;
        1.4. 维度表创建(这里举例：时间，年、月、日、时)
            drop table if exists t_dim_time;
            create table t_dim_time 
            (
            date_key int,...
            )
            row format delimited 
            fields terminated by &apos;,&apos;;
        1.5 创建明细宽表 ods_weblog_detail 时间可以明细为 年月日时分秒，
            referer_url 可以明细为 host、path、query、queryid
            从预清洗后的表中得到这些数据，如果是referer_url 需要使用Hive里定义的函数：
                lateral view parse_url_tuple(正则表达式)这个方法，自动把url转换为host、path等
            如果是时间拓宽明细表的话 就是 substring
dw层：ods通过ETL处理之后得到dw层
        多维度统计PV总量：
            b) 与时间维度表关联查询
                insert into table dw_pvs_everyday select count(*) as pvs,a.month as month,a.day as day 
                from 
                (select distinct month, day from t_dim_time) a 
                join 
                ods_weblog_detail b 
                on a.month=b.month and a.day=b.day group by a.month,a.day;
            c) 按照referer维度进行统计每小时各来访 url 产生的 PV 量
                insert into table dw_pvs_referer_everyhour partition(datestr=&apos;20130918&apos;)
                select http_referer,ref_host,month,day,hour,count(1) as pv_referer_cnt
                from 
                ods_weblog_detail
                group by http_referer,ref_host,month,day,hour
                having ref_host is not null
                order by hour asc,day asc,month asc,pv_referer_cnt desc;
            d) 人均浏览量
                统计今日所有来访者平均请求的页面数。
                    insert into table dw_avgpv_user_everyday
                    select 
                    &apos;20130918&apos;,sum(b.pvs)/count(b.remote_addr) 
                    from
                    (select remote_addr,count(1) as pvs from ods_weblog_detail where datestr=&apos;20130918&apos; group by remote_addr) b; 
            e)特别重要：分组求TopN ************非常重要**********
                row_number()函数
                    row_number() over (partition by xxx order by xxx) rank
                insert into table dw_pvs_refhost_topn_everyhour partition(datestr=&apos;20130918&apos;) 
                select t.hour,t.od,t.ref_host,t.ref_host_cnts 
                from(
                select ref_host,ref_host_cnts,concat(month,day,hour) as hour,row_number() over (partition by concat(month,day,hour) order by ref_host_cnts desc
                ) as od 
                from 
                dw_pvs_refererhost_everyhour) t 
                where od&lt;=3;
            f) 受访分析(从页面的角度分析)
                热门页面统计
                    统计每日最热门的页面 top10
                        insert into table dw_hotpages_everydayselect &apos;20130918&apos;,a.request,a.request_counts 
                        from
                        (select request as request,count(request) as request_counts 
                        from 
                        ods_weblog_detail 
                        where datestr=&apos;20130918&apos; 
                        group by request having request is not null
                        ) a 
                        order by a.request_counts desc 
                        limit 10;
            g) 每小时独立访客及其产生的 pv
                insert into table dw_user_dstc_ip_h 
                select remote_addr,count(1) as pvs,concat(month,day,hour) as hour 
                from 
                ods_weblog_detail Where datestr=&apos;20130918&apos; 
                group by concat(month,day,hour),remote_addr;
                    在以上的结果基础上，统计每小时独立访客总数
                        select count(1) as dstc_ip_cnts,hour from dw_user_dstc_ip_h group by hour;
                    统计每日独立访客总数
                        select remote_addr,count(1) as counts,concat(month,day) as day 
                        from 
                        ods_weblog_detail Where datestr=&apos;20130918&apos; 
                        group by concat(month,day),remote_addr;
                    统计每月独立访客总数
                        select 
                        remote_addr,count(1) as counts,month 
                        from 
                        ods_weblog_detail 
                        group by month,remote_addr;
            h) 每日新访访客 today left join old ***************非常重要*************
                insert into table dw_user_new_d partition(datestr=&apos;20130918&apos;) 
                select tmp.day as day,tmp.today_addr as new_ip 
                from 
                ( select today.day as day,today.remote_addr as today_addr,old.ip as old_addr from (select distinct remote_addr as remote_addr,&quot;20130918&quot; as day from ods_weblog_detail where datestr=&quot;20130918&quot;) today left outer join dw_user_dsct_history old on today.remote_addr=old.ip ) tmp 
                where tmp.old_addr is null;
            注意：每日新用户追加到累计表
            i) 访客 Visit 分析(点击流模型)
                查询今日所有回头访客及其访问次数。
                    insert overwrite table dw_user_returning partition(datestr=&apos;20130918&apos;) 
                    select tmp.day,tmp.remote_addr,tmp.acc_cnt 
                    from 
                    (select &apos;20130918&apos; as day,remote_addr,count(session) as acc_cnt from ods_click_stream_visit group by remote_addr) tmp 
                    where tmp.acc_cnt&gt;1;
            j) 人均访问频次
                统计出每天所有用户访问网站的平均次数(visit)
                    select sum(pagevisits)/count(distinct remote_addr) from ods_click_stream_visit where datestr=&apos;20130918&apos;;
            k) 关键路径转化率分析(漏斗模型) -- 基于PageViews模型
                定义好业务流程中的页面标识，下例中的步骤为[模型设计]: Step1、 /item
                                                          Step2、 /category
                                                         Step3、 /index
                                                        Step4、 /order
                --查询每一步人数存入 dw_oute_numbs
                create table dw_oute_numbs as
                select &apos;step1&apos; as step,count(distinct remote_addr) and request like &apos;/item%&apos;
                union
                select &apos;step2&apos; as step,count(distinct remote_addr) and request like &apos;/category%&apos;
                union
                select &apos;step3&apos; as step,count(distinct remote_addr) and request like &apos;/order%&apos;
                union
                select &apos;step4&apos; as step,count(distinct remote_addr) and request like &apos;/index%&apos;;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                注:UNION 将多个 SELECT 语句的结果集合并为一个独立的结果集。
                ***利用级联求和自己和自己join ******************非常重要********************
                inner join
                select abs.step,abs.numbs,abs.rate as abs_ratio,rel.rate as leakage_rate
                from
                (
                select tmp.rnstep as step,tmp.rnnumbs as numbs,tmp.rnnumbs/tmp.rrnumbs as rate
                from
                (
                select rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs inner join
                dw_oute_numbs rr) tmp
                where tmp.rrstep=&apos;step1&apos;
                ) abs
                left outer join
                (
                select tmp.rrstep as step,tmp.rrnumbs/tmp.rnnumbs as rate
                from
                (
                select rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs inner join
                dw_oute_numbs rr) tmp
                where cast(substr(tmp.rnstep,5,1) as int)=cast(substr(tmp.rrstep,5,1) as int)-1
                ) rel
                on abs.step=rel.step;
                from dw_oute_numbs rn
                其中 cast(substr(tmp.rnstep,5,1) as int) 是 把字符串截取字符 然后强制转化为int
            ) 还可以按照栏目纬度和UA（user agent）纬度来分析PV,
                为了说明PV是可以从各个纬度去分析的

app层：应用层来拿数据展示
</code></pre><p>Sqoop：是Hadoop和关系数据库服务器之间传送数据的一种工具-sql到Hadoop和Hadoop到sql</p>
<pre><code>sqoop工作机制是将导入或导出命令翻译成MapReduce程序来实现，
在翻译出的MapReduce中主要是对inputformat和outputformat进行定制
1.从关系型数据库(mysql)导入到hadoop 是 DBIputformat，import命令
    bin/sqoop import \
    --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
    --password hadoop \
    --target-dir /sqoopresult \ //--target-dir 可以用来指定导出数据存放至 HDFS 的目录;
    --table emp --m 1   //m 1 表示一个map来跑
2.导入 mysql 表数据到 HIVE
    2.1 将关系型数据的表结构复制到 hive 中
    bin/sqoop create-hive-table \
    --connect jdbc:mysql://node-21:3306/sqoopdb \ --table emp_add \
    --username root \
    --password hadoop \
    --hive-table test.emp_add_sp
    2.2 以上只是复制表的结构，并没有将数据导进去，将数据导入Hive表中
        bin/sqoop import \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --table emp_add \
        --hive-table test.emp_add_sp \
        --hive-import \   ****
        --m 1
    2.3 复杂查询条件:如果不指定分隔符是默认逗号
        bin/sqoop import \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --target-dir /wherequery12 \
        --query &apos;select id,name,deg from emp WHERE --split-by id \
        --fields-terminated-by &apos;\t&apos; \
        --m 1
    2.4 下面的命令用于在 EMP 表执行增量导入:
        bin/sqoop import \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --table emp --m 1 \
        --incremental append \  ****
        --check-column id \  ****
        --last-value 1205    ****
    3. Sqoop 导出
        将数据从 HDFS 导出到 RDBMS 数据库导出前，目标表必须存在于目标数据库中。
        bin/sqoop export \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --table employee \
        --export-dir /emp/emp_data
        还可以用下面命令指定输入文件的分隔符
        --input-fields-terminated-by &apos;\t&apos;
</code></pre><h4 id="工作流调度："><a href="#工作流调度：" class="headerlink" title="工作流调度："></a>工作流调度：</h4><pre><code>整个项目的数据按照处理过程，从数据采集到数据分析，再到结果数据的到处，
一系列的任务可以分割成若干个azkaban的job单元，然后由工作流调度器调度执行。
调度脚本的编写难点在于shell脚本
shell脚本大体框架如下：
    #!/bin/bash
    #set java env
    #set hadoop env
    #设置一些主类、目录等常量
    #获取时间信息
    #shell 主程序、结合流程控制(if....else)去分别执行 shell 命令。 更多工作流及 hql 脚本定义见参考资料。
    hive -e执行sql语句
</code></pre><h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><pre><code>Echarts：
    百度前端技术部开发的，基于JavaScript的数据可视化图标库，
    可以构建折线图(区域图)、柱状 图(条状图)、散点图(气泡图)、饼图(环形图)、
    K 线图、地图、力导向布局图以及和弦图， 同时支持任意维度的堆积和多图表混合展现。
javaEE中web.xml 的&lt;url-pattern&gt;/&lt;/url-pattern&gt; 是拦截所有，jsp除外

1.Mybatis example 排序问题 example.setOrderByClause(&quot;`dateStr` ASC&quot;);
查询结果便可以根据 dataStr 字段正序排列(从小到大)
如何区分不同数据仓库层的表：
2.Echarts 前端数据格式问题
注意，当异步加载数据的时候，前端一般需要的是数据格式是数组。一定要对应上。在 这里我们可以使用 Java Bean 封装数据，然后转换成 json 扔到前端，对应    上相应的字段即 可。
ObjectMapper om = new ObjectMapper(); beanJson = om.writeValueAsString(bean);
3.Controller 返回的 json @RequestMapping(value=&quot;/xxxx&quot;,produces=&quot;application/json;charset=UTF-8&quot;)
@ResponseBody    

一般使用第一种[业内默认的]
1.表之前加前缀 ods_T_access.log
                dw_T_access.log
2.针对不同的数据仓库层 建立对应的数据库 database
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-电商分析指标" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/电商分析指标/" class="article-date">
  	<time datetime="2018-04-05T13:29:30.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/电商分析指标/">
        电商常见分析指标
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>电商常见分析指标<br>信息流、物流和资金流三大平台是电子商务的三个最为重要的平台。而电子商务信息系统最核心的能力是大数据能力，包括大数据处理、数据分析和数据挖掘能力。无论是电商平台（如淘宝）还是在电商平台上销售产品的卖家，都需要掌握大数据分析的能力。越成熟的电商平台，越需要以通过大数据能力驱动电子商务运营的精细化，更好的提升运营效果，提升业绩。构建系统的电子商务数据分析指标体系是数据电商精细化运营的重要前提，本文将重点介绍电商数据分析指标体系。<br>电商数据分析指标体系分为八大类指标，包括总体运营指标、网站流量累指标、销售转化指标、客户价值指标、商品及供应链指标、营销活动指标、风险控制指标和市场竞争指标。不同类别指标对应电商运营的不同环节，如网站流量指标对应的是网站运营环节，销售转化、客户价值和营销活动指标对应的是电商销售环节。<br>1.电商总体运营指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaO3t.png" alt="CYaO3t.png"></p>
<p>电商总体运营整体指标主要面向的人群电商运营的高层，通过总体运营指标评估电商运营的整体效果。电商总体运营整体指标包括四方面的指标：</p>
<p>（1）流量类指标</p>
<p>独立访客数（UV），指访问电商网站的不重复用户数。对于PC网站，统计系统会在每个访问网站的用户浏览器上“种”一个cookie来标记这个用户，这样每当被标记cookie的用户访问网站时，统计系统都会识别到此用户。在一定统计周期内如（一天）统计系统会利用消重技术，对同一cookie在一天内多次访问网站的用户仅记录为一个用户。而在移动终端区分独立用户的方式则是按独立设备计算独立用户。</p>
<p>页面访问数（PV），即页面浏览量，用户每一次对电商网站或着移动电商应用中的每个网页访问均被记录一次，用户对同一页面的多次访问，访问量累计。<br>人均页面访问数，即页面访问数（PV）／独立访客数，该指标反映的是网站访问粘性。</p>
<p>（2）订单产生效率指标</p>
<p>总订单数量，即访客完成网上下单的订单数之和。<br>访问到下单的转化率，即电商网站下单的次数与访问该网站的次数之比。</p>
<p>（3）总体销售业绩指标</p>
<p>网站成交额（GMV），电商成交金额，即只要网民下单，生成订单号，便可以计算在GMV里面。<br>销售金额。销售金额是货品出售的金额总额。</p>
<p>注：无论这个订单最终是否成交，有些订单下单未付款或取消，都算GMV，销售金额一般只指实际成交金额，所以，GMV的数字一般比销售金额大。</p>
<p>客单价，即订单金额与订单数量的比值。</p>
<p>（4）整体指标</p>
<p>销售毛利，是销售收入与成本的差值。销售毛利中只扣除了商品原始成本，不扣除没有计入成本的期间费用（管理费用、财务费用、营业费用）。<br>毛利率，是衡量电商企业盈利能力的指标，是销售毛利与销售收入的比值。如京东的2014年毛利率连续四个季度稳步上升，从第一季度的10.0％上升至第四季度的12.7％，体现出京东盈利能力的提升。</p>
<p>2.网站流量指标<br><img src="https://s1.ax1x.com/2018/05/03/CYa6AJ.png" alt="CYa6AJ.png"></p>
<p>（1）流量规模类指标<br>常用的流量规模类指标包括独立访客数和页面访问数，相应的指标定义在前文（电商总体运营指标）已经描述，在此不在赘述</p>
<p>（2）流量成本累指标<br>单位访客获取成本。该指标指在流量推广中，广告活动产生的投放费用与广告活动带来的独立访客数的比值。单位访客成本最好与平均每个访客带来的收入以及这些访客带来的转化率进行关联分析。若单位访客成本上升，但访客转化率和单位访客收入不变或下降，则很可能流量推广出现问题，尤其要关注渠道推广的作弊问题。</p>
<p>（3）流量质量类指标<br>跳出率（Bounce Rate）也被称为蹦失率，为浏览单页即退出的次数/该页访问次数，跳出率只能衡量该页做为着陆页面（LandingPage）的访问。如果花钱做推广，着落页的跳出率高，很可能是因为推广渠道选择出现失误，推广渠道目标人群和和被推广网站到目标人群不够匹配，导致大部分访客来了访问一次就离开。</p>
<p>页面访问时长。页访问时长是指单个页面被访问的时间。并不是页面访问时长越长越好，要视情况而定。对于电商网站，页面访问时间要结合转化率来看，如果页面访问时间长，但转化率低，则页面体验出现问题的可能性很大。</p>
<p>人均页面浏览量。人均页面浏览量是指在统计周期内，平均每个访客所浏览的页面量。人均页面浏览量反应的是网站的粘性。</p>
<p>（4）会员类指标</p>
<p>注册会员数。指一定统计周期内的注册会员数量。<br>活跃会员数。活跃会员数，指在一定时期内有消费或登录行为的会员总数。<br>活跃会员率。即活跃会员占注册会员总数的比重。<br>会员复购率。指在统计周期内产生二次及二次以上购买的会员占购买会员的总数。<br>会员平均购买次数。指在统计周期内每个会员平均购买的次数，即订单总数/购买用户总数。会员复购率高的电商网站平均购买次数也高。<br>会员回购率。指上一期末活跃会员在下一期时间内有购买行为的会员比率。<br>会员留存率。会员在某段时间内开始访问你的网站，经过一段时间后，仍然会继续访问你的网站就被认作是留存，这部分会员占当时新增会员的比例就是新会员留存率，这种留存的计算方法是按照活跃来计算，另外一种计算留存的方法是按消费来计算，即某段的新增消费用户在往后一段时间时间周期（时间周期可以是日、周、月、季度和半年度）还继续消费的会员比率。留存率一般看新会员留存率，当然也可以看活跃会员留存。留存率反应的是电商留住会员的能力。</p>
<p>3.网站销售（转化率）类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaghR.png" alt="CYaghR.png"></p>
<p>（1）购物车类指标<br>基础类指标，包括一定统计周期内加入购物车次数、加入购物车买家数、加入购物车买家数以及加入购物车商品数。<br>转化类指标，主要是购物车支付转化率，即一定周期内加入购物车商品支付买家数与加入购物车购买家数的比值。</p>
<p>（2）下单类指标<br>基础类指标，包括一定统计周期内的下单笔数、下单金额以及下单买家数。<br>转化类指标，主要是浏览下单转化率，即下单买家数与网站访客数（UV）的比值。</p>
<p>（3）支付类指标<br>基础统计类指标，包括一定统计周期内支付金额、支付买家数和支付商品数。<br>转化类指标。包括浏览-支付买家转化率（支付买家数/网站访客数）、下单-支付金额转化率（支付金额/下单金额）、下单-支付买家数转化率（支付买家数/下单买家数）和下单-支付时长（下单时间到支付时间的差值）。</p>
<p>4.客户价值类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaW1x.png" alt="CYaW1x.png"></p>
<p>客户指标。常见客户指标包括一定统计周期内的累计购买客户数和客单价。客单价是指每一个客户平均购买商品的金额，也即是平均交易金额，即成交金额与成交用户数的比值。</p>
<p>新客户指标。常见新客户指标包括一定统计周期内的新客户数量、新客户获取成本和新客户客单价。其中，新客户客单价是指第一次在店铺中产生消费行为的客户所产生交易额与新客户数量的比值。影响新客户客单价的因素除了与推广渠道的质量有关系，还与电商店铺活动以及关联销售有关。</p>
<p>老客户指标。常见老客户指标包括消费频率、最近一次购买时间、消费金额和重复购买率。消费频率是指客户在一定期间内所购买的次数；最近一次购买时间表示客户最近一次购买的时间离现在有多远；客户消费金额指客户在最近一段时间内购买的金额。消费频率越高，最近一次购买时间离现在越近，消费金额越高的客户越有价值。重复购买率则指消费者对该品牌产品或者服务的重复购买次数，重复购买率越多，则反应出消费者对品牌的忠诚度就越高，反之则越低。重复购买率可以按两种口径来统计：第一种，从客户数角度，重复购买率指在一定周期内下单次数在两次及两次以上的人数与总下单人数之比，如在一个月内，有100个客户成交，其中有20个是购买两次及以上，则重复购买率为20%；第二种，按交易计算，即重复购买交易次数与总交易次数的比值，如某月内，一共产生了100笔交易，其中有20个人有了二次购买，这20人中的10个人又有了三次购买，则重复购买次数为30次，重复购买率为30%。</p>
<p>5.商品类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYafc6.png" alt="CYafc6.png"></p>
<p>产品总数指标。包括SKU、SPU和在线SPU。SKU是物理上不可分割的最小存货单位。SPU即Standard Product Unit （标准化产品单元），SPU是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。通俗点讲，属性值、特性相同的商品就可以称为一个SPU。如iphone5S是一个SPU，而iPhone 5S配置为16G版、4G手机、颜色为金色、网络类型为TD-LTE/TD-SCDMA/WCDMA/GSM则是一个SKU。在线SPU则是在线商品的SPU数。</p>
<p>产品优势性指标。主要是独家产品的收入占比，即独家销售的产品收入占总销售收入的比例。</p>
<p>品牌存量指标。包括品牌数和在线品牌数指标。品牌数指商品的品牌总数量。在线品牌数则指在线商品的品牌总数量。</p>
<p>上架。包括上架商品SKU数、上架商品SPU数、上架在线SPU数、上架商品数和上架在线商品数。</p>
<p>首发。包括首次上架商品数和首次上架在线商品数。</p>
<p>6.市场营销活动指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaIBD.png" alt="CYaIBD.png"></p>
<p>市场营销活动指标。包括新增访问人数、新增注册人数、总访问次数、订单数量、下单转化率以及ROI。其中，下单转化率是指活动期间，某活动所带来的下单的次数与访问该活动的次数之比。投资回报率（ROI）是指，某一活动期间，产生的交易金额与活动投放成本金额的比值。</p>
<p>广告投放指标。包括新增访问人数、新增注册人数、总访问次数、订单数量、UV订单转化率、广告投资回报率。其中，下单转化率是指某广告所带来的下单的次数与访问该活动的次数之比。投资回报率（ROI）是指，某广告产生的交易金额与广告投放成本金额的比值。</p>
<p>7.风控类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYa7AH.png" alt="CYa7AH.png"></p>
<p>买家评价指标。包括买家评价数，买家评价卖家数、买家评价上传图片数、买家评价率、买家好评率以及卖家差评率。其中，买家评价率是指某段时间参与评价的卖家与该时间段买家数量的比值，是反映用户对评价的参与度，电商网站目前都在积极引导用户评价，以作为其他买家购物时候的参考。买家好评率指某段时间内好评的买家数量与该时间段买家数量的比值。同样，买家差评率指某段时间内差评的买家数量与该时间段买家数量的比值。尤其是买家差评率，是非常值得关注的指标，需要监控起来，一旦发现买家差评率在加速上升，一定要提高警惕，分析引起差评率上升的原因，及时改进。</p>
<p>买家投诉类指标。包括发起投诉（或申诉），撤销投诉（或申诉），投诉率（买家投诉人数占买家数量的比例）等。投诉量和投诉率都需要及时监控，以发现问题，及时优化。</p>
<p>8.市场竞争类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaHNd.png" alt="CYaHNd.png"></p>
<p>市场份额相关指标，包括市场占有率、市场扩大率和用户份额。市场占有率指电商网站交易额占同期所有同类型电商网站整体交易额的比重；市场扩大率指购物网站占有率较上一个统计周期增长的百分比；用户份额指购物网站独立访问用户数占同期所有B2C购物网站合计独立访问用户数的比例。</p>
<p>网站排名，包括交易额排名和流量排名。交易额排名指电商网站交易额在所有同类电商网站中的排名；流量排名指电商网站独立访客数量在所有同类电商网站中的排名。</p>
<p>总之，本文介绍了电商数据分析的基础指标体系，涵盖了流量、销售转化率、客户价值、商品类目、营销活动、风控和市场竞争指标，这些指标都需要系统化的进行统计和监控，才能更好的发现电商运营健康度的问题，以更好及时改进和优化，提升电商收入。如销售转化率，其本质上是一个漏斗模型，如从网站首页到最终购买各个阶段的转化率的监控和分析是网站运营健康度很重要的分析方向。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-HBASE：出自Google论文的发表" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/HBASE：出自Google论文的发表/" class="article-date">
  	<time datetime="2018-04-05T13:29:30.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/HBASE：出自Google论文的发表/">
        HBASE总结：出自Google论文的发表
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HBASE：出自Google论文的发表"><a href="#HBASE：出自Google论文的发表" class="headerlink" title="HBASE：出自Google论文的发表"></a>HBASE：出自Google论文的发表</h2><h6 id="列式存储、多版本-timestamp-的、nosql数据库"><a href="#列式存储、多版本-timestamp-的、nosql数据库" class="headerlink" title="[列式存储、多版本(timestamp)的、nosql数据库]"></a>[列式存储、多版本(timestamp)的、nosql数据库]</h6><h6 id="hbase架构图：要能手画出来"><a href="#hbase架构图：要能手画出来" class="headerlink" title="hbase架构图：要能手画出来"></a>hbase架构图：要能<strong>手画</strong>出来</h6><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/ynWG7DnR777pXkx0IGWDtEit4H6y7VkXIgYmJDtxY48!/b/dPMAAAAAAAAA&amp;bo=EAUIAwAAAAARBy4!&amp;rf=viewer_4" alt=""></p>
<h6 id="HBase没有单节点故障-因为zookeeper-，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作"><a href="#HBase没有单节点故障-因为zookeeper-，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作" class="headerlink" title="**HBase没有单节点故障(因为zookeeper)，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作"></a>**HBase没有单节点故障(因为zookeeper)，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作</h6><h4 id="HBase的功能："><a href="#HBase的功能：" class="headerlink" title="HBase的功能："></a>HBase的功能：</h4><pre><code>*hadoop数据库:
    1.存储数据 
    2.检索数据
*和RDBMS[关系型数据库]相比：
    1.海量数据：数据条目数--上亿
    2.检索的速度：准时性、秒级别
*基于hdfs： hdfs的优势
    1.数据安全性[副本机制]
    2.普通商用PC server就ok
</code></pre><h5 id="1-Table中的所有行都是按照rowkey的字典序排列"><a href="#1-Table中的所有行都是按照rowkey的字典序排列" class="headerlink" title="1.Table中的所有行都是按照rowkey的字典序排列"></a>1.Table中的所有行都是按照rowkey的字典序排列</h5><h5 id="2-Table在行的方向上分割为多个Region"><a href="#2-Table在行的方向上分割为多个Region" class="headerlink" title="2.Table在行的方向上分割为多个Region"></a>2.Table在行的方向上分割为多个Region</h5><h5 id="3-Region是按照大小分割的，每个表开始只有一个region-会有一个starkey和endkey-前毕后包-，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。"><a href="#3-Region是按照大小分割的，每个表开始只有一个region-会有一个starkey和endkey-前毕后包-，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。" class="headerlink" title="3.Region是按照大小分割的，每个表开始只有一个region[会有一个starkey和endkey,前毕后包]，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。"></a>3.Region是按照大小分割的，每个表开始只有一个region[会有一个starkey和endkey,前毕后包]，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。</h5><h5 id="4-Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上"><a href="#4-Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上" class="headerlink" title="4.Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上."></a>4.Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上.</h5><h5 id="5-Region虽然是分布式存储的最小单元，但是不是存储的最小单元。"><a href="#5-Region虽然是分布式存储的最小单元，但是不是存储的最小单元。" class="headerlink" title="5.Region虽然是分布式存储的最小单元，但是不是存储的最小单元。"></a>5.Region虽然是分布式存储的最小单元，但是不是存储的最小单元。</h5><pre><code>存储的最小单元是cell，{rowkey,column,version}
        *唯一性
        *数据没有类型，以字节码形式存储

region由一个或多个store组成，每个store保存一个column family；
每个store又由一个memStore和0至多个storeFile组成；
memStore存储在内存中，storeFile存储在hdfs上
</code></pre><h6 id="hbase-旧版本的web监听端口-60010，新版本是16010"><a href="#hbase-旧版本的web监听端口-60010，新版本是16010" class="headerlink" title="hbase 旧版本的web监听端口 60010，新版本是16010"></a>hbase 旧版本的web监听端口 60010，新版本是16010</h6><h3 id="HBASE数据写入流程："><a href="#HBASE数据写入流程：" class="headerlink" title="HBASE数据写入流程："></a>HBASE数据写入流程：</h3><pre><code>put ---&gt; cell
    step0： 先进行记录 ---&gt;HLog 【WAL（write ahead logging:日志预写功能）】 loc：hdfs
            存储的log数据类型是sequenceFile
</code></pre><p>——- WAL的作用是防止写入内存时，region挂掉，数据丢失</p>
<pre><code>step1： 写入memStore     loc：内存
step2： 当内存达到一定阈值时，写入storeFile  ---&gt; loc： hdfs
</code></pre><h3 id="HBASE-用户读取数据流程："><a href="#HBASE-用户读取数据流程：" class="headerlink" title="HBASE 用户读取数据流程："></a>HBASE 用户读取数据流程：</h3><pre><code>1)先到memStore里面去读
2)然后到BlockCache里面读 --&gt; *每个RegionServer只有一个BlockCache*
3)最后到HFile里面读取数据
4)然后对数据进行Merge --&gt; 最后返回数据集
</code></pre><h4 id="MemSore和BlockCache"><a href="#MemSore和BlockCache" class="headerlink" title="MemSore和BlockCache"></a>MemSore和BlockCache</h4><pre><code>*HBase上的RegionServer的内存分为两部分：
    1.Memstore --&gt; 用来 写
        写请求会先写入Memstore，RegionServer会给每个Region提供一个Memstore，当Memstore写满64M以后，会启动flush舒心到磁盘。
    2.BlockCache --&gt; 用来读
        读请求先到Memstore中查询数据，查不到就到BlockCache中查询，再差不读奥就会到磁盘上读，并把读的结果放入BlockCache。
        BlockCache达到上限后，会启动淘汰机制，淘汰掉最老的一批数据
*在注重读取响应时间的场景下，可以将BlockCache设置大些，Memstore设置小些，以加大缓存的命中率。
</code></pre><h3 id="数据检索的三种方式："><a href="#数据检索的三种方式：" class="headerlink" title="数据检索的三种方式："></a>数据检索的三种方式：</h3><pre><code>1.get rowkey --- 最快的方式
2.scan range --- 用的最多的方式 【一般先range扫描，然后get】
3.scan ---全表扫描，基本不用
</code></pre><h4 id="HBase中有类似于RDBMS中Database的概念"><a href="#HBase中有类似于RDBMS中Database的概念" class="headerlink" title="HBase中有类似于RDBMS中Database的概念"></a>HBase中有类似于RDBMS中Database的概念</h4><pre><code>命名空间：
    *用户自定义的表的命名空间，默认情况下 ---&gt;  default
    *系统自带的元数据的表的命名空间 ---&gt; hbase
</code></pre><h3 id="HBase数据迁移"><a href="#HBase数据迁移" class="headerlink" title="HBase数据迁移"></a>HBase数据迁移</h3><pre><code>使用 MapReduce把文件生成hdfs上的HFile，然后进行bulk load into hbase table
</code></pre><h1 id="HBase表RowKey的设计-："><a href="#HBase表RowKey的设计-：" class="headerlink" title="**HBase表RowKey的设计**："></a><strong>**</strong>HBase表RowKey的设计<strong>**</strong>：</h1><h5 id="现实环境中出现的问题："><a href="#现实环境中出现的问题：" class="headerlink" title="**现实环境中出现的问题："></a>**现实环境中出现的问题：</h5><pre><code>*默认情况下创建一张HBase表，自动会为表创建一个Region [startkey,endkey) 前毕后包
    *无论在测试环境还是生产环境中，创建完表以后会往表中导入大量数据
        步骤：
            file/datas -&gt; HFile -&gt; bulk load into hbase table
        此时Region只有一个，而region是被RegionServer管理的，
        当导入数据量慢慢增大后Region就会被split成两个Region，
        但是此时RegionServer很可能就会挂掉，此时就会出现问题...

解决方案[举例]：
    创建表时，多创建一些Region(依据表的rowkey进行设计 + 结合业务)
        比如说：有5个region，他们被多个RegionServer管理
            再插入数据时，会向5个Region中分别插入数据，这样就均衡了
</code></pre><h5 id="具体解决方案："><a href="#具体解决方案：" class="headerlink" title="**具体解决方案："></a>**具体解决方案：</h5><p>表的RowKey设计中：</p>
<pre><code>**如何在海量数据中，查询到我想要的数据???
    核心思想：
        1.依据RowKey查询最快
        2.对RowKey进行范围查询range
        3.前缀查询  
            比如：startkey:15221467820_20180409000053 和 endkey:15221467828_20180419000053
            这时只会比较15221467820 和 15221467828 而后面的_2018.. 则不会进行匹配    
</code></pre><p>解决方法：</p>
<pre><code>a).HBase的预分区：
    Region的划分依赖于RowKey，预先预估一些RowKey(年月日时分秒)[最常用的就是这两种，其他的方法rowkey设计都不能自定义]
        1. create &apos;tb1&apos;,&apos;cf1&apos;,splits =&gt; [&apos;20180409000000&apos;,&apos;20180419000000&apos;,&apos;20180429000000&apos;]
        2. create &apos;tb1&apos;,&apos;cf1&apos;,splits_file =&gt; &apos;/root/data/logs-split.txt&apos;
           [这种情况就是把分段的rowkey写入了文件里]
    这样就分为了4个Region，因为它会把头和尾也算进去
b).根据业务来对RowKey的设计：
        举例：移动运营商电话呼叫查询详单：
            RowKey: 
                phone + time 
                15221467820_20180409010000
            infor: 
                上海  主叫   152216888888   30        国内通话 0.00
                area active opposite_phone talk_time model price 
        依据查询条件：
            phone + (start_time - end_time)
        代码：
            scan 
                startrow
                    15221467820_20180409000000
                stoprow
                    15221467820_20180419000000
    保证了实时性
</code></pre><h3 id="HBase数据存储二级索引-索引表-的设计以及数据同步解决方案："><a href="#HBase数据存储二级索引-索引表-的设计以及数据同步解决方案：" class="headerlink" title="HBase数据存储二级索引[索引表]的设计以及数据同步解决方案："></a>HBase数据存储二级索引[索引表]的设计以及数据同步解决方案：</h3><p>基于以上的移动运营商电话呼叫查询详单：</p>
<pre><code>二级索引表：
    RowKey:
        opposite_phone
    columnFamily对应的colume:
        主表的RowKey
主表和索引表的同步：
    &gt;&gt; phoenix 
            &gt;&gt; jdbc 方式才能同步
    &gt;&gt;先在solr里创建索引表
     solr   //在solr中创建索引表
            国外有个框架交 lily
                cloudera search 封装了lily，只要配置下cloudera
                cloudera search会在主表插入数据时，自动更新solr里的索引表
</code></pre><h2 id="HBase之-表的属性"><a href="#HBase之-表的属性" class="headerlink" title="HBase之 表的属性:"></a>HBase之 表的属性:</h2><h3 id="面试会问到：-每个RegionServer只有一个HLog和一个BlockCache"><a href="#面试会问到：-每个RegionServer只有一个HLog和一个BlockCache" class="headerlink" title="面试会问到：**每个RegionServer只有一个HLog和一个BlockCache"></a>面试会问到：**每个RegionServer只有一个HLog和一个BlockCache</h3><pre><code>1.HBase的表的压缩[HFile compression]--表的属性:
**在企业中通常使用snappy格式进行压缩，可以减少容量的存储，和减少io流，提高传输效率
    跟hive一样，需要一些编译jar包，然后放到lib的native里
    [这里使用软连接 ln -s /xx/xx /xx  具体看官方文档]，还有一些参数的配置，才能使用
        创建表的时候：
            create &apos;user&apos;,{NAME=&apos;cf1&apos;,COMPRESSION=&gt;&apos;SNAPPY&apos;}
2. HBase 标的属性：IN_MEMORY=&gt;&apos;false&apos;、BLOCKCACHE=&gt;&apos;true&apos;
        HBase为什么查询这么快 就是因为这些表的属性的存储机制！！
        通常我们自定义创建的表的相应列簇下的IN_MEMORY属性都是false!
        通过describe &apos;hbase:meta&apos;可以查询到 IN_MEMORY=&gt;&apos;true&apos; BLOCKCACHE =&gt; &apos;true&apos;,
        意为meta元数据存放于blockcache的inmemory内存中
            IN_MEMORY队列存放HBase的meta表元数据信息，因为meta存储了用户表的
            通常 BLOCKCACHE 这个属性的设置要慎用，比如我们查询后的数据后续还会用到这时可以设置为true
            如果后续用不到这些数据就要设置此属性为false
</code></pre><h4 id="HBase的Cache分等级："><a href="#HBase的Cache分等级：" class="headerlink" title="HBase的Cache分等级："></a>HBase的Cache分等级：</h4><pre><code>默认情况下：整个BlockCache的内存分配 
1.IN_MEMORY 1/4    in_memory 用于存储hbase的meta元数据信息    2.Single 1/4   single就是用的次数较少的数据
3.Muti    1/2          muti就是用的次数较多的数据
如果BlockCache的内存到达阈值，会先清理single然后清理muti，in_memory的内存不会被清理!
</code></pre><h3 id="HBase表的compaction"><a href="#HBase表的compaction" class="headerlink" title="HBase表的compaction"></a>HBase表的compaction</h3><h6 id="随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种："><a href="#随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种：" class="headerlink" title="随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种："></a>随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种：</h6><pre><code>1.minor compaction
    Minor Compaction是指选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，
    在这个过程中不会处理已经Deleted或Expired的Cell。一次Minor Compaction的结果是更少并且更大的StoreFile。
2.major compaction
    Major Compaction是指将所有的StoreFile合并成一个StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL过期数据、版本号超过设定版本号的数据。另外，一般情况下，Major Compaction时间会持续比较长，会阻塞数据的吸入和查询!!!，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此线上业务都会将关闭自动触发Major Compaction功能，改为手动在业务低峰期触发。
</code></pre><h4 id="HBase和Hive集成："><a href="#HBase和Hive集成：" class="headerlink" title="HBase和Hive集成："></a>HBase和Hive集成：</h4><pre><code>此时实质上Hive就是HBase的一个客户端!
应用场景：
    日志文件 --&gt; hive --&gt; hive-hbase-table --&gt; insert .. select ...
</code></pre><h4 id="电商订单查询之HBase："><a href="#电商订单查询之HBase：" class="headerlink" title="电商订单查询之HBase："></a>电商订单查询之HBase：</h4><pre><code>1.用户下单后--&gt;存到RDBMS[关系型数据库]中 即未完成的订单
2.订单完成后--&gt;数据迁移到HBase中

查询比如三个月内的订单：
 1.主表[订单显示表]：
    rowkey: userid_orderCreateTime_orderId  
    cfname: orderInfor
    [本来userid+orderCreateTime就可以保证唯一性，加orderId的原因是为了二次查询]
 2.订单详情表:
     rowkey: orderId_orderItemId
    cfname: itemInfor
 3.索引表：
     比如用户输入订单号：我们后台Redis或者MongoDB里会有相对应的订单编码
     获取到订单编码然后到索引表中去查
         索引表的rowkey：订单编号 column: 主表的rowkey
         就可以到hbase主表中查询订单
</code></pre><h5 id="补充文件类型-tsv-tab-csv-comma-："><a href="#补充文件类型-tsv-tab-csv-comma-：" class="headerlink" title="补充文件类型 .tsv[tab] .csv[comma]："></a>补充文件类型 .tsv[tab] .csv[comma]：</h5><pre><code>user.tsv tsv后缀格式的文件：每行数据以tab 制表符分割的 
user.csv csv后缀格式的文件：每行数据以逗号分割的
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-DMP" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/DMP/" class="article-date">
  	<time datetime="2018-04-05T13:29:28.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/DMP/">
        dmp介绍
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>dmp</p>
<p>（数据管理平台）<br> 编辑<br>DMP(Data Management Platform)数据管理平台，是把分散的多方数据进行整合纳入统一的技术平台，并对这些数据进行标准化和细分，让用户可以把这些细分结果推向现有的互动营销环境里的平台。<br>中文名<br>数据管理平台<br>外文名<br>Data Management Platform<br>释    义<br>数据进行整合纳入统一的技术平台<br>简    称<br>DMP<br>目录<br>.    1 作用<br>.    2 类型<br>作用<br>编辑<br>•能快速查询、反馈和快速呈现结果<br>•能帮助客户更快进入到市场周期中<br>•能促成企业用户和合作伙伴之间的合作<br>•能深入的预测分析并作出反应<br>•能带来各方面的竞争优势<br>•能降低信息获取及人力成本<br>类型<br>编辑<br>1、结构化的数据，比如Oracle数据库数据等；<br>　　2、非结构化的数据，比如各种文件、图像、音频等数据，等等。<br>　　结构化数据（即数据库数据）在当今的信息系统中占据最核心、最重要的位置。结构化数据从产生―使用―消亡这样一个完整过程的管理，就是数据生命周期管理（所谓的ILM）。<br>核心元素包括：<br>•数据整合及标准化能力：采用统一化的方式，将各方数据吸纳整合。<br>•数据细分管理能力：创建出独一无二、有意义的客户细分，进行有效营销活动。<br>•功能健全的数据标签：提供数据标签灵活性，便于营销活动的使用。<br>•自助式的用户界面：基于网页web界面或其他集成方案直接获取数据工具，功能和几种形式报表和分析。<br>•相关渠道环境的连接：跟相关渠道的集成，包含网站端、展示广告、电子邮件以及搜索和视频，让营销者能找到、定位和提供细分群体相关高度的营销信息。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2018 rongyuewu
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>