<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">rongyuewu</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="https://github.com/smackgg/hexo-theme-smackdown">smackdown</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">rongyuewu</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">rongyuewu</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-storm的架构" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/storm的架构/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/storm的架构/">
        storm
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="storm"><a href="#storm" class="headerlink" title="storm"></a>storm</h1><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/t7Rbr8aYgIO4*9TbdsjJMgxTPY8JHwAmijtuJfP2rbo!/b/dEMBAAAAAAAA&amp;bo=wAY4BAAAAAADN.g!&amp;rf=viewer_4" alt=""></p>
<h3 id="Storm架构"><a href="#Storm架构" class="headerlink" title="Storm架构"></a>Storm架构</h3><pre><code>类似于Hadoop的架构，主从(Master/Slave)
Nimbus: 主
    集群的主节点，负责任务(task)的指派和分发、资源的分配
Supervisor: 从
    可以启动多个Worker，具体几个呢？可以通过配置来指定
    一个Topo可以运行在多个Worker之上，也可以通过配置来指定
    集群的从节点，(负责干活的)，负责执行任务的具体部分
    启动和停止自己管理的Worker进程
无状态，在他们上面的信息(元数据)会存储在ZK中
Worker: 运行具体组件逻辑(Spout/Bolt)的进程
=====================分割线===================
task： 
    Spout和Bolt
    Worker中每一个Spout和Bolt的线程称为一个Task
executor： spout和bolt可能会共享一个线程
</code></pre><h5 id="提交代码到集群："><a href="#提交代码到集群：" class="headerlink" title="提交代码到集群："></a>提交代码到集群：</h5><pre><code>storm jar /home/hadoop/lib/storm-1.0.jar com.bigdata.ClusterSumStormTopology
</code></pre><h5 id="storm-其他命令的使用"><a href="#storm-其他命令的使用" class="headerlink" title="storm 其他命令的使用"></a>storm 其他命令的使用</h5><pre><code>list
    Syntax: storm list
    List the running topologies and their statuses.
如何停止作业
    kill
        Syntax: storm kill topology-name [-w wait-time-secs]
</code></pre><p>并行度</p>
<pre><code>一个worker进程执行的是一个topo的子集
一个worker进程会启动1..n个executor线程来执行一个topo的component
一个运行的topo就是由集群中多台物理机上的多个worker进程组成

executor是一个被worker进程启动的单独线程，每个executor只会运行1个topo的一个component
task是最终运行spout或者bolt代码的最小执行单元

默认：
    一个supervisor节点最多启动4个worker进程  
    每一个topo默认占用一个worker进程         
    每个worker进程会启动一个executor        
    每个executor启动一个task   

Total slots:4  
Executors: 3   但是stormUI上是 spout + bolt = 2  why 3?
隐藏的acker 导致的
</code></pre><h5 id="通过代码对并行度的理解："><a href="#通过代码对并行度的理解：" class="headerlink" title="通过代码对并行度的理解："></a>通过代码对并行度的理解：</h5><pre><code>Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes
topologyBuilder.setSpout(&quot;blue-spout&quot;, new BlueSpout(), 2); // set parallelism hint to 2
topologyBuilder.setBolt(&quot;green-bolt&quot;, new GreenBolt(), 2)
           .setNumTasks(4)
           .shuffleGrouping(&quot;blue-spout&quot;);topologyBuilder.setBolt(&quot;yellow-bolt&quot;, new YellowBolt(), 6)
           .shuffleGrouping(&quot;green-bolt&quot;);StormSubmitter.submitTopology(
    &quot;mytopology&quot;,
    conf,
    topologyBuilder.createTopology()
);

##解释：
conf.setNumWorkers(2) ---&gt;两个worker
setSpout(&quot;blue-spout&quot;, new BlueSpout(), 2)---&gt; 2个spout executor 对应默认的2个task
setBolt(&quot;green-bolt&quot;, new GreenBolt(), 2).setNumTasks(4) ---&gt; 2个executor 4个task(因为它指定了setNumTasks个数)

结果：
1个topology
2个workers  
2+2+2[2个worker 就有2个acker] = 6个executors  
2+4+2[2个worker 就有2个acker默认2个task] = 8个task
</code></pre><h5 id="修改正在运行的topology的并行度："><a href="#修改正在运行的topology的并行度：" class="headerlink" title="修改正在运行的topology的并行度："></a>修改正在运行的topology的并行度：</h5><pre><code>#Reconfigure the topology &quot;mytopology&quot; to use 5 worker processes,
#the spout &quot;blue-spout&quot; to use 3 executors 
#the bolt &quot;yellow-bolt&quot; to use 10 executors.

1.可以使用StormUI来rebalance the topology.
2.也可以使用命令行来修改：$ storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10
</code></pre><p>stream grouping分组策略内置[build-in]有8种：</p>
<pre><code>常用的有两种：
1.shuffle grouping 随机分配也就是轮询RoundRobin 这样不会造成数据倾斜
2.fileds grouping 比如按照字段hash取模分组 
如果想自定义分组策略：--&gt; 自定义分组策略需要实现CustomStreamGroup接口
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-ID-Mapping" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/ID-Mapping/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/ID-Mapping/">
        解密大数据ID-Mapping
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>集奥聚合带你解密大数据ID-Mapping<br>来源：数据观 时间：2016-06-27 14:46:24 作者：集奥聚合<br>　　谈到大数据，有一个非常基本但又关键的环节就是ID-Mapping（Identifier -Mapping）。ID-Mapping通俗的说就是把几份不同来源的数据，通过各种技术手段识别为同一个对象或主体，例如同一台设备（直接），同一个用户（间接），同一家企业（间接）等等，可以形象地理解为用户画像的“拼图”过程。一个用户的行为信息、属性数据是分散在很多不同的数据来源的，因此从单个数据来看，都相当于“盲人摸象”，看到的只是这个用户一个片面的画像，而ID-Mapping能把碎片化的数据全部串联起来，消除数据孤岛，提供一个用户的完整信息视图，同时让某一个领域的数据在另一个领域绽放出巨大的价值。<br>　　ID-Mapping有非常多的用处，比如跨屏跟踪和跨设备跟踪，将一个用户的手机、PC、平板等设备的上的行为信息串联到一起。再比如这两年非常热的程序化交易，它的一个重要环节就是要把当前广告请求的用户和第一方DMP平台里的用户历史兴趣数据匹配起来。可以说，没有ID-Mapping，程序化交易就变成了盲目投放，它的实时竞价，精准投放的优势也就不存在了。<br>　　ID-Mapping既然有这么大的作用，那么应该如何做好ID-Mapping呢？这个环节不是一个简单的按照Key匹配的过程，集奥聚合作为领先的第三方大数据公司，研发了多项ID-Mapping 的独家技术，用新的匹配技术和算法模型来重塑了ID-Mapping过程。据粗略评估，集奥聚合ID-Mapping系统有能力把十几个数据源的 56亿ID（Identifier，即标识符）匹配到一起，准确率达到 95%以上，有效用户总量提升了30%，平均每个用户的标签量提升200%以上。值得注意的是，这里的Identifier是指标识符，而并非Identity（身份信息），集奥聚合可在完全脱敏，不（也无需）识别、指出用户姓甚名谁的身份信息的情况下合法地将标识符对应至某匿名用户。<br>　　简单来说，集奥聚合ID-Mapping体系有三个层面。<br>　　第一个层面是物理Mapping<br>　　这是最单纯基本的层面，也就是如何精准地记录和标识一个用户，例如利用硬件设备码生成一个统一的设备码，利用一些强账号来标识用户等等。这个层面上主要的技术难度在于ID的稳定性、唯一性和持久性。<br>　　第二个层面是基于用户行为做迭代滚动Mapping<br>　　由于原始数据存在噪音，同一个用户的多份数据、多种ID之间是“多对多”的关系。那么哪些ID是可信的呢？<br>　　我们设计了一个置信度传播的机器学习图模型来帮助确定哪些身份ID是可信的。</p>
<p>　　算法示意图如上，每个节点是一个UID或QQ号或GID等标识的潜在的“用户”<br>　　 一开始节点之间关系的概率是随机的<br>　　 其中总有两个ID的关系是强置信的prior<br>　　 迭代收敛后，哪些ID是归属于同一个用户的标识符被识别出来<br>　　大体来说，这个算法的过程是给每一个ID，以及两个ID，如IMEI和邮箱之间的pair关系都有一个预设的置信度。而所有的ID根据两两关联构成了一张图，那么每个ID的置信度根据这张网的结构传播给相关联的ID，同时也从其他ID那边接收置信度，而pair关系的置信度不变。当算法迭代收敛时，高置信度的ID就是可信的。同一个子图内的ID就标识了同一个用户。用类似的算法，我们也可以评价每个数据源的质量等。<br>　　第三个层面是基于用户兴趣做相似用户的合并<br>　　如果说层面二主要还在判断标识一个用户的ID是否正确，那么层面三致力于把行为相似的用户给合并起来。<br>　　例如，某一个用户的设备多次连接同一个Wi-Fi网关，但是每次链接都会随机更换ID，那么相当于这个用户的数据“分裂”在多个不同ID下。那么如何把这些ID合并成同一个用户呢？<br>　　除了上述做法之外，集奥聚合开发了相似用户合并技术。基于用户的上网时间偏好、网址访问偏好、点击行为偏好、浏览行为偏好、APP偏好和社交账号偏好等，为每个用户提取了上千个特征之后，进行相似用户的聚类。<br>　　聚类中选择类中心附近的用户，再加上一些辅助准则判定，就可以把用户合并起来。<br>　　经实际测试，可以把用户ID总量减少80%，同时保持用户合并的准确率在91%以上。使用的历史数据时间窗越长就越精准。仅此一项就能让用户的标签密度提升 500%。最早出现于安卓<br>深入浅出理解 Cookie Mapping<br>Posted on 2014 年 11 月 9 日 by Abbo<br>在RTB（实时竞价广告，Real-Time-Bidding）广告领域（当然实际上不仅仅是这个领域），有一个常见的词汇叫 Cookie Mapping（Cookie 匹配），一会又是DSP（需求方供应平台）与DSP的Cookie Mapping，一会又是DSP与Ad Exchange的Cookie Mapping，一会还有DMP（数据管理平台）与DSP的Cookie Mapping，已经完全把大家搞浑了。许多互联网广告从业者都不清楚到底什么是 Cookie Mapping，到底又是为什么要 Cookie Mapping。今天就以小小的笔记，分享大家疑问的解答。<br>用户唯一标识体系<br>在互联网中，我们有着许多标识唯一用户的技术手段，其中，最为常见的就是 Cookie 了（什么是Cookie请参看网站分析中的Cookie）。简单的多，Cookie具备几个特征：<br>•    唯一性，一个Cookie是唯一存在于一个域名下的；<br>•    归属权，一个Cookie必须属于某一个域名，且相互不能访问使用；<br>•    持久性，一个Cookie可以持久的存在于一个浏览器中。<br>正因为Cookie具备上述几个特征，也就衍生出Cookie在使用上的一些特点了，我们以DSP.COM（广告购买平台），ADX.COM（广告交易平台），DMP.COM（数据管理平台）为例，存在以下结论：<br>•    DSP.COM，ADX.COM，DMP.COM都存在各自的用户标识体系（各自定义的唯一ID标识）；<br>•    用户Abbo在上述三个产品的ID分别是dsp-1，adx-a，dmp-①，且相互不能访问使用。<br>就这样，DSP.COM，ADX.COM，DMP.COM都可以唯一的标识出用户Abbo，但他们并不能互相读取标识信息。<br>共享用户特征<br>由于客户需求，广告主在DSP.COM，ADX.COM，DMP.COM均有业务存在：<br>•    广告主使用DSP.COM进行广告投放，并且用户Abbo点击了游戏广告；<br>•    用户Abbo主动使用了DMP.COM提供的浏览器购物比价插件服务；<br>•    用户Abbo点击过位于交易平台ADX.COM上的职业学习、求职类广告；<br>刚好，DSP.COM识别出了Abbo喜欢玩游戏特征，DMP.COM识别出了Abbo是男性用户，ADX.COM识别出了Abbo是个年轻人。此时问题来了，由于三方的数据并不共享，因此对于广告主而言，仅知道dsp-1喜欢玩游戏，adx-a是年轻人，dmp-①是男性用户。广告主并不能直接知道Abbo是个喜爱玩游戏的年轻男性。<br>最终目标，我们需要不同产品体系中的用户的特征，合并绑定到一个用户上来，这也就是本文主题的关键——Cookie Mapping。<br>常见 Mapping 方式<br>我们刚刚看到，不同厂商、产品对用户都使用了不同的标识体系，诸如dsp-a，adx-a，dmp-①此类。因此，我们在Cookie Mapping中的最为基础的信息表——ID映射关系，俗称Cookie Mapping表。它负责使dsp-1，adx-a，dmp-①关联起来。<br>要使同一个用户在不同体系中关联起来，只有一个做法，那就是当用户发生行为的时候，同时能够联通多家厂商、产品。也就是出现了以下最常见的几种Mapping方式生成ID映射关系表：<br>•    用户加载网页代码时候，同时加载DSP.COM，ADX.COM，DMP.COM的代码，互相调用Mapping接口传输ID信息；（客户端Mapping）<br>•    用户加载网页代码时候，由服务端转发携带ID的请求，由ADX.COM服务器告诉DSP.COM相关ID信息。（服务端Mapping）<br>这样一来，经过大量的Mapping匹配后，不同厂商、产品之间也就自然形成了一套对应ID映射关系表格了。<br>移动端的 Mapping<br>移动终端的发展趋势，Cookie的效果已经远不如PC端了——PC端的用户上网行为，往往发生在一两款Web浏览软件（浏览器）中，而移动端App较为分散，用户行为、特征体现在更多的应用程序（App）上。况且，移动终端的唯一性，存在着更多的ID体系标识唯一用户，诸如MAC地址、iOS IDFA、Android ID等等。这些ID往往是具备一定唯一性，并且能够在不同App中共享的标识信息。因此，移动终端有时候也不需要 Mapping，如果约定俗成的使用某一类ID也是可以进行唯一用户标识的。<br>斗胆小结<br>斗胆小结本文，观点并不一定全部正确，如有不足，还请点出：<br>•    唯一标识需求将长期存在；<br>•    Cookie标识在PC端短期内（10年）不会消失；<br>•    多终端的发展，将出现更多标识体系；<br>•    Mapping ID的需求将长期存在。<br>Open-ID是一个很好的想法，也是一个很好的应用，特别是第三方开源Open-ID产品，个人觉得还是值得一</p>
<p>一点做用户画像的人生经验（一）：ID强打通</p>
<ol>
<li>背景<br>在构建精准用户画像时，面临着这样一个问题：日志采集不能成功地收集用户的所有ID，且每条业务线有各自定义的UID用来标识用户，从而造成了用户ID的零碎化。因此，为了做用户标签的整合，用户ID之间的强打通（亦称为ID-Mapping）成了迫切的需求。大概三年前，在知乎上有这样一个与之相类似的问题：如何用MR实现并查集以对海量数据pair做聚合；目前为止还无人解答。本文将提供一个可能的解决方案——如何用MR计算框架来实现大数据下的ID强打通。<br>首先，简要地介绍下Android设备常见的ID：<br>•    IMEI（International Mobile Equipment Identity），即通常所说的手机序列号、手机“串号”，用于在移动电话网络中识别每一部独立的手机等行动通讯装置；序列号共有15位数字，前6位（TAC）是型号核准号码，代表手机类型。接着2位（FAC）是最后装配号，代表产地。后6位（SNR）是串号，代表生产顺序号。最后1位（SP）一般为0，是检验码，备用。<br>•    MAC(Media Access Control)一般代指MAC位址，为网卡的标识，用来定义网络设备的位置。<br>•    IMSI（International Mobile SubscriberIdentification Number），储存在SIM卡中，可用于区别移动用户的有效信息；其总长度不超过15位，同样使用0～9的数字。其中MCC是移动用户所属国家代号，占3位数字，中国的MCC规定为460；MNC是移动网号码，最多由两位数字组成，用于识别移动用户所归属的移动通信网;MSIN是移动用户识别码，用以识别某一移动通信网中的移动用户。<br>•    Android ID是系统随机生成的设备ID 为一串64位的编码（十六进制的字符串），通过它可以知道设备的寿命（在设备恢复出厂设置或刷机后，该值可能会改变）。</li>
<li>设计<br>从图论的角度出发，ID强打通更像是将小连通图合并成一个大连通图；比如，在日志中出现如下三条记录，分别表示三个ID集合（小连通图）：<br>A   B   C<pre><code>C   D
    D   E
</code></pre>通过将三个小连通图合并，便可得到一个大连通图——完整的ID集合列表A B C D E。淘宝明风介绍了如何用Spark GraphX通过outerJoinVertices等运算符来做大数据下的多图合并；针对ID强打通的场景，也可采用类似的思路：日志数据构建大的稀疏图，然后采用自join的方式做打通。但是，我并没有选用GraphX，理由如下：<br>•    GraphX只支持有向图，而不支持无向图，而ID之间的关联关系是一个无向连通图；<br>•    GraphX的join操作不完全可控，“不完全可控”是指在做图合并时我们需要做过滤山寨设备、一对多的ID等操作，而在GraphX封装好的join算子上实现过滤操作则成本过高。<br>因而，基于MR计算模型（Spark框架）我设计新的ID打通算法；算法流程如下：打通的map阶段将ID集合id_set中每一个Id做key然后进行打散（id_set.map(id -&gt; id_set))），Reduce阶段按key做id_set的合并。通过观察发现：仅需要两步MR便可完成上述打通的操作。以上面的例子做说明，第一步MR完成后，打通ID集合为：A B C D、 C D E，第二步MR完成后便得到完整的ID集合列表A B C D E。但是，在两步MR过程中，所有的key都会对应一个聚合结果，而其中一些聚合结果只是中间结果。故而引入了key_set用于保存聚合时的key值，加入了第三步MR，通过比较key_set与id_set来对中间聚合结果进行过滤。算法的伪代码如下：<br>MR step1:<br> Map: <pre><code>input: id_set
process: flatMap id_set;
output: id -&gt; (id_set, 1)
</code></pre> Rduce:<pre><code>process: reduceByKey
output: id -&gt; (id_set, empty key_set, int_value)
</code></pre></li>
</ol>
<p>MR step2:<br>    Map:<br>        input: id -&gt; (id_set, empty key_set, int_value)<br>        process: flatMap id_set, if have id_aggregation, then add key to key_set<br>        output: id -&gt; (id_set, key_set, int_value)<br>    Reduce:<br>        process: reduceByKey<br>        output: id -&gt; (id_set, key_set, int_value)</p>
<p>MR step3:<br>    Map:<br>        input: id -&gt; (id_set, empty key_set, int_value)<br>        process: flatMap id_set, if have id_aggregation, then add key to key_set<br>        output: id -&gt; (id_set, key_set, int_value)<br>    Reduce:<br>        process: reduceByKey<br>        output: id -&gt; (id_set, key_set, int_value)</p>
<p>Filters:<br>    process: if have id_aggregation, then add key to key_set<br>    filter: if no id_aggregation or key_set == id_set<br>    distinct</p>
<ol start="3">
<li>实现<br>针对上述ID强打通算法，Spark实现代码如下：<br>case class DvcId(id: String, value: String)<br>val log: RDD[mutable.Set[DvcId]]// MR1val rdd1: RDD[(DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))] = log<br>.flatMap { set =&gt;<br> set.map(t =&gt; (t, (set, 1)))<br>}.reduceByKey { (t1, t2) =&gt;<br> t1._1 ++= t2._1<br> val added = t1._2 + t2._2<br> (t1._1, added)<br>}.map { t =&gt;<br> (t._1, (t._2._1, mutable.Set.empty[DvcId], t._2._2))<br>}// MR2val rdd2: RDD[(DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))] = rdd1<br>.flatMap(flatIdSet).reduceByKey(tuple3Add)// MR3val rdd3: RDD[(DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))] = rdd2<br>.flatMap(flatIdSet).reduceByKey(tuple3Add)// filterval rdd4 = rdd3.filter { t =&gt;<br>t._2._2 += t._1<br>t._2._3 == 1 || (t._2._1 – t._2.<em>2).isEmpty<br>}.map(</em>._2._1).distinct()<br>// flat id_setdef flatIdSet(row: (DvcId, (mutable.Set[DvcId], mutable.Set[DvcId], Int))) = {<br>row._2._3 match {<br> case 1 =&gt;<br>   Array((row._1, (row._2._1, row._2._2, row._2._3)))<br> case _ =&gt;<br>   row._2._2 += row._1 // add key to keySet<br>   row._2._1.map(d =&gt; (d, (row._2._1, row._2._2, row._2._3))).toArray<br>}<br>}<br>def tuple3Add(t1: (mutable.Set[DvcId], mutable.Set[DvcId], Int),<pre><code>t2: (mutable.Set[DvcId], mutable.Set[DvcId], Int)) = {
</code></pre>t1._1 ++= t2._1<br>t1._2 ++= t2._2<br>val added = t1._3 + t2._3<br>(t1._1, t1._2, added)<br>}<br>其中，引入常量1是为了标记该条记录是否发生了ID聚合的情况。<br>ID强打通算法实现起来比较简单，但是在实际的应用时，日志数据往往是带噪声的：<br>•    有山寨设备；<br>•    ID之间存在着一对多的情况，比如，各业务线的UID的靠谱程度不一，有的UID会对应到多个设备。<br>另外，ID强打通后是HDFS的离线数据，为了提供线上服务、保证ID之间的一一对应关系，应选择何种分布式数据库、表应如何设计、如何做到数据更新时而不影响线上服务等等，则是另一个需要思考的问题。</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-电商分析指标" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/电商分析指标/" class="article-date">
  	<time datetime="2018-04-05T13:29:30.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/电商分析指标/">
        电商常见分析指标
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>电商常见分析指标<br>信息流、物流和资金流三大平台是电子商务的三个最为重要的平台。而电子商务信息系统最核心的能力是大数据能力，包括大数据处理、数据分析和数据挖掘能力。无论是电商平台（如淘宝）还是在电商平台上销售产品的卖家，都需要掌握大数据分析的能力。越成熟的电商平台，越需要以通过大数据能力驱动电子商务运营的精细化，更好的提升运营效果，提升业绩。构建系统的电子商务数据分析指标体系是数据电商精细化运营的重要前提，本文将重点介绍电商数据分析指标体系。<br>电商数据分析指标体系分为八大类指标，包括总体运营指标、网站流量累指标、销售转化指标、客户价值指标、商品及供应链指标、营销活动指标、风险控制指标和市场竞争指标。不同类别指标对应电商运营的不同环节，如网站流量指标对应的是网站运营环节，销售转化、客户价值和营销活动指标对应的是电商销售环节。<br>1.电商总体运营指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaO3t.png" alt="CYaO3t.png"></p>
<p>电商总体运营整体指标主要面向的人群电商运营的高层，通过总体运营指标评估电商运营的整体效果。电商总体运营整体指标包括四方面的指标：</p>
<p>（1）流量类指标</p>
<p>独立访客数（UV），指访问电商网站的不重复用户数。对于PC网站，统计系统会在每个访问网站的用户浏览器上“种”一个cookie来标记这个用户，这样每当被标记cookie的用户访问网站时，统计系统都会识别到此用户。在一定统计周期内如（一天）统计系统会利用消重技术，对同一cookie在一天内多次访问网站的用户仅记录为一个用户。而在移动终端区分独立用户的方式则是按独立设备计算独立用户。</p>
<p>页面访问数（PV），即页面浏览量，用户每一次对电商网站或着移动电商应用中的每个网页访问均被记录一次，用户对同一页面的多次访问，访问量累计。<br>人均页面访问数，即页面访问数（PV）／独立访客数，该指标反映的是网站访问粘性。</p>
<p>（2）订单产生效率指标</p>
<p>总订单数量，即访客完成网上下单的订单数之和。<br>访问到下单的转化率，即电商网站下单的次数与访问该网站的次数之比。</p>
<p>（3）总体销售业绩指标</p>
<p>网站成交额（GMV），电商成交金额，即只要网民下单，生成订单号，便可以计算在GMV里面。<br>销售金额。销售金额是货品出售的金额总额。</p>
<p>注：无论这个订单最终是否成交，有些订单下单未付款或取消，都算GMV，销售金额一般只指实际成交金额，所以，GMV的数字一般比销售金额大。</p>
<p>客单价，即订单金额与订单数量的比值。</p>
<p>（4）整体指标</p>
<p>销售毛利，是销售收入与成本的差值。销售毛利中只扣除了商品原始成本，不扣除没有计入成本的期间费用（管理费用、财务费用、营业费用）。<br>毛利率，是衡量电商企业盈利能力的指标，是销售毛利与销售收入的比值。如京东的2014年毛利率连续四个季度稳步上升，从第一季度的10.0％上升至第四季度的12.7％，体现出京东盈利能力的提升。</p>
<p>2.网站流量指标<br><img src="https://s1.ax1x.com/2018/05/03/CYa6AJ.png" alt="CYa6AJ.png"></p>
<p>（1）流量规模类指标<br>常用的流量规模类指标包括独立访客数和页面访问数，相应的指标定义在前文（电商总体运营指标）已经描述，在此不在赘述</p>
<p>（2）流量成本累指标<br>单位访客获取成本。该指标指在流量推广中，广告活动产生的投放费用与广告活动带来的独立访客数的比值。单位访客成本最好与平均每个访客带来的收入以及这些访客带来的转化率进行关联分析。若单位访客成本上升，但访客转化率和单位访客收入不变或下降，则很可能流量推广出现问题，尤其要关注渠道推广的作弊问题。</p>
<p>（3）流量质量类指标<br>跳出率（Bounce Rate）也被称为蹦失率，为浏览单页即退出的次数/该页访问次数，跳出率只能衡量该页做为着陆页面（LandingPage）的访问。如果花钱做推广，着落页的跳出率高，很可能是因为推广渠道选择出现失误，推广渠道目标人群和和被推广网站到目标人群不够匹配，导致大部分访客来了访问一次就离开。</p>
<p>页面访问时长。页访问时长是指单个页面被访问的时间。并不是页面访问时长越长越好，要视情况而定。对于电商网站，页面访问时间要结合转化率来看，如果页面访问时间长，但转化率低，则页面体验出现问题的可能性很大。</p>
<p>人均页面浏览量。人均页面浏览量是指在统计周期内，平均每个访客所浏览的页面量。人均页面浏览量反应的是网站的粘性。</p>
<p>（4）会员类指标</p>
<p>注册会员数。指一定统计周期内的注册会员数量。<br>活跃会员数。活跃会员数，指在一定时期内有消费或登录行为的会员总数。<br>活跃会员率。即活跃会员占注册会员总数的比重。<br>会员复购率。指在统计周期内产生二次及二次以上购买的会员占购买会员的总数。<br>会员平均购买次数。指在统计周期内每个会员平均购买的次数，即订单总数/购买用户总数。会员复购率高的电商网站平均购买次数也高。<br>会员回购率。指上一期末活跃会员在下一期时间内有购买行为的会员比率。<br>会员留存率。会员在某段时间内开始访问你的网站，经过一段时间后，仍然会继续访问你的网站就被认作是留存，这部分会员占当时新增会员的比例就是新会员留存率，这种留存的计算方法是按照活跃来计算，另外一种计算留存的方法是按消费来计算，即某段的新增消费用户在往后一段时间时间周期（时间周期可以是日、周、月、季度和半年度）还继续消费的会员比率。留存率一般看新会员留存率，当然也可以看活跃会员留存。留存率反应的是电商留住会员的能力。</p>
<p>3.网站销售（转化率）类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaghR.png" alt="CYaghR.png"></p>
<p>（1）购物车类指标<br>基础类指标，包括一定统计周期内加入购物车次数、加入购物车买家数、加入购物车买家数以及加入购物车商品数。<br>转化类指标，主要是购物车支付转化率，即一定周期内加入购物车商品支付买家数与加入购物车购买家数的比值。</p>
<p>（2）下单类指标<br>基础类指标，包括一定统计周期内的下单笔数、下单金额以及下单买家数。<br>转化类指标，主要是浏览下单转化率，即下单买家数与网站访客数（UV）的比值。</p>
<p>（3）支付类指标<br>基础统计类指标，包括一定统计周期内支付金额、支付买家数和支付商品数。<br>转化类指标。包括浏览-支付买家转化率（支付买家数/网站访客数）、下单-支付金额转化率（支付金额/下单金额）、下单-支付买家数转化率（支付买家数/下单买家数）和下单-支付时长（下单时间到支付时间的差值）。</p>
<p>4.客户价值类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaW1x.png" alt="CYaW1x.png"></p>
<p>客户指标。常见客户指标包括一定统计周期内的累计购买客户数和客单价。客单价是指每一个客户平均购买商品的金额，也即是平均交易金额，即成交金额与成交用户数的比值。</p>
<p>新客户指标。常见新客户指标包括一定统计周期内的新客户数量、新客户获取成本和新客户客单价。其中，新客户客单价是指第一次在店铺中产生消费行为的客户所产生交易额与新客户数量的比值。影响新客户客单价的因素除了与推广渠道的质量有关系，还与电商店铺活动以及关联销售有关。</p>
<p>老客户指标。常见老客户指标包括消费频率、最近一次购买时间、消费金额和重复购买率。消费频率是指客户在一定期间内所购买的次数；最近一次购买时间表示客户最近一次购买的时间离现在有多远；客户消费金额指客户在最近一段时间内购买的金额。消费频率越高，最近一次购买时间离现在越近，消费金额越高的客户越有价值。重复购买率则指消费者对该品牌产品或者服务的重复购买次数，重复购买率越多，则反应出消费者对品牌的忠诚度就越高，反之则越低。重复购买率可以按两种口径来统计：第一种，从客户数角度，重复购买率指在一定周期内下单次数在两次及两次以上的人数与总下单人数之比，如在一个月内，有100个客户成交，其中有20个是购买两次及以上，则重复购买率为20%；第二种，按交易计算，即重复购买交易次数与总交易次数的比值，如某月内，一共产生了100笔交易，其中有20个人有了二次购买，这20人中的10个人又有了三次购买，则重复购买次数为30次，重复购买率为30%。</p>
<p>5.商品类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYafc6.png" alt="CYafc6.png"></p>
<p>产品总数指标。包括SKU、SPU和在线SPU。SKU是物理上不可分割的最小存货单位。SPU即Standard Product Unit （标准化产品单元），SPU是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。通俗点讲，属性值、特性相同的商品就可以称为一个SPU。如iphone5S是一个SPU，而iPhone 5S配置为16G版、4G手机、颜色为金色、网络类型为TD-LTE/TD-SCDMA/WCDMA/GSM则是一个SKU。在线SPU则是在线商品的SPU数。</p>
<p>产品优势性指标。主要是独家产品的收入占比，即独家销售的产品收入占总销售收入的比例。</p>
<p>品牌存量指标。包括品牌数和在线品牌数指标。品牌数指商品的品牌总数量。在线品牌数则指在线商品的品牌总数量。</p>
<p>上架。包括上架商品SKU数、上架商品SPU数、上架在线SPU数、上架商品数和上架在线商品数。</p>
<p>首发。包括首次上架商品数和首次上架在线商品数。</p>
<p>6.市场营销活动指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaIBD.png" alt="CYaIBD.png"></p>
<p>市场营销活动指标。包括新增访问人数、新增注册人数、总访问次数、订单数量、下单转化率以及ROI。其中，下单转化率是指活动期间，某活动所带来的下单的次数与访问该活动的次数之比。投资回报率（ROI）是指，某一活动期间，产生的交易金额与活动投放成本金额的比值。</p>
<p>广告投放指标。包括新增访问人数、新增注册人数、总访问次数、订单数量、UV订单转化率、广告投资回报率。其中，下单转化率是指某广告所带来的下单的次数与访问该活动的次数之比。投资回报率（ROI）是指，某广告产生的交易金额与广告投放成本金额的比值。</p>
<p>7.风控类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYa7AH.png" alt="CYa7AH.png"></p>
<p>买家评价指标。包括买家评价数，买家评价卖家数、买家评价上传图片数、买家评价率、买家好评率以及卖家差评率。其中，买家评价率是指某段时间参与评价的卖家与该时间段买家数量的比值，是反映用户对评价的参与度，电商网站目前都在积极引导用户评价，以作为其他买家购物时候的参考。买家好评率指某段时间内好评的买家数量与该时间段买家数量的比值。同样，买家差评率指某段时间内差评的买家数量与该时间段买家数量的比值。尤其是买家差评率，是非常值得关注的指标，需要监控起来，一旦发现买家差评率在加速上升，一定要提高警惕，分析引起差评率上升的原因，及时改进。</p>
<p>买家投诉类指标。包括发起投诉（或申诉），撤销投诉（或申诉），投诉率（买家投诉人数占买家数量的比例）等。投诉量和投诉率都需要及时监控，以发现问题，及时优化。</p>
<p>8.市场竞争类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaHNd.png" alt="CYaHNd.png"></p>
<p>市场份额相关指标，包括市场占有率、市场扩大率和用户份额。市场占有率指电商网站交易额占同期所有同类型电商网站整体交易额的比重；市场扩大率指购物网站占有率较上一个统计周期增长的百分比；用户份额指购物网站独立访问用户数占同期所有B2C购物网站合计独立访问用户数的比例。</p>
<p>网站排名，包括交易额排名和流量排名。交易额排名指电商网站交易额在所有同类电商网站中的排名；流量排名指电商网站独立访客数量在所有同类电商网站中的排名。</p>
<p>总之，本文介绍了电商数据分析的基础指标体系，涵盖了流量、销售转化率、客户价值、商品类目、营销活动、风控和市场竞争指标，这些指标都需要系统化的进行统计和监控，才能更好的发现电商运营健康度的问题，以更好及时改进和优化，提升电商收入。如销售转化率，其本质上是一个漏斗模型，如从网站首页到最终购买各个阶段的转化率的监控和分析是网站运营健康度很重要的分析方向。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-HBASE：出自Google论文的发表" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/HBASE：出自Google论文的发表/" class="article-date">
  	<time datetime="2018-04-05T13:29:30.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/HBASE：出自Google论文的发表/">
        HBASE总结：出自Google论文的发表
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HBASE：出自Google论文的发表"><a href="#HBASE：出自Google论文的发表" class="headerlink" title="HBASE：出自Google论文的发表"></a>HBASE：出自Google论文的发表</h2><h6 id="列式存储、多版本-timestamp-的、nosql数据库"><a href="#列式存储、多版本-timestamp-的、nosql数据库" class="headerlink" title="[列式存储、多版本(timestamp)的、nosql数据库]"></a>[列式存储、多版本(timestamp)的、nosql数据库]</h6><h6 id="hbase架构图：要能手画出来"><a href="#hbase架构图：要能手画出来" class="headerlink" title="hbase架构图：要能手画出来"></a>hbase架构图：要能<strong>手画</strong>出来</h6><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/ynWG7DnR777pXkx0IGWDtEit4H6y7VkXIgYmJDtxY48!/b/dPMAAAAAAAAA&amp;bo=EAUIAwAAAAARBy4!&amp;rf=viewer_4" alt=""></p>
<h6 id="HBase没有单节点故障-因为zookeeper-，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作"><a href="#HBase没有单节点故障-因为zookeeper-，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作" class="headerlink" title="**HBase没有单节点故障(因为zookeeper)，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作"></a>**HBase没有单节点故障(因为zookeeper)，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作</h6><h4 id="HBase的功能："><a href="#HBase的功能：" class="headerlink" title="HBase的功能："></a>HBase的功能：</h4><pre><code>*hadoop数据库:
    1.存储数据 
    2.检索数据
*和RDBMS[关系型数据库]相比：
    1.海量数据：数据条目数--上亿
    2.检索的速度：准时性、秒级别
*基于hdfs： hdfs的优势
    1.数据安全性[副本机制]
    2.普通商用PC server就ok
</code></pre><h5 id="1-Table中的所有行都是按照rowkey的字典序排列"><a href="#1-Table中的所有行都是按照rowkey的字典序排列" class="headerlink" title="1.Table中的所有行都是按照rowkey的字典序排列"></a>1.Table中的所有行都是按照rowkey的字典序排列</h5><h5 id="2-Table在行的方向上分割为多个Region"><a href="#2-Table在行的方向上分割为多个Region" class="headerlink" title="2.Table在行的方向上分割为多个Region"></a>2.Table在行的方向上分割为多个Region</h5><h5 id="3-Region是按照大小分割的，每个表开始只有一个region-会有一个starkey和endkey-前毕后包-，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。"><a href="#3-Region是按照大小分割的，每个表开始只有一个region-会有一个starkey和endkey-前毕后包-，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。" class="headerlink" title="3.Region是按照大小分割的，每个表开始只有一个region[会有一个starkey和endkey,前毕后包]，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。"></a>3.Region是按照大小分割的，每个表开始只有一个region[会有一个starkey和endkey,前毕后包]，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。</h5><h5 id="4-Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上"><a href="#4-Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上" class="headerlink" title="4.Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上."></a>4.Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上.</h5><h5 id="5-Region虽然是分布式存储的最小单元，但是不是存储的最小单元。"><a href="#5-Region虽然是分布式存储的最小单元，但是不是存储的最小单元。" class="headerlink" title="5.Region虽然是分布式存储的最小单元，但是不是存储的最小单元。"></a>5.Region虽然是分布式存储的最小单元，但是不是存储的最小单元。</h5><pre><code>存储的最小单元是cell，{rowkey,column,version}
        *唯一性
        *数据没有类型，以字节码形式存储

region由一个或多个store组成，每个store保存一个column family；
每个store又由一个memStore和0至多个storeFile组成；
memStore存储在内存中，storeFile存储在hdfs上
</code></pre><h6 id="hbase-旧版本的web监听端口-60010，新版本是16010"><a href="#hbase-旧版本的web监听端口-60010，新版本是16010" class="headerlink" title="hbase 旧版本的web监听端口 60010，新版本是16010"></a>hbase 旧版本的web监听端口 60010，新版本是16010</h6><h3 id="HBASE数据写入流程："><a href="#HBASE数据写入流程：" class="headerlink" title="HBASE数据写入流程："></a>HBASE数据写入流程：</h3><pre><code>put ---&gt; cell
    step0： 先进行记录 ---&gt;HLog 【WAL（write ahead logging:日志预写功能）】 loc：hdfs
            存储的log数据类型是sequenceFile
</code></pre><p>——- WAL的作用是防止写入内存时，region挂掉，数据丢失</p>
<pre><code>step1： 写入memStore     loc：内存
step2： 当内存达到一定阈值时，写入storeFile  ---&gt; loc： hdfs
</code></pre><h3 id="HBASE-用户读取数据流程："><a href="#HBASE-用户读取数据流程：" class="headerlink" title="HBASE 用户读取数据流程："></a>HBASE 用户读取数据流程：</h3><pre><code>1)先到memStore里面去读
2)然后到BlockCache里面读 --&gt; *每个RegionServer只有一个BlockCache*
3)最后到HFile里面读取数据
4)然后对数据进行Merge --&gt; 最后返回数据集
</code></pre><h4 id="MemSore和BlockCache"><a href="#MemSore和BlockCache" class="headerlink" title="MemSore和BlockCache"></a>MemSore和BlockCache</h4><pre><code>*HBase上的RegionServer的内存分为两部分：
    1.Memstore --&gt; 用来 写
        写请求会先写入Memstore，RegionServer会给每个Region提供一个Memstore，当Memstore写满64M以后，会启动flush舒心到磁盘。
    2.BlockCache --&gt; 用来读
        读请求先到Memstore中查询数据，查不到就到BlockCache中查询，再差不读奥就会到磁盘上读，并把读的结果放入BlockCache。
        BlockCache达到上限后，会启动淘汰机制，淘汰掉最老的一批数据
*在注重读取响应时间的场景下，可以将BlockCache设置大些，Memstore设置小些，以加大缓存的命中率。
</code></pre><h3 id="数据检索的三种方式："><a href="#数据检索的三种方式：" class="headerlink" title="数据检索的三种方式："></a>数据检索的三种方式：</h3><pre><code>1.get rowkey --- 最快的方式
2.scan range --- 用的最多的方式 【一般先range扫描，然后get】
3.scan ---全表扫描，基本不用
</code></pre><h4 id="HBase中有类似于RDBMS中Database的概念"><a href="#HBase中有类似于RDBMS中Database的概念" class="headerlink" title="HBase中有类似于RDBMS中Database的概念"></a>HBase中有类似于RDBMS中Database的概念</h4><pre><code>命名空间：
    *用户自定义的表的命名空间，默认情况下 ---&gt;  default
    *系统自带的元数据的表的命名空间 ---&gt; hbase
</code></pre><h3 id="HBase数据迁移"><a href="#HBase数据迁移" class="headerlink" title="HBase数据迁移"></a>HBase数据迁移</h3><pre><code>使用 MapReduce把文件生成hdfs上的HFile，然后进行bulk load into hbase table
</code></pre><h1 id="HBase表RowKey的设计-："><a href="#HBase表RowKey的设计-：" class="headerlink" title="**HBase表RowKey的设计**："></a><strong>**</strong>HBase表RowKey的设计<strong>**</strong>：</h1><h5 id="现实环境中出现的问题："><a href="#现实环境中出现的问题：" class="headerlink" title="**现实环境中出现的问题："></a>**现实环境中出现的问题：</h5><pre><code>*默认情况下创建一张HBase表，自动会为表创建一个Region [startkey,endkey) 前毕后包
    *无论在测试环境还是生产环境中，创建完表以后会往表中导入大量数据
        步骤：
            file/datas -&gt; HFile -&gt; bulk load into hbase table
        此时Region只有一个，而region是被RegionServer管理的，
        当导入数据量慢慢增大后Region就会被split成两个Region，
        但是此时RegionServer很可能就会挂掉，此时就会出现问题...

解决方案[举例]：
    创建表时，多创建一些Region(依据表的rowkey进行设计 + 结合业务)
        比如说：有5个region，他们被多个RegionServer管理
            再插入数据时，会向5个Region中分别插入数据，这样就均衡了
</code></pre><h5 id="具体解决方案："><a href="#具体解决方案：" class="headerlink" title="**具体解决方案："></a>**具体解决方案：</h5><p>表的RowKey设计中：</p>
<pre><code>**如何在海量数据中，查询到我想要的数据???
    核心思想：
        1.依据RowKey查询最快
        2.对RowKey进行范围查询range
        3.前缀查询  
            比如：startkey:15221467820_20180409000053 和 endkey:15221467828_20180419000053
            这时只会比较15221467820 和 15221467828 而后面的_2018.. 则不会进行匹配    
</code></pre><p>解决方法：</p>
<pre><code>a).HBase的预分区：
    Region的划分依赖于RowKey，预先预估一些RowKey(年月日时分秒)[最常用的就是这两种，其他的方法rowkey设计都不能自定义]
        1. create &apos;tb1&apos;,&apos;cf1&apos;,splits =&gt; [&apos;20180409000000&apos;,&apos;20180419000000&apos;,&apos;20180429000000&apos;]
        2. create &apos;tb1&apos;,&apos;cf1&apos;,splits_file =&gt; &apos;/root/data/logs-split.txt&apos;
           [这种情况就是把分段的rowkey写入了文件里]
    这样就分为了4个Region，因为它会把头和尾也算进去
b).根据业务来对RowKey的设计：
        举例：移动运营商电话呼叫查询详单：
            RowKey: 
                phone + time 
                15221467820_20180409010000
            infor: 
                上海  主叫   152216888888   30        国内通话 0.00
                area active opposite_phone talk_time model price 
        依据查询条件：
            phone + (start_time - end_time)
        代码：
            scan 
                startrow
                    15221467820_20180409000000
                stoprow
                    15221467820_20180419000000
    保证了实时性
</code></pre><h3 id="HBase数据存储二级索引-索引表-的设计以及数据同步解决方案："><a href="#HBase数据存储二级索引-索引表-的设计以及数据同步解决方案：" class="headerlink" title="HBase数据存储二级索引[索引表]的设计以及数据同步解决方案："></a>HBase数据存储二级索引[索引表]的设计以及数据同步解决方案：</h3><p>基于以上的移动运营商电话呼叫查询详单：</p>
<pre><code>二级索引表：
    RowKey:
        opposite_phone
    columnFamily对应的colume:
        主表的RowKey
主表和索引表的同步：
    &gt;&gt; phoenix 
            &gt;&gt; jdbc 方式才能同步
    &gt;&gt;先在solr里创建索引表
     solr   //在solr中创建索引表
            国外有个框架交 lily
                cloudera search 封装了lily，只要配置下cloudera
                cloudera search会在主表插入数据时，自动更新solr里的索引表
</code></pre><h2 id="HBase之-表的属性"><a href="#HBase之-表的属性" class="headerlink" title="HBase之 表的属性:"></a>HBase之 表的属性:</h2><h3 id="面试会问到：-每个RegionServer只有一个HLog和一个BlockCache"><a href="#面试会问到：-每个RegionServer只有一个HLog和一个BlockCache" class="headerlink" title="面试会问到：**每个RegionServer只有一个HLog和一个BlockCache"></a>面试会问到：**每个RegionServer只有一个HLog和一个BlockCache</h3><pre><code>1.HBase的表的压缩[HFile compression]--表的属性:
**在企业中通常使用snappy格式进行压缩，可以减少容量的存储，和减少io流，提高传输效率
    跟hive一样，需要一些编译jar包，然后放到lib的native里
    [这里使用软连接 ln -s /xx/xx /xx  具体看官方文档]，还有一些参数的配置，才能使用
        创建表的时候：
            create &apos;user&apos;,{NAME=&apos;cf1&apos;,COMPRESSION=&gt;&apos;SNAPPY&apos;}
2. HBase 标的属性：IN_MEMORY=&gt;&apos;false&apos;、BLOCKCACHE=&gt;&apos;true&apos;
        HBase为什么查询这么快 就是因为这些表的属性的存储机制！！
        通常我们自定义创建的表的相应列簇下的IN_MEMORY属性都是false!
        通过describe &apos;hbase:meta&apos;可以查询到 IN_MEMORY=&gt;&apos;true&apos; BLOCKCACHE =&gt; &apos;true&apos;,
        意为meta元数据存放于blockcache的inmemory内存中
            IN_MEMORY队列存放HBase的meta表元数据信息，因为meta存储了用户表的
            通常 BLOCKCACHE 这个属性的设置要慎用，比如我们查询后的数据后续还会用到这时可以设置为true
            如果后续用不到这些数据就要设置此属性为false
</code></pre><h4 id="HBase的Cache分等级："><a href="#HBase的Cache分等级：" class="headerlink" title="HBase的Cache分等级："></a>HBase的Cache分等级：</h4><pre><code>默认情况下：整个BlockCache的内存分配 
1.IN_MEMORY 1/4    in_memory 用于存储hbase的meta元数据信息    2.Single 1/4   single就是用的次数较少的数据
3.Muti    1/2          muti就是用的次数较多的数据
如果BlockCache的内存到达阈值，会先清理single然后清理muti，in_memory的内存不会被清理!
</code></pre><h3 id="HBase表的compaction"><a href="#HBase表的compaction" class="headerlink" title="HBase表的compaction"></a>HBase表的compaction</h3><h6 id="随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种："><a href="#随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种：" class="headerlink" title="随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种："></a>随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种：</h6><pre><code>1.minor compaction
    Minor Compaction是指选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，
    在这个过程中不会处理已经Deleted或Expired的Cell。一次Minor Compaction的结果是更少并且更大的StoreFile。
2.major compaction
    Major Compaction是指将所有的StoreFile合并成一个StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL过期数据、版本号超过设定版本号的数据。另外，一般情况下，Major Compaction时间会持续比较长，会阻塞数据的吸入和查询!!!，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此线上业务都会将关闭自动触发Major Compaction功能，改为手动在业务低峰期触发。
</code></pre><h4 id="HBase和Hive集成："><a href="#HBase和Hive集成：" class="headerlink" title="HBase和Hive集成："></a>HBase和Hive集成：</h4><pre><code>此时实质上Hive就是HBase的一个客户端!
应用场景：
    日志文件 --&gt; hive --&gt; hive-hbase-table --&gt; insert .. select ...
</code></pre><h4 id="电商订单查询之HBase："><a href="#电商订单查询之HBase：" class="headerlink" title="电商订单查询之HBase："></a>电商订单查询之HBase：</h4><pre><code>1.用户下单后--&gt;存到RDBMS[关系型数据库]中 即未完成的订单
2.订单完成后--&gt;数据迁移到HBase中

查询比如三个月内的订单：
 1.主表[订单显示表]：
    rowkey: userid_orderCreateTime_orderId  
    cfname: orderInfor
    [本来userid+orderCreateTime就可以保证唯一性，加orderId的原因是为了二次查询]
 2.订单详情表:
     rowkey: orderId_orderItemId
    cfname: itemInfor
 3.索引表：
     比如用户输入订单号：我们后台Redis或者MongoDB里会有相对应的订单编码
     获取到订单编码然后到索引表中去查
         索引表的rowkey：订单编号 column: 主表的rowkey
         就可以到hbase主表中查询订单
</code></pre><h5 id="补充文件类型-tsv-tab-csv-comma-："><a href="#补充文件类型-tsv-tab-csv-comma-：" class="headerlink" title="补充文件类型 .tsv[tab] .csv[comma]："></a>补充文件类型 .tsv[tab] .csv[comma]：</h5><pre><code>user.tsv tsv后缀格式的文件：每行数据以tab 制表符分割的 
user.csv csv后缀格式的文件：每行数据以逗号分割的
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-DMP" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/DMP/" class="article-date">
  	<time datetime="2018-04-05T13:29:28.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/DMP/">
        dmp介绍
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>dmp</p>
<p>（数据管理平台）<br> 编辑<br>DMP(Data Management Platform)数据管理平台，是把分散的多方数据进行整合纳入统一的技术平台，并对这些数据进行标准化和细分，让用户可以把这些细分结果推向现有的互动营销环境里的平台。<br>中文名<br>数据管理平台<br>外文名<br>Data Management Platform<br>释    义<br>数据进行整合纳入统一的技术平台<br>简    称<br>DMP<br>目录<br>.    1 作用<br>.    2 类型<br>作用<br>编辑<br>•能快速查询、反馈和快速呈现结果<br>•能帮助客户更快进入到市场周期中<br>•能促成企业用户和合作伙伴之间的合作<br>•能深入的预测分析并作出反应<br>•能带来各方面的竞争优势<br>•能降低信息获取及人力成本<br>类型<br>编辑<br>1、结构化的数据，比如Oracle数据库数据等；<br>　　2、非结构化的数据，比如各种文件、图像、音频等数据，等等。<br>　　结构化数据（即数据库数据）在当今的信息系统中占据最核心、最重要的位置。结构化数据从产生―使用―消亡这样一个完整过程的管理，就是数据生命周期管理（所谓的ILM）。<br>核心元素包括：<br>•数据整合及标准化能力：采用统一化的方式，将各方数据吸纳整合。<br>•数据细分管理能力：创建出独一无二、有意义的客户细分，进行有效营销活动。<br>•功能健全的数据标签：提供数据标签灵活性，便于营销活动的使用。<br>•自助式的用户界面：基于网页web界面或其他集成方案直接获取数据工具，功能和几种形式报表和分析。<br>•相关渠道环境的连接：跟相关渠道的集成，包含网站端、展示广告、电子邮件以及搜索和视频，让营销者能找到、定位和提供细分群体相关高度的营销信息。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-文本相似度怎么比较---TF-IDF算法及应用" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/文本相似度怎么比较---TF-IDF算法及应用/" class="article-date">
  	<time datetime="2018-04-05T11:29:30.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/文本相似度怎么比较---TF-IDF算法及应用/">
        文本相似度比较-----TF-IDF算法及应用
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>【算法】TF-IDF算法及应用</p>
<p>**小编邀请您，先思考：</p>
<blockquote>
<p>1 如何计算TF-IDF？<br>2 TF-IDF有什么应用？<br>3 如何提取文本的关键词和摘要？**</p>
</blockquote>
<p>有一篇很长的文章，我要用计算机提取它的关键词（Automatic Keyphrase extraction），完全不加以人工干预，请问怎样才能正确做到？</p>
<p>这个问题涉及到数据挖掘、文本处理、信息检索等很多计算机前沿领域，但是出乎意料的是，有一个非常简单的经典算法，可以给出令人相当满意的结果。它简单到都不需要高等数学，普通人只用10分钟就可以理解，这就是我今天想要介绍的TF-IDF（<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a> ）算法。</p>
<p>让我们从一个实例开始讲起。假定现在有一篇长文《中国的蜜蜂养殖》，我们准备用计算机提取它的关键词。</p>
<p>一个容易想到的思路，就是找到出现次数最多的词。如果某个词很重要，它应该在这篇文章中多次出现。于是，我们进行”词频”（Term Frequency，缩写为TF）统计。</p>
<p>结果你肯定猜到了，出现次数最多的词是—-“的”、”是”、”在”—-这一类最常用的词。它们叫做”停用词”（ <a href="http://baike.baidu.com/view/3784680.htm" target="_blank" rel="noopener">http://baike.baidu.com/view/3784680.htm</a> ）（stop words），表示对找到结果毫无帮助、必须过滤掉的词。</p>
<p>假设我们把它们都过滤掉了，只考虑剩下的有实际意义的词。这样又会遇到了另一个问题，我们可能发现”中国”、”蜜蜂”、”养殖”这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？</p>
<p>显然不是这样。因为”中国”是很常见的词，相对而言，”蜜蜂”和”养殖”不那么常见。如果这三个词在一篇文章的出现次数一样多，有理由认为，”蜜蜂”和”养殖”的重要程度要大于”中国”，也就是说，在关键词排序上面，”蜜蜂”和”养殖”应该排在”中国”的前面。</p>
<p>所以，我们需要一个重要性调整系数，衡量一个词是不是常见词。<strong>如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。</strong></p>
<p>用统计学语言表达，就是在词频的基础上，要对每个词分配一个”重要性”权重。最常见的词（”的”、”是”、”在”）给予最小的权重，较常见的词（”中国”）给予较小的权重，较少见的词（”蜜蜂”、”养殖”）给予较大的权重。这个权重叫做”逆文档频率”（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。</p>
<p><strong>知道了”词频”（TF）和”逆文档频率”（IDF）以后，将这两个值相乘，就得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。</strong></p>
<p>下面就是这个算法的细节。<br><strong>第一步，计算词频。</strong><br><img src="https://s1.ax1x.com/2018/05/04/Ct7BC9.png" alt="Ct7BC9.png"><br>考虑到文章有长短之分，为了便于不同文章的比较，进行”词频”标准化。<br><img src="https://s1.ax1x.com/2018/05/04/Ct7cDK.png" alt="Ct7cDK.png"><br>或者<br><img src="https://s1.ax1x.com/2018/05/04/Ct7RED.png" alt="Ct7RED.png"></p>
<p><strong>第二步，计算逆文档频率。</strong><br>这时，需要一个语料库（corpus），用来模拟语言的使用环境。<br><img src="https://s1.ax1x.com/2018/05/04/Ct74Cd.png" alt="Ct74Cd.png"><br>如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。</p>
<p><strong>第三步，计算TF-IDF。</strong><br><img src="https://s1.ax1x.com/2018/05/04/CtHqQ1.png" alt="CtHqQ1.png"><br><strong>可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。</strong>所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。</p>
<p>还是以《中国的蜜蜂养殖》为例，假定该文长度为1000个词，”中国”、”蜜蜂”、”养殖”各出现20次，则这三个词的”词频”（TF）都为0.02。然后，搜索Google发现，包含”的”字的网页共有250亿张，假定这就是中文网页总数。包含”中国”的网页共有62.3亿张，包含”蜜蜂”的网页为0.484亿张，包含”养殖”的网页为0.973亿张。则它们的逆文档频率（IDF）和TF-IDF如下：<br><img src="https://s1.ax1x.com/2018/05/04/CtbUfJ.png" alt="CtbUfJ.png"><br>从上表可见，”蜜蜂”的TF-IDF值最高，”养殖”其次，”中国”最低。（如果还计算”的”字的TF-IDF，那将是一个极其接近0的值。）所以，如果只选择一个词，”蜜蜂”就是这篇文章的关键词。</p>
<p>除了自动提取关键词，TF-IDF算法还可以用于许多别的地方。比如，信息检索时，对于每个文档，都可以分别计算一组搜索词（”中国”、”蜜蜂”、”养殖”）的TF-IDF，将它们相加，就可以得到整个文档的TF-IDF。这个值最高的文档就是与搜索词最相关的文档。</p>
<p>TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以”词频”衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。）</p>
<p><strong>找出相似文章</strong><br>我们再来研究另一个相关的问题。有些时候，除了找到关键词，我们还希望找到与原文章相似的其他文章。比如，”Google新闻”在主新闻下方，还提供多条相似的新闻。<br><img src="https://s1.ax1x.com/2018/05/04/Ctbs0K.jpg" alt="Ctbs0K.jpg"></p>
<p>为了找出相似的文章，需要用到”余弦相似性” （ <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Cosine_similarity</a> ）（cosine similiarity）。下面，我举一个例子来说明，什么是”余弦相似性”。</p>
<p>为了简单起见，我们先从句子着手。</p>
<blockquote>
<p>   句子A：我喜欢看电视，不喜欢看电影。<br>   句子B：我不喜欢看电视，也不喜欢看电影。</p>
</blockquote>
<p>请问怎样才能计算上面两句话的相似程度？<br>基本思路是：如果这两句话的用词越相似，它们的内容就应该越相似。因此，可以从词频入手，计算它们的相似程度。</p>
<p><strong>第一步，分词。</strong></p>
<blockquote>
<p>   句子A：我/喜欢/看/电视，不/喜欢/看/电影。<br>   句子B：我/不/喜欢/看/电视，也/不/喜欢/看/电影。</p>
</blockquote>
<p><strong>第二步，列出所有的词。</strong></p>
<blockquote>
<p>   我，喜欢，看，电视，电影，不，也。</p>
</blockquote>
<p><strong>第三步，计算词频。</strong></p>
<blockquote>
<p>   句子A：我 1，喜欢 2，看 2，电视 1，电影 1，不 1，也 0。<br>   句子B：我 1，喜欢 2，看 2，电视 1，电影 1，不 2，也 1。</p>
</blockquote>
<p><strong>第四步，写出词频向量。</strong></p>
<blockquote>
<p>   句子A：[1, 2, 2, 1, 1, 1, 0]<br>   句子B：[1, 2, 2, 1, 1, 2, 1]</p>
</blockquote>
<p>到这里，问题就变成了如何计算这两个向量的相似程度。<br>我们可以把它们想象成空间中的两条线段，都是从原点（[0, 0, …]）出发，指向不同的方向。两条线段之间形成一个夹角，如果夹角为0度，意味着方向相同、线段重合；如果夹角为90度，意味着形成直角，方向完全不相似；如果夹角为180度，意味着方向正好相反。<strong>因此，我们可以通过夹角的大小，来判断向量的相似程度。夹角越小，就代表越相似。</strong><br><img src="https://s1.ax1x.com/2018/05/04/CtbckD.png" alt="CtbckD.png"><br>以二维空间为例，上图的a和b是两个向量，我们要计算它们的夹角θ。余弦定理告诉我们，可以用下面的公式求得：<br><img src="https://s1.ax1x.com/2018/05/04/Ctb2fH.png" alt="Ctb2fH.png"><br><img src="https://s1.ax1x.com/2018/05/04/Ctbf1A.png" alt="Ctbf1A.png"><br>假定a向量是[x1, y1]，b向量是[x2, y2]，那么可以将余弦定理改写成下面的形式：<br><img src="https://s1.ax1x.com/2018/05/04/CtbInP.png" alt="CtbInP.png"><br><img src="https://s1.ax1x.com/2018/05/04/Ctbo0f.png" alt="Ctbo0f.png"><br>数学家已经证明，余弦的这种计算方法对n维向量也成立。假定A和B是两个n维向量，A是 [A1, A2, …, An] ，B是 [B1, B2, …, Bn] ，则A与B的夹角θ的余弦等于：<br><img src="https://s1.ax1x.com/2018/05/04/CtbvXq.png" alt="CtbvXq.png"><br>使用这个公式，我们就可以得到，句子A与句子B的夹角的余弦。<br><img src="https://s1.ax1x.com/2018/05/04/CtqCAU.png" alt="CtqCAU.png"><br><strong>余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫”余弦相似性”。</strong>所以，上面的句子A和句子B是很相似的，事实上它们的夹角大约为20.3度。<br>由此，我们就得到了”找出相似文章”的一种算法：<br>　　（1）使用TF-IDF算法，找出两篇文章的关键词；<br>　　（2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；<br>　　（3）生成两篇文章各自的词频向量；<br>　　（4）计算两个向量的余弦相似度，值越大就表示越相似。<br>“余弦相似度”是一种非常有用的算法，只要是计算两个向量的相似程度，都可以采用它。</p>
<p><strong>自动摘要</strong><br>有时候，很简单的数学方法，就可以完成很复杂的任务。<br>前两部分就是很好的例子。仅仅依靠统计词频，就能找出关键词和相似文章。虽然它们算不上效果最好的方法，但肯定是最简便易行的方法。<br>接下来讨论如何通过词频，对文章进行自动摘要（Automatic summarization）。</p>
<p>如果能从3000字的文章，提炼出150字的摘要，就可以为读者节省大量阅读时间。由人完成的摘要叫”人工摘要”，由机器完成的就叫”自动摘要”。许多网站都需要它，比如论文网站、新闻网站、搜索引擎等等。2007年，美国学者的论文《A Survey on Automatic Text Summarization》（Dipanjan Das, Andre F.T. Martins, 2007）总结了目前的自动摘要算法。其中，很重要的一种就是词频统计。</p>
<p>这种方法最早出自1958年的IBM公司科学家H.P. Luhn的论文《The Automatic Creation of Literature Abstracts》。<br>Luhn博士认为，文章的信息都包含在句子中，有些句子包含的信息多，有些句子包含的信息少。”自动摘要”就是要找出那些包含信息最多的句子。<br>句子的信息量用”关键词”来衡量。如果包含的关键词越多，就说明这个句子越重要。Luhn提出用”簇”（cluster）表示关键词的聚集。所谓”簇”就是包含多个关键词的句子片段。<br><img src="https://s1.ax1x.com/2018/05/04/CtqPNF.png" alt="CtqPNF.png"><br>上图就是Luhn原始论文的插图，被框起来的部分就是一个”簇”。只要关键词之间的距离小于”门槛值”，它们就被认为处于同一个簇之中。Luhn建议的门槛值是4或5。也就是说，如果两个关键词之间有5个以上的其他词，就可以把这两个关键词分在两个簇。</p>
<p>下一步，对于每个簇，都计算它的重要性分值。<br><img src="https://s1.ax1x.com/2018/05/04/CtqA39.png" alt="CtqA39.png"><br>以前图为例，其中的簇一共有7个词，其中4个是关键词。因此，它的重要性分值等于 ( 4 x 4 ) / 7 = 2.3。</p>
<p>然后，找出包含分值最高的簇的句子（比如5句），把它们合在一起，就构成了这篇文章的自动摘要。具体实现可以参见《Mining the Social Web: Analyzing Data from Facebook, Twitter, LinkedIn, and Other Social Media Sites》（O’Reilly, 2011）一书的第8章，python代码见github。</p>
<p>Luhn的这种算法后来被简化，不再区分”簇”，只考虑句子包含的关键词。下面就是一个例子（采用伪码表示），只考虑关键词首先出现的句子。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">　　Summarizer(originalText, maxSummarySize):</span><br><span class="line">　　　　// 计算原始文本的词频，生成一个数组，比如[(<span class="number">10</span>,<span class="string">'the'</span>), (<span class="number">3</span>,<span class="string">'language'</span>), (<span class="number">8</span>,<span class="string">'code'</span>)...]</span><br><span class="line">　　　　wordFrequences = getWordCounts(originalText)</span><br><span class="line">　　　　// 过滤掉停用词，数组变成[(<span class="number">3</span>, <span class="string">'language'</span>), (<span class="number">8</span>, <span class="string">'code'</span>)...]</span><br><span class="line">　　　　contentWordFrequences = filtStopWords(wordFrequences)</span><br><span class="line">　　　　// 按照词频进行排序，数组变成[<span class="string">'code'</span>, <span class="string">'language'</span>...]</span><br><span class="line">　　　　contentWordsSortbyFreq = sortByFreqThenDropFreq(contentWordFrequences)</span><br><span class="line">　　　　// 将文章分成句子</span><br><span class="line">　　　　sentences = getSentences(originalText)</span><br><span class="line">　　　　// 选择关键词首先出现的句子</span><br><span class="line">　　　　setSummarySentences = &#123;&#125;</span><br><span class="line">　　　　foreach word <span class="keyword">in</span> contentWordsSortbyFreq:</span><br><span class="line">　　　　　　firstMatchingSentence = search(sentences, word)</span><br><span class="line">　　　　　　setSummarySentences.add(firstMatchingSentence)</span><br><span class="line">　　　　　　<span class="keyword">if</span> setSummarySentences.size() = maxSummarySize:</span><br><span class="line">　　　　　　　　<span class="keyword">break</span></span><br><span class="line">　　　　// 将选中的句子按照出现顺序，组成摘要</span><br><span class="line">　　　　summary = <span class="string">""</span></span><br><span class="line">　　　　foreach sentence <span class="keyword">in</span> sentences:</span><br><span class="line">　　　　　　<span class="keyword">if</span> sentence <span class="keyword">in</span> setSummarySentences:</span><br><span class="line">　　　　　　　　summary = summary + <span class="string">" "</span> + sentence</span><br><span class="line">　　　　<span class="keyword">return</span> summary</span><br></pre></td></tr></table></figure></p>
<p>类似的算法已经被写成了工具，比如基于Java的Classifier4J库的SimpleSummariser模块、基于C语言的OTS库、以及基于classifier4J的C#实现和python实现。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2018 rongyuewu
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>