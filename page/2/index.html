<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">rongyuewu</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="https://github.com/smackgg/hexo-theme-smackdown">smackdown</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">rongyuewu</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">rongyuewu</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-点击流日志分析流程" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/点击流日志分析流程/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/点击流日志分析流程/">
        点击流日志分析流程
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="点击流日志分析流程"><a href="#点击流日志分析流程" class="headerlink" title="点击流日志分析流程"></a>点击流日志分析流程</h1><p>流程图如下：<br>    <img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/UbKrmZeyW3FpirSZBEMFrDGawneMp6lIpLWKfxekmLw!/b/dJUAAAAAAAAA&amp;bo=QgauAgAAAAARB9g!&amp;rf=viewer_4" alt=""><br>    <img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/FimQ39x6PIwqFTl56xYF*n.qFv3LMDsUx5Bk7YsN*vs!/b/dGoBAAAAAAAA&amp;bo=kgU4BAAAAAARB5s!&amp;rf=viewer_4" alt=""></p>
<h6 id="原始数据："><a href="#原始数据：" class="headerlink" title="原始数据："></a>原始数据：</h6><pre><code>194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] &quot;GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/4.0 (compatible;)&quot;
字段解析:
1、访客 ip 地址: 58.215.204.118
2、访客用户信息: - -
3、请求时间:[18/Sep/2013:06:51:35 +0000]
4、请求方式:GET
5、请求的 url:/wp-includes/js/jquery/jquery.js?ver=1.10.2 6、请求所用协议:HTTP/1.1
7、响应码:304
8、返回的数据流量:0
9、访客的来源 url:http://blog.fens.me/nodejs-socketio-chat/
10 、 访 客 所 用 浏 览 器 : Mozilla/5.0 (Windows NT 5.1; rv:23.0) Firefox/23.0
Gecko/20100101
</code></pre><p>1.根据原始数据生成点击流模型 PageViews 和 Visits</p>
<pre><code>PageViews:
    根据IP判断是否是同一用户，根据前后两条日志时间相差是否在30分钟内，
    判断访问日志是否是属于同一个session[会话]，按照时间顺序标上步骤，
    这样就构成了一条访问轨迹线;
Visits：
    侧重于体现用户在一次session中的进入离开时间、进入离开页面，
    还有统计出在本次session中用户总共访问了几个页面
</code></pre><p>2.漏斗模型</p>
<pre><code>逐层递减
</code></pre><p>3.常见指标：</p>
<pre><code>骨灰级指标：
    IP：1天内访问网站的不重复IP总数
    PV[PageView]:用户每打开1次网页，记录1个PV
    UV[Unique Pageview]:1天以内，访问网站不重复的用户数据(以cookie为依据)，1天内同1访客多次访问网站只被计算1次
基础级指标：
    访问次数：
        访客从进入网站到离开网站一系列活动极为一次访问，也就是session
    网站停留时间：
        访问者在网站上花费的时间
    页面停留时间：
        访问者在某个特定页面或某组网页上所花费的时间
复合级指标:
    人均浏览页面：
        浏览次数/独立访客数  --体现网站对访客的吸引程度
    二跳率：
        二跳率的概念是当网站页面展开后，用户在页面上产生的首次点击被称为“二跳”，二跳的次数即为“二跳量”。二跳量与到达量（进入网站的人）的比值称为页面的二跳率。
    跳出率：
        跳出率是指在只访问了入口页面（例如网站首页）就离开的访问量与所产生总访问量的百分比。跳出率计算公式：跳出率=访问一个页面后离开网站的次数/总访问次数。
    二跳率越高越好，跳出率越低越好。
4.基础分析(PV,IP,UV)
5.来源分析
6.受访分析
7.访客分析
    终端详情[PC,移动端]新老访客、忠诚度、活跃度
8.转化路径分析
</code></pre><blockquote>
<blockquote>
<blockquote>
<p>数据处理流程：</p>
</blockquote>
</blockquote>
</blockquote>
<pre><code>&gt; 数据采集
    Flume采集需要在配置文件里配置source、channel、sink：
        source:
            1.spoolDir的作用是：监控文件夹，如果有新的文件产生，采集开始
            2.exec tail -f access.log 只能监听文件追加的内容
    以上1和2都没办法满足我们的log日志采集，因为既要监控文件也要监控文件夹，
    **好在Flume1.7的稳定版本提供了TAILDIR类型的source，
    可以监控一个目录，并且使用正则表达式匹配目录中的文件名进行实时收集，具体配置详情如下：
        a1.sources.r1.type = TAILDIR
        a1.sources.r1.positionFile = /var/log/flume/taildir_position.json
        a1.sources.r1.filegroups = f1 f2
        a1.sources.r1.filegroups.f1 = /var/log/test1/example.log 
        a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*
    解释：
        filegroups:指定 filegroups，可以有多个[每个还可以使用正则表达式来匹配]，
        以空格分隔;(TailSource 可以同时监控 tail 多个目录中的文件)
      **positionFile:解决了机器重启后无法**断点续传**的问题[检查点文件会以 json 格式保存已经 tail 文件的位置]

&gt; 数据预处理
    通过MapReduce程序对采集到的原始日志数据进行预处理，
    比如清洗，格式整理，滤除脏数据等，并且梳理成点击流模型数据
        1.一般来说，开发中针对不合法的数据，我们不是直接删除，而是打个标签 比如true或者false，
        因为这些数据可能对这个场景是无用的，但是对其他场景是有用的
        2.编写MapReduce程序，只有map没有reduce，因为输入一条数据，处理完后直接输出不需要聚合,setReduceNum = 0 ,输出的结果文件就是part-m
        3.编写相对应的Javabean的时候要实现Writable接口，
        重写toString方法的时候是按照Hive的默认分隔符&apos;\001&apos;进行分割的，
        导入hive表的时候直接按照默认的分隔符就ok了,
        注意：readFields 和 write 的方法写得时候要一致对应
        4.业务要求状态码为400以上的设置valid为false，还有时间不合法[为null或者双引号]
        5.过滤掉静态资源[图片/css/js],这个标准是根据业务来定的，一般会在mapper的set up 
        初始化方法里定义hashSet来存这些准则，然后进行标记清除
        6.PageViews数据生成： (session[UUID] + stayTime + step)
        ---------------------------------------------------------------------------------------
                    Session + IP + 地址 + 时间 + 访问页面 + URL + 停留时长 + 第几步
        ---------------------------------------------------------------------------------------
            a) 编写MapReduce程序，map端以Ip为key，Javabean为value，发送到reduce，
            reduce端进行values的排序，这里需要遍历values，每一次都new
            一个新的Javabean然后进行赋值[因为Javabean是引用类型，然后添加入新的ArrayList中，
            如果不重新new 一个新的对象的话，那么到最后ArrayList里面的对象就是同一个，因为他们指向的都是同一块堆内存！！！]，
            再把这个Javabean添加到新的ArrayList中
            b) 按照时间排序,然后对新的ArrayList进行排序，Collections.sort(beansList,new Comparabtor(javabean){中间获取时间来进行升序排序})
            c) 从有序的beans中分辨出歌词visit，并对一次visit中所访问的page按顺序标号step
                核心思想：比较相邻两条记录中的时间差，如果时间差&lt;30分钟，则该两条记录属于同一个session[生成UUID]，否则属于不同的session
                只有1条的和大于1条的，他们的默认时间都是60秒
        7.Visits模型：数据来自PageViews模型
        ---------------------------------------------------------------------------------------
        Session + 起始时间 + 结束时间 + 进入页面URL + 离开页面URL + 访问页面数 + 停留时长 + IP + referer
        ---------------------------------------------------------------------------------------
             编写MapReduce程序：
                 mapper端：k为session，value：javabean
                 reduce端：
                     以step进行排序


&gt; 数据入库
    将预处理之后的数据导入到Hive仓库中相应的库和表中
&gt; 数据分析
    项目的核心内容，即根据需求开发ETL分析语句，得出各种统计结果
&gt; 工作流调度：
    简单的任务调度:
        可以使用Linux的crontab -e 来设置调度，但是其缺点是无法设置依赖
    复杂的任务调度：
        推荐使用 : azkaban [Java语言实现的，他有管理页面，配置起来比较简单]
            azkaban是由LinkedIn公司推出的一个批量工作流任务调度器，
            用于在一个工作流内以一个特定的顺序运行一组工作和流程。
            使用job配置文件简历任务之间的依赖关系，并提供了一个已使用的web用户界面维护和跟踪工作流。
            支持command、Java、Hive、pig、Hadoop，而且是基于java开发，代码结构清晰，抑郁二次开发
                azkaban的组成：
                    1.mysql服务器
                        用于存储项目、日志或者执行计划[执行周期等]之类的信息
                    2.web服务器：
                        使用jetty[开源的serverlet容器]对外提供web服务，使用户可以通过wen页面方便管理
                    3.executor服务器：
                        负责具体的工作流的提交、执行
           配置azkaban步骤：[cluster模式]
                   1.生成keystore证书文件，mv到webserver下
                   2.配置为年修改一下时区：Asia/Shanghai
                   3.配置数据库mysql
                   4.配置user admin的登录
          使用azkaban的步骤：
                  1.创建a.job文件并且已经配置b.job ，里面的type为command，中间可以配置的dependencies=b，然后command=xxxx
                    期间要把这些job打成zip包，通过web提交上去配置立刻执行还是scheduler定期执行
                      # a.job
                    type=command
                    dependencies=b
                    command=echo hahaha
                这样的话a的job任务就会等待b结束后再执行
                2.hdfs操作任务
                    command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop fs -mkdir /azaz
                  3.MapReduce操作任务
                      command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop jar hadoop-mapreduce- examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout
                  4.hive操作任务
                      执行一个命令是 command=/xx/hive -e &apos;show tables&apos;
                      执行一个文件，里面是hive sql语句， commmand=/xx/hive -f &apos;test.sql&apos;
                      Hive 脚本: test.sql
                        type=command
                        command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop jar hadoop-mapreduce- examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout
                        use default;
                        drop table aztest;
                        create table aztest(id int,name string) row format delimited fields terminated by &apos;,&apos;;
                        load data inpath &apos;/aztest/hiveinput&apos; into table aztest;
                        create table azres as select * from aztest;
                        insert overwrite directory &apos;/aztest/hiveoutput&apos; select count(1) from aztest;
        不推荐使用: ooize[虽然是Apache旗下的，但是工作流的过程是编写大量的XML文件配置，而且代码复杂度比较高，不易于二次开发]
&gt; 数据展现
    将分析所得到的数据进行数据可视化，一般通过图表[百度的echarts]进行展示
</code></pre><h2 id="模块开发-数据仓库的设计"><a href="#模块开发-数据仓库的设计" class="headerlink" title="模块开发-数据仓库的设计"></a>模块开发-数据仓库的设计</h2><pre><code>1.纬度建模
    纬度表(demension)
        通常指 按照类别、区域或者时间等等来分析，维度表数据比较固定，数据量小
     事实表
        事实表的设计是以能够正确记录历史信息为准则也就是一条一条的数据，就像是消费记录里面有product_id
        维度表的设计是以能够以合适的角度来聚合主题内容为准则  这边有product_id对应的产品信息
2.纬度建模三种模式：
    2.1 星型模式 [像星星一样]
            由一个事实表和一组维度表组成    
                比如：
                    事实表里有地域主键、时间键、部门键、产品键 对应有4个维度表相关联
    2.2 雪花模式[不常用，因为不容易维护！！！]
            在星型模式基础上，维度表还有维度表
    2.3 星座模式 [开发常用！！！]
        基于多张事实表，而且共享纬度信息
</code></pre><p>本项目数据仓库的设计：</p>
<pre><code>1.事实表的设计 ods_weblog_orgin =&gt; 对应mr清洗完之后的数据 【窄表】和【宽表或者明细表】
    窄表：对应原始数据表，字段跟数据中一一对应，但是不利于分析
---------------------------------------------------------------------------------------------------------------
        valid  remote_addr remote_user time_local  request status  body_bytes_sent http_referer  http_user_agent
        是否有效 访客IP         访客用户信息  请求时间     请求url  响应码    相应字节数       来源url         访客终端信息
---------------------------------------------------------------------------------------------------------------
    宽表：把某些融合各种信息的字段 提取出不同的信息作为新的字段
        相对于之前的窄表 字段增加了，所以叫宽表，
        比如时间戳，如果是之前的话 &apos;2018-09-09 18:09:09&apos;这种时间不利于分析，
        如果分成年，月，日，那么分析时直接group by day 或者 year 或者day 就ok了
        还有referer_url也是如此，可以拆分为host或者参数之类的

2.维度表的设计如：
    时间维度 t_dim_time: date_key year month day hour 
    访客地域纬度t_dim_area: area_ID 北京 上海 广州 深圳
    终端类型维度 t_dim_termination: uc firefox chrome safari ios android
    网站栏目纬度 t_dim_section: 进口食品、生鲜日配、时令果蔬、奶制品、
                                休闲保健、酒饮冲调茶叶、粮油副食、母婴玩具、个护清洁、家具家电
    维度表的数据一般要结合业务情况自己写脚本按照规则生成，也可以用工具来生成，方便后续关联分析
    比如事先生成时间维度表中的数据，跨度从业务需求的日期到当前的日期即可，具体根据分析粒度，
    库生成年，季，月，周，天，时等相关信息，用于分析
</code></pre><p>数据仓库三层架构：</p>
<pre><code>ods层：数据就是通过mr清洗过的数据，带有标签valid或者标识的数据
    1.创建ODS层数据表
        1.1. 原始日志数据表 :创建按照时间来分区的hive 分区表
            drop table if exists ods_weblog_origin;
            create table ods_weblog_origin
            (
            valid string,remote_addr string,remote_user string, time_local string,request string,status string, body_bytes_sent string, http_referer string, http_user_agent string
            )
            partitioned by (datestr string)
            row format delimited 
            fields terminated by &apos;\001&apos;;
        1.2. 点击流模型 PageViews表
            drop table if exists ods_click_pageviews;
            create table ods_click_pageviews
            (
            session string,remote_addr string,remote_user string, time_local string,request string,visit_step string, page_staylong string, http_referer string, http_user_agent string, body_bytes_sent string, status string
            )
            partitioned by (datestr string)
            row format delimited 
            fields terminated by &apos;\001&apos;;
        1.3. 点击流模型 Visits
            drop table if exists ods_click_visits;
            create table ods_click_visits
            (
            )
            partitioned by (datestr string)
            row format delimited 
            fields terminated by &apos;\001&apos;;
        1.4. 维度表创建(这里举例：时间，年、月、日、时)
            drop table if exists t_dim_time;
            create table t_dim_time 
            (
            date_key int,...
            )
            row format delimited 
            fields terminated by &apos;,&apos;;
        1.5 创建明细宽表 ods_weblog_detail 时间可以明细为 年月日时分秒，
            referer_url 可以明细为 host、path、query、queryid
            从预清洗后的表中得到这些数据，如果是referer_url 需要使用Hive里定义的函数：
                lateral view parse_url_tuple(正则表达式)这个方法，自动把url转换为host、path等
            如果是时间拓宽明细表的话 就是 substring
dw层：ods通过ETL处理之后得到dw层
        多维度统计PV总量：
            b) 与时间维度表关联查询
                insert into table dw_pvs_everyday select count(*) as pvs,a.month as month,a.day as day 
                from 
                (select distinct month, day from t_dim_time) a 
                join 
                ods_weblog_detail b 
                on a.month=b.month and a.day=b.day group by a.month,a.day;
            c) 按照referer维度进行统计每小时各来访 url 产生的 PV 量
                insert into table dw_pvs_referer_everyhour partition(datestr=&apos;20130918&apos;)
                select http_referer,ref_host,month,day,hour,count(1) as pv_referer_cnt
                from 
                ods_weblog_detail
                group by http_referer,ref_host,month,day,hour
                having ref_host is not null
                order by hour asc,day asc,month asc,pv_referer_cnt desc;
            d) 人均浏览量
                统计今日所有来访者平均请求的页面数。
                    insert into table dw_avgpv_user_everyday
                    select 
                    &apos;20130918&apos;,sum(b.pvs)/count(b.remote_addr) 
                    from
                    (select remote_addr,count(1) as pvs from ods_weblog_detail where datestr=&apos;20130918&apos; group by remote_addr) b; 
            e)特别重要：分组求TopN ************非常重要**********
                row_number()函数
                    row_number() over (partition by xxx order by xxx) rank
                insert into table dw_pvs_refhost_topn_everyhour partition(datestr=&apos;20130918&apos;) 
                select t.hour,t.od,t.ref_host,t.ref_host_cnts 
                from(
                select ref_host,ref_host_cnts,concat(month,day,hour) as hour,row_number() over (partition by concat(month,day,hour) order by ref_host_cnts desc
                ) as od 
                from 
                dw_pvs_refererhost_everyhour) t 
                where od&lt;=3;
            f) 受访分析(从页面的角度分析)
                热门页面统计
                    统计每日最热门的页面 top10
                        insert into table dw_hotpages_everydayselect &apos;20130918&apos;,a.request,a.request_counts 
                        from
                        (select request as request,count(request) as request_counts 
                        from 
                        ods_weblog_detail 
                        where datestr=&apos;20130918&apos; 
                        group by request having request is not null
                        ) a 
                        order by a.request_counts desc 
                        limit 10;
            g) 每小时独立访客及其产生的 pv
                insert into table dw_user_dstc_ip_h 
                select remote_addr,count(1) as pvs,concat(month,day,hour) as hour 
                from 
                ods_weblog_detail Where datestr=&apos;20130918&apos; 
                group by concat(month,day,hour),remote_addr;
                    在以上的结果基础上，统计每小时独立访客总数
                        select count(1) as dstc_ip_cnts,hour from dw_user_dstc_ip_h group by hour;
                    统计每日独立访客总数
                        select remote_addr,count(1) as counts,concat(month,day) as day 
                        from 
                        ods_weblog_detail Where datestr=&apos;20130918&apos; 
                        group by concat(month,day),remote_addr;
                    统计每月独立访客总数
                        select 
                        remote_addr,count(1) as counts,month 
                        from 
                        ods_weblog_detail 
                        group by month,remote_addr;
            h) 每日新访访客 today left join old ***************非常重要*************
                insert into table dw_user_new_d partition(datestr=&apos;20130918&apos;) 
                select tmp.day as day,tmp.today_addr as new_ip 
                from 
                ( select today.day as day,today.remote_addr as today_addr,old.ip as old_addr from (select distinct remote_addr as remote_addr,&quot;20130918&quot; as day from ods_weblog_detail where datestr=&quot;20130918&quot;) today left outer join dw_user_dsct_history old on today.remote_addr=old.ip ) tmp 
                where tmp.old_addr is null;
            注意：每日新用户追加到累计表
            i) 访客 Visit 分析(点击流模型)
                查询今日所有回头访客及其访问次数。
                    insert overwrite table dw_user_returning partition(datestr=&apos;20130918&apos;) 
                    select tmp.day,tmp.remote_addr,tmp.acc_cnt 
                    from 
                    (select &apos;20130918&apos; as day,remote_addr,count(session) as acc_cnt from ods_click_stream_visit group by remote_addr) tmp 
                    where tmp.acc_cnt&gt;1;
            j) 人均访问频次
                统计出每天所有用户访问网站的平均次数(visit)
                    select sum(pagevisits)/count(distinct remote_addr) from ods_click_stream_visit where datestr=&apos;20130918&apos;;
            k) 关键路径转化率分析(漏斗模型) -- 基于PageViews模型
                定义好业务流程中的页面标识，下例中的步骤为[模型设计]: Step1、 /item
                                                          Step2、 /category
                                                         Step3、 /index
                                                        Step4、 /order
                --查询每一步人数存入 dw_oute_numbs
                create table dw_oute_numbs as
                select &apos;step1&apos; as step,count(distinct remote_addr) and request like &apos;/item%&apos;
                union
                select &apos;step2&apos; as step,count(distinct remote_addr) and request like &apos;/category%&apos;
                union
                select &apos;step3&apos; as step,count(distinct remote_addr) and request like &apos;/order%&apos;
                union
                select &apos;step4&apos; as step,count(distinct remote_addr) and request like &apos;/index%&apos;;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                as numbs from ods_click_pageviews where datestr=&apos;20130920&apos;
                注:UNION 将多个 SELECT 语句的结果集合并为一个独立的结果集。
                ***利用级联求和自己和自己join ******************非常重要********************
                inner join
                select abs.step,abs.numbs,abs.rate as abs_ratio,rel.rate as leakage_rate
                from
                (
                select tmp.rnstep as step,tmp.rnnumbs as numbs,tmp.rnnumbs/tmp.rrnumbs as rate
                from
                (
                select rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs inner join
                dw_oute_numbs rr) tmp
                where tmp.rrstep=&apos;step1&apos;
                ) abs
                left outer join
                (
                select tmp.rrstep as step,tmp.rrnumbs/tmp.rnnumbs as rate
                from
                (
                select rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs inner join
                dw_oute_numbs rr) tmp
                where cast(substr(tmp.rnstep,5,1) as int)=cast(substr(tmp.rrstep,5,1) as int)-1
                ) rel
                on abs.step=rel.step;
                from dw_oute_numbs rn
                其中 cast(substr(tmp.rnstep,5,1) as int) 是 把字符串截取字符 然后强制转化为int
            ) 还可以按照栏目纬度和UA（user agent）纬度来分析PV,
                为了说明PV是可以从各个纬度去分析的

app层：应用层来拿数据展示
</code></pre><p>Sqoop：是Hadoop和关系数据库服务器之间传送数据的一种工具-sql到Hadoop和Hadoop到sql</p>
<pre><code>sqoop工作机制是将导入或导出命令翻译成MapReduce程序来实现，
在翻译出的MapReduce中主要是对inputformat和outputformat进行定制
1.从关系型数据库(mysql)导入到hadoop 是 DBIputformat，import命令
    bin/sqoop import \
    --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
    --password hadoop \
    --target-dir /sqoopresult \ //--target-dir 可以用来指定导出数据存放至 HDFS 的目录;
    --table emp --m 1   //m 1 表示一个map来跑
2.导入 mysql 表数据到 HIVE
    2.1 将关系型数据的表结构复制到 hive 中
    bin/sqoop create-hive-table \
    --connect jdbc:mysql://node-21:3306/sqoopdb \ --table emp_add \
    --username root \
    --password hadoop \
    --hive-table test.emp_add_sp
    2.2 以上只是复制表的结构，并没有将数据导进去，将数据导入Hive表中
        bin/sqoop import \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --table emp_add \
        --hive-table test.emp_add_sp \
        --hive-import \   ****
        --m 1
    2.3 复杂查询条件:如果不指定分隔符是默认逗号
        bin/sqoop import \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --target-dir /wherequery12 \
        --query &apos;select id,name,deg from emp WHERE --split-by id \
        --fields-terminated-by &apos;\t&apos; \
        --m 1
    2.4 下面的命令用于在 EMP 表执行增量导入:
        bin/sqoop import \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --table emp --m 1 \
        --incremental append \  ****
        --check-column id \  ****
        --last-value 1205    ****
    3. Sqoop 导出
        将数据从 HDFS 导出到 RDBMS 数据库导出前，目标表必须存在于目标数据库中。
        bin/sqoop export \
        --connect jdbc:mysql://node-21:3306/sqoopdb \ --username root \
        --password hadoop \
        --table employee \
        --export-dir /emp/emp_data
        还可以用下面命令指定输入文件的分隔符
        --input-fields-terminated-by &apos;\t&apos;
</code></pre><h4 id="工作流调度："><a href="#工作流调度：" class="headerlink" title="工作流调度："></a>工作流调度：</h4><pre><code>整个项目的数据按照处理过程，从数据采集到数据分析，再到结果数据的到处，
一系列的任务可以分割成若干个azkaban的job单元，然后由工作流调度器调度执行。
调度脚本的编写难点在于shell脚本
shell脚本大体框架如下：
    #!/bin/bash
    #set java env
    #set hadoop env
    #设置一些主类、目录等常量
    #获取时间信息
    #shell 主程序、结合流程控制(if....else)去分别执行 shell 命令。 更多工作流及 hql 脚本定义见参考资料。
    hive -e执行sql语句
</code></pre><h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><pre><code>Echarts：
    百度前端技术部开发的，基于JavaScript的数据可视化图标库，
    可以构建折线图(区域图)、柱状 图(条状图)、散点图(气泡图)、饼图(环形图)、
    K 线图、地图、力导向布局图以及和弦图， 同时支持任意维度的堆积和多图表混合展现。
javaEE中web.xml 的&lt;url-pattern&gt;/&lt;/url-pattern&gt; 是拦截所有，jsp除外

1.Mybatis example 排序问题 example.setOrderByClause(&quot;`dateStr` ASC&quot;);
查询结果便可以根据 dataStr 字段正序排列(从小到大)
如何区分不同数据仓库层的表：
2.Echarts 前端数据格式问题
注意，当异步加载数据的时候，前端一般需要的是数据格式是数组。一定要对应上。在 这里我们可以使用 Java Bean 封装数据，然后转换成 json 扔到前端，对应    上相应的字段即 可。
ObjectMapper om = new ObjectMapper(); beanJson = om.writeValueAsString(bean);
3.Controller 返回的 json @RequestMapping(value=&quot;/xxxx&quot;,produces=&quot;application/json;charset=UTF-8&quot;)
@ResponseBody    

一般使用第一种[业内默认的]
1.表之前加前缀 ods_T_access.log
                dw_T_access.log
2.针对不同的数据仓库层 建立对应的数据库 database
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-storm的架构" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/storm的架构/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/storm的架构/">
        storm
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="storm"><a href="#storm" class="headerlink" title="storm"></a>storm</h1><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/t7Rbr8aYgIO4*9TbdsjJMgxTPY8JHwAmijtuJfP2rbo!/b/dEMBAAAAAAAA&amp;bo=wAY4BAAAAAADN.g!&amp;rf=viewer_4" alt=""></p>
<h3 id="Storm架构"><a href="#Storm架构" class="headerlink" title="Storm架构"></a>Storm架构</h3><pre><code>类似于Hadoop的架构，主从(Master/Slave)
Nimbus: 主
    集群的主节点，负责任务(task)的指派和分发、资源的分配
Supervisor: 从
    可以启动多个Worker，具体几个呢？可以通过配置来指定
    一个Topo可以运行在多个Worker之上，也可以通过配置来指定
    集群的从节点，(负责干活的)，负责执行任务的具体部分
    启动和停止自己管理的Worker进程
无状态，在他们上面的信息(元数据)会存储在ZK中
Worker: 运行具体组件逻辑(Spout/Bolt)的进程
=====================分割线===================
task： 
    Spout和Bolt
    Worker中每一个Spout和Bolt的线程称为一个Task
executor： spout和bolt可能会共享一个线程
</code></pre><h5 id="提交代码到集群："><a href="#提交代码到集群：" class="headerlink" title="提交代码到集群："></a>提交代码到集群：</h5><pre><code>storm jar /home/hadoop/lib/storm-1.0.jar com.bigdata.ClusterSumStormTopology
</code></pre><h5 id="storm-其他命令的使用"><a href="#storm-其他命令的使用" class="headerlink" title="storm 其他命令的使用"></a>storm 其他命令的使用</h5><pre><code>list
    Syntax: storm list
    List the running topologies and their statuses.
如何停止作业
    kill
        Syntax: storm kill topology-name [-w wait-time-secs]
</code></pre><p>并行度</p>
<pre><code>一个worker进程执行的是一个topo的子集
一个worker进程会启动1..n个executor线程来执行一个topo的component
一个运行的topo就是由集群中多台物理机上的多个worker进程组成

executor是一个被worker进程启动的单独线程，每个executor只会运行1个topo的一个component
task是最终运行spout或者bolt代码的最小执行单元

默认：
    一个supervisor节点最多启动4个worker进程  
    每一个topo默认占用一个worker进程         
    每个worker进程会启动一个executor        
    每个executor启动一个task   

Total slots:4  
Executors: 3   但是stormUI上是 spout + bolt = 2  why 3?
隐藏的acker 导致的
</code></pre><h5 id="通过代码对并行度的理解："><a href="#通过代码对并行度的理解：" class="headerlink" title="通过代码对并行度的理解："></a>通过代码对并行度的理解：</h5><pre><code>Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes
topologyBuilder.setSpout(&quot;blue-spout&quot;, new BlueSpout(), 2); // set parallelism hint to 2
topologyBuilder.setBolt(&quot;green-bolt&quot;, new GreenBolt(), 2)
           .setNumTasks(4)
           .shuffleGrouping(&quot;blue-spout&quot;);topologyBuilder.setBolt(&quot;yellow-bolt&quot;, new YellowBolt(), 6)
           .shuffleGrouping(&quot;green-bolt&quot;);StormSubmitter.submitTopology(
    &quot;mytopology&quot;,
    conf,
    topologyBuilder.createTopology()
);

##解释：
conf.setNumWorkers(2) ---&gt;两个worker
setSpout(&quot;blue-spout&quot;, new BlueSpout(), 2)---&gt; 2个spout executor 对应默认的2个task
setBolt(&quot;green-bolt&quot;, new GreenBolt(), 2).setNumTasks(4) ---&gt; 2个executor 4个task(因为它指定了setNumTasks个数)

结果：
1个topology
2个workers  
2+2+2[2个worker 就有2个acker] = 6个executors  
2+4+2[2个worker 就有2个acker默认2个task] = 8个task
</code></pre><h5 id="修改正在运行的topology的并行度："><a href="#修改正在运行的topology的并行度：" class="headerlink" title="修改正在运行的topology的并行度："></a>修改正在运行的topology的并行度：</h5><pre><code>#Reconfigure the topology &quot;mytopology&quot; to use 5 worker processes,
#the spout &quot;blue-spout&quot; to use 3 executors 
#the bolt &quot;yellow-bolt&quot; to use 10 executors.

1.可以使用StormUI来rebalance the topology.
2.也可以使用命令行来修改：$ storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10
</code></pre><p>stream grouping分组策略内置[build-in]有8种：</p>
<pre><code>常用的有两种：
1.shuffle grouping 随机分配也就是轮询RoundRobin 这样不会造成数据倾斜
2.fileds grouping 比如按照字段hash取模分组 
如果想自定义分组策略：--&gt; 自定义分组策略需要实现CustomStreamGroup接口
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-决策树算法及模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/07/决策树算法及模型/" class="article-date">
  	<time datetime="2018-04-07T04:29:30.000Z" itemprop="datePublished">2018-04-07</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/07/决策树算法及模型/">
        决策树算法及模型
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>决策树</strong></p>
<p>决策树（Decision Tree）是一种简单但是广泛使用的分类器。通过训练数据构建决策树，可以高效的对未知的数据进行分类。决策数有两大优点：1）决策树模型可以读性好，具有描述性，有助于人工分析；2）效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。</p>
<p><strong>如何预测</strong></p>
<p>先看看下面的数据表格：<br><img src="https://s1.ax1x.com/2018/05/08/CdDKU0.png" alt="CdDKU0.png"></p>
<p>上表根据历史数据，记录已有的用户是否可以偿还债务，以及相关的信息。通过该数据，构建的决策树如下：<br><img src="https://s1.ax1x.com/2018/05/08/CdD32F.png" alt="CdD32F.png"></p>
<p>比如新来一个用户：无房产，单身，年收入55K，那么根据上面的决策树，可以预测他无法偿还债务（蓝色虚线路径）。从上面的决策树，还可以知道是否拥有房产可以很大的决定用户是否可以偿还债务，对借贷业务具有指导意义。</p>
<p><strong>基本步骤</strong></p>
<p>决策树构建的基本步骤如下：</p>
<blockquote>
<ol>
<li>开始，所有记录看作一个节点</li>
</ol>
</blockquote>
<blockquote>
<ol start="2">
<li>遍历每个变量的每一种分割方式，找到最好的分割点</li>
</ol>
</blockquote>
<blockquote>
<ol start="3">
<li>分割成两个节点N1和N2</li>
</ol>
</blockquote>
<blockquote>
<ol start="4">
<li>对N1和N2分别继续执行2-3步，直到每个节点足够“纯”为止</li>
</ol>
</blockquote>
<p><strong>决策树的变量可以有两种：</strong></p>
<p>1） 数字型（Numeric）：变量类型是整数或浮点数，如前面例子中的“年收入”。用“&gt;=”，“&gt;”,“&lt;”或“&lt;=”作为分割条件（排序后，利用已有的分割情况，可以优化分割算法的时间复杂度）。</p>
<p>2） 名称型（Nominal）：类似编程语言中的枚举类型，变量只能重有限的选项中选取，比如前面例子中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”。使用“=”来分割。</p>
<p>如何评估分割点的好坏？如果一个分割点可以将当前的所有节点分为两类，使得每一类都很“纯”，也就是同一类的记录较多，那么就是一个好分割点。比如上面的例子，“拥有房产”，可以将记录分成了两类，“是”的节点全部都可以偿还债务，非常“纯”；“否”的节点，可以偿还贷款和无法偿还贷款的人都有，不是很“纯”，但是两个节点加起来的纯度之和与原始节点的纯度之差最大，所以按照这种方法分割。构建决策树采用贪心算法，只考虑当前纯度差最大的情况作为分割点。</p>
<p><strong>量化纯度</strong></p>
<p>前面讲到，决策树是根据“纯度”来构建的，如何量化纯度呢？这里介绍三种纯度计算方法。如果记录被分为n类，每一类的比例P(i)=第i类的数目/总数目。还是拿上面的例子，10个数据中可以偿还债务的记录比例为P(1) = 7/10 = 0.7，无法偿还的为P(2) = 3/10 = 0.3，N = 2。<br><img src="https://s1.ax1x.com/2018/05/08/CdD0PK.png" alt="CdD0PK.png"></p>
<p><strong>纯度差</strong>，也称为信息增益（Information Gain），公式如下：<br><img src="https://s1.ax1x.com/2018/05/08/CdDD2D.png" alt="CdDD2D.png"></p>
<p>其中，I代表不纯度（也就是上面三个公式的任意一种），K代表分割的节点数，一般K = 2。vj表示子节点中的记录数目。上面公式实际上就是当前节点的不纯度减去子节点不纯度的加权平均数，权重由子节点记录数与当前节点记录数的比例决定。</p>
<p><strong>停止条件</strong></p>
<p>决策树的构建过程是一个递归的过程，所以需要确定停止条件，否则过程将不会结束。一种最直观的方式是当每个子节点只有一种类型的记录时停止，但是这样往往会使得树的节点过多，导致过拟合问题（Overfitting）。另一种可行的方法是当前节点中的记录数低于一个最小的阀值，那么就停止分割，将max(P(i))对应的分类作为当前叶节点的分类。</p>
<p><strong>过渡拟合</strong></p>
<p>采用上面算法生成的决策树在事件中往往会导致过滤拟合。也就是该决策树对训练数据可以得到很低的错误率，但是运用到测试数据上却得到非常高的错误率。过渡拟合的原因有以下几点：</p>
<p>噪音数据：训练数据中存在噪音数据，决策树的某些节点有噪音数据作为分割标准，导致决策树无法代表真实数据。<br>缺少代表性数据：训练数据没有包含所有具有代表性的数据，导致某一类数据无法很好的匹配，这一点可以通过观察混淆矩阵（Confusion Matrix）分析得出。<br>多重比较（Mulitple Comparition）：举个列子，股票分析师预测股票涨或跌。假设分析师都是靠随机猜测，也就是他们正确的概率是0.5。每一个人预测10次，那么预测正确的次数在8次或8次以上的概率为<br><img src="https://s1.ax1x.com/2018/05/08/CdDrxe.png" alt="CdDrxe.png"><br>，只有5%左右，比较低。但是如果50个分析师，每个人预测10次，选择至少一个人得到8次或以上的人作为代表，那么概率为<br><img src="https://s1.ax1x.com/2018/05/08/CdD6rd.png" alt="CdD6rd.png"><br>，概率十分大，随着分析师人数的增加，概率无限接近1。但是，选出来的分析师其实是打酱油的，他对未来的预测不能做任何保证。上面这个例子就是多重比较。这一情况和决策树选取分割点类似，需要在每个变量的每一个值中选取一个作为分割的代表，所以选出一个噪音分割标准的概率是很大的。</p>
<p><strong>优化方案1：修剪枝叶</strong></p>
<p>决策树过渡拟合往往是因为太过“茂盛”，也就是节点过多，所以需要裁剪（Prune Tree）枝叶。裁剪枝叶的策略对决策树正确率的影响很大。主要有两种裁剪策略。</p>
<p>前置裁剪 在构建决策树的过程时，提前停止。那么，会将切分节点的条件设置的很苛刻，导致决策树很短小。结果就是决策树无法达到最优。实践证明这中策略无法得到较好的结果。</p>
<p>后置裁剪 决策树构建好后，然后才开始裁剪。采用两种方法：1）用单一叶节点代替整个子树，叶节点的分类采用子树中最主要的分类；2）将一个字数完全替代另外一颗子树。后置裁剪有个问题就是计算效率，有些节点计算后就被裁剪了，导致有点浪费。</p>
<p><strong>优化方案2：K-Fold Cross Validation</strong></p>
<p>首先计算出整体的决策树T，叶节点个数记作N，设i属于[1,N]。对每个i，使用K-Fold Validataion方法计算决策树，并裁剪到i个节点，计算错误率，最后求出平均错误率。这样可以用具有最小错误率对应的i作为最终决策树的大小，对原始决策树进行裁剪，得到最优决策树。</p>
<p><strong>优化方案3：Random Forest</strong></p>
<p>Random Forest是用训练数据随机的计算出许多决策树，形成了一个森林。然后用这个森林对未知数据进行预测，选取投票最多的分类。实践证明，此算法的错误率得到了经一步的降低。这种方法背后的原理可以用“三个臭皮匠定一个诸葛亮”这句谚语来概括。一颗树预测正确的概率可能不高，但是集体预测正确的概率却很高。</p>
<p><strong>准确率估计</strong></p>
<p>决策树T构建好后，需要估计预测准确率。直观说明，比如N条测试数据，X预测正确的记录数，那么可以估计acc = X/N为T的准确率。但是，这样不是很科学。因为我们是通过样本估计的准确率，很有可能存在偏差。所以，比较科学的方法是估计一个准确率的区间，这里就要用到统计学中的置信区间（Confidence Interval）。</p>
<p>设T的准确率p是一个客观存在的值，X的概率分布为X ~ B(N,p)，即X遵循概率为p，次数为N的二项分布（Binomial Distribution），期望E(X) = N<em>p，方差Var(X) = N</em>p<em>(1-p)。由于当N很大时，二项分布可以近似有正太分布（Normal Distribution）计算，一般N会很大，所以X ~ N(np,n</em>p<em>(1-p))。可以算出，acc = X/N的期望E(acc) = E(X/N) = E(X)/N = p，方差Var(acc) = Var(X/N) = Var(X) / N2 = p</em>(1-p) / N，所以acc ~ N(p,p*(1-p)/N)。这样，就可以通过正太分布的置信区间的计算方式计算执行区间了。</p>
<p>正太分布的置信区间求解如下：</p>
<p>1） 将acc标准化，即<br><img src="https://s1.ax1x.com/2018/05/08/CdDRat.png" alt="CdDRat.png"></p>
<p>2） 选择置信水平α= 95%，或其他值，这取决于你需要对这个区间有多自信。一般来说，α越大，区间越大。</p>
<p>3） 求出 α/2和1-α/2对应的标准正太分布的统计量<br><img src="https://s1.ax1x.com/2018/05/08/CdD4G8.png" alt="CdD4G8.png"><br><img src="https://s1.ax1x.com/2018/05/08/CdD5RS.png" alt="CdD5RS.png"><br>（均为常量）。然后解下面关于p的不等式。acc可以有样本估计得出。即可以得到关于p的执行区间<br><img src="https://s1.ax1x.com/2018/05/08/CdDHqs.png" alt="CdDHqs.png"></p>
<p><strong>another example:</strong><br>有一天，小明无聊，对宿舍玩CS的舍友进行统计，结果刚记下四行，被舍友认为影响发挥，给踢到床下去了，让我们看看可怜的小明的记录：</p>
<blockquote>
<hr>
<h2 id="武器-子弹数量-血-行为"><a href="#武器-子弹数量-血-行为" class="headerlink" title="武器 | 子弹数量 | 血 | 行为"></a>武器 | 子弹数量 | 血 | 行为</h2><p>机枪 |    多    | 少 | 战斗<br>机枪 |    少    | 多 | 逃跑<br>小刀 |    少    | 多 | 战斗</p>
<h2 id="小刀-少-少-逃跑"><a href="#小刀-少-少-逃跑" class="headerlink" title="小刀 |    少    | 少 | 逃跑"></a>小刀 |    少    | 少 | 逃跑</h2></blockquote>
<p>为了对得起小明记录的这四条记录，我们对其进行决策树分析，从数据中看：</p>
<blockquote>
<ol>
<li>如果一个玩家子弹很多，那么即使血少他也会战斗，如果子弹少的话，即使血多，他也会逃跑隐蔽起来；</li>
<li>那我们再看子弹少的情况下，武器靠刀子，当血多时候，他还是会打一打得，但是血少，就立即逃跑隐蔽了。</li>
</ol>
</blockquote>
<p>这是我们大脑直觉上去分析，既然本文我是想聊一聊决策树，那么我们就用决策树来对小明的这些数据小试牛刀一下，顺便来慰藉一下小明（从小到大我们已经看过无数的小明了，这里再借用一下大度的小明）。</p>
<p>我们现在将数据分为两块：</p>
<blockquote>
<p>X = {武器类型，子弹数量，血}<br>Y = {行为}<br>我们建立这颗决策树的目的就是，让计算机自动去寻找最合适的映射关系，即：Y = f(X)，所谓听上去大雅的“数据挖掘”学科，干得也差不多就是这回事，X我们称之为样本，Y我们称之为结果（行为/类）。</p>
</blockquote>
<p>样本是多维的，X = {x1,x2,…xn}，如本例：X = {x1=武器类型，x2=子弹数量，x3=血}，我们就是要通过这些不同维度的观测记录数据，和应对的不同结果，找到规律（映射关系），举个例子：</p>
<blockquote>
<p>X = {天气，温度，湿度，女友约会} -&gt; Y = {是否答应兄弟下午去打篮球}<br>X = {老妈说你是胖子，老婆说你是胖子，自己上秤评估自己体重} -&gt; Y = {去办健身卡减肥}</p>
</blockquote>
<p>这样来说，X的多维不同的数据，大个比方，更像是很多大臣，那么我们就是要根据这些大臣的意见，来决策，如本例：</p>
<blockquote>
<p>左大臣：武器类型<br>中大臣：子弹数量<br>右大臣：血</p>
</blockquote>
<p>这些大臣每个人都有想法，左右着皇帝继续战斗还是撤退，但是三个也不能全信，那么我们就要根据他们的陈年老帐（训练样本）来评判他们的话语的重要性，当然，优先级高的肯定话语是有重量的，我们先提前来预览一下这个例子训练出来的决策树的样子：<br><img src="https://s1.ax1x.com/2018/05/08/CdrCL9.png" alt="CdrCL9.png"></p>
<p>这个根据小明的数据训练出来的决策树是不是和我们刚才拍脑门分析出来的结果差不多呢？看，子弹多就开打，子弹少，在看看用什么武器，如果又没子弹又用机枪，那铁定跑，如果用小刀，在掂量一下自己血厚不厚，厚则打，不厚则逃，看来决策树分析的结果还是可以的啊,接下来，我们来研究研究，计算机（这个只会重复人们给它设定的代码的家伙）是如何实现这样的分析的。</p>
<p>既然是三个大臣提意见{左大臣：武器类型，中大臣：子弹数量，右大臣：血}，那么我们要分析一下历史数据（训练数据）他们哪个话更靠谱：</p>
<p>我们先单纯的看看左大臣的历史战绩（统计训练样本）：</p>
<blockquote>
<p>机枪 -&gt; 战斗<br>机枪 -&gt; 逃跑<br>小刀 -&gt; 战斗<br>小刀 -&gt; 逃跑<br>用机枪，你战斗逃跑的概率都是50%，用刀子，你亦似打似逃！看来这个大臣立场不坚定啊！</p>
</blockquote>
<p>再看看中大臣的：</p>
<blockquote>
<p>子弹多 -&gt; 战斗<br>子弹少 -&gt; 逃跑<br>子弹少 -&gt; 战斗<br>子弹少 -&gt; 逃跑<br>用机枪，你战斗概率是100%，用刀子，你33.3%打，你66.6%撤！这位大臣似乎坚定了一些。</p>
</blockquote>
<p>再看看右大臣的：</p>
<blockquote>
<p>血少 -&gt; 战斗<br>血多 -&gt; 逃跑<br>血多 -&gt; 战斗<br>血少 -&gt; 逃跑<br>和左大臣一样，立场不坚定，50:50啊！</p>
</blockquote>
<p>这样，中大臣的话的重量就提升了，因此决策书的第一层就重用中大臣吧（中大臣变成一品大员）</p>
<p>计算机是怎么来做到这一步的呢？且让我一步一步讲：</p>
<p>决策树训练中，有一个很重要的尺子，来衡量大臣的可信度，这个尺子，就是信息论的熵(Entropy)，这个熵是何许人也，竟然朝廷大臣的可信度竟然用次来衡量，让我们对他做个自我介绍吧：<br>熵，洋名为（Entropy），乃测量信息的混乱程度为职，纵横科学界各门学术之中，为人低调，俭朴，就一个很短的公式：E = sum(-p(I)*log(p(I)))，I=1:N（N类结果，如本例两种，战斗或逃跑），当信息一致，所有样本都属于一个类别I，那么熵为0，如果样本完全随机，那么熵为1，表明这个臣子对这种状态的预测就是胡言乱语。</p>
<p>OK，熵，告诉我你对这个数据的看法：</p>
<blockquote>
<p>E(机枪) = -(1/2)Log2(1/2) - (1/2)Log(1/2) = 0.5 + 0.5 = 1<br>E(小刀) = -(1/2)Log2(1/2) - (1/2)Log(1/2) = 0.5 + 0.5 = 1<br>E(子弹多) = -(1/1)Log2(1/1) - (0/1)Log(0/1) = 0 + 0 = 0<br>E(子弹少) = -(1/3)Log2(1/3) - (2/3)Log(2/3) = 0.5283 + 0.39 = 0.9183<br>E(血多) = -(1/2)Log2(1/2) - (1/2)Log(1/2) = 0.5 + 0.5 = 1<br>E(血少) = -(1/2)Log2(1/2) - (1/2)Log(1/2) = 0.5 + 0.5 = 1</p>
</blockquote>
<p>那么我们怎么用这个熵来衡量大臣（每维数据）的可信度呢，这里还要再引出一位仁兄，其是熵的上级，他熟知熵的能力，很会用熵，他就是信息增益(Information Gain)，我们来看看这位上级是如何用熵来衡量的：<br>Gain(Sample,Action) = E(sample) - sum(|Sample(v)|/Sample * E(Sample(v)))</p>
<p>OK，Information Gain，说说你是怎么评估这个例子的三位大臣的！</p>
<blockquote>
<p>Gain(武器类型) = E(S) - (2/4)<em>E(机枪) - (2/4)</em>E(小刀) = 1 - (2/4)<em>1 - (2/4)</em>1 = 0<br>Gain(子弹数量) = E(S) - (1/4)<em>E(子弹多) - (3/4)</em>E(子弹少) = 1 - (1/4)<em>0 - (3/4)</em>0.9183 = 0.3113<br>Gain(血量) = E(S) - (2/4)<em>E(血多) - (2/4)</em>E(血少) = 1 - (2/4)<em>1 - (2/4)</em>1 = 0</p>
</blockquote>
<p>接着，计算机通过信息增益结果，选择最大的，作为一品大员<br><img src="https://s1.ax1x.com/2018/05/08/CdrEi6.png" alt="CdrEi6.png"></p>
<p>且看一品大员对子弹多的情况下料事如神（暂且不说本例样本少），但是其在子弹少的情况下，决策还是不行的，那么，再用同样的方法，再去选择二品，三品，这就是决策树的训练。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-HBASE：出自Google论文的发表" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/HBASE：出自Google论文的发表/" class="article-date">
  	<time datetime="2018-04-05T13:29:30.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/HBASE：出自Google论文的发表/">
        HBASE总结：出自Google论文的发表
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HBASE：出自Google论文的发表"><a href="#HBASE：出自Google论文的发表" class="headerlink" title="HBASE：出自Google论文的发表"></a>HBASE：出自Google论文的发表</h2><h6 id="列式存储、多版本-timestamp-的、nosql数据库"><a href="#列式存储、多版本-timestamp-的、nosql数据库" class="headerlink" title="[列式存储、多版本(timestamp)的、nosql数据库]"></a>[列式存储、多版本(timestamp)的、nosql数据库]</h6><h6 id="hbase架构图：要能手画出来"><a href="#hbase架构图：要能手画出来" class="headerlink" title="hbase架构图：要能手画出来"></a>hbase架构图：要能<strong>手画</strong>出来</h6><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/ynWG7DnR777pXkx0IGWDtEit4H6y7VkXIgYmJDtxY48!/b/dPMAAAAAAAAA&amp;bo=EAUIAwAAAAARBy4!&amp;rf=viewer_4" alt=""></p>
<h6 id="HBase没有单节点故障-因为zookeeper-，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作"><a href="#HBase没有单节点故障-因为zookeeper-，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作" class="headerlink" title="**HBase没有单节点故障(因为zookeeper)，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作"></a>**HBase没有单节点故障(因为zookeeper)，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作</h6><h4 id="HBase的功能："><a href="#HBase的功能：" class="headerlink" title="HBase的功能："></a>HBase的功能：</h4><pre><code>*hadoop数据库:
    1.存储数据 
    2.检索数据
*和RDBMS[关系型数据库]相比：
    1.海量数据：数据条目数--上亿
    2.检索的速度：准时性、秒级别
*基于hdfs： hdfs的优势
    1.数据安全性[副本机制]
    2.普通商用PC server就ok
</code></pre><h5 id="1-Table中的所有行都是按照rowkey的字典序排列"><a href="#1-Table中的所有行都是按照rowkey的字典序排列" class="headerlink" title="1.Table中的所有行都是按照rowkey的字典序排列"></a>1.Table中的所有行都是按照rowkey的字典序排列</h5><h5 id="2-Table在行的方向上分割为多个Region"><a href="#2-Table在行的方向上分割为多个Region" class="headerlink" title="2.Table在行的方向上分割为多个Region"></a>2.Table在行的方向上分割为多个Region</h5><h5 id="3-Region是按照大小分割的，每个表开始只有一个region-会有一个starkey和endkey-前毕后包-，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。"><a href="#3-Region是按照大小分割的，每个表开始只有一个region-会有一个starkey和endkey-前毕后包-，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。" class="headerlink" title="3.Region是按照大小分割的，每个表开始只有一个region[会有一个starkey和endkey,前毕后包]，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。"></a>3.Region是按照大小分割的，每个表开始只有一个region[会有一个starkey和endkey,前毕后包]，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。</h5><h5 id="4-Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上"><a href="#4-Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上" class="headerlink" title="4.Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上."></a>4.Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上.</h5><h5 id="5-Region虽然是分布式存储的最小单元，但是不是存储的最小单元。"><a href="#5-Region虽然是分布式存储的最小单元，但是不是存储的最小单元。" class="headerlink" title="5.Region虽然是分布式存储的最小单元，但是不是存储的最小单元。"></a>5.Region虽然是分布式存储的最小单元，但是不是存储的最小单元。</h5><pre><code>存储的最小单元是cell，{rowkey,column,version}
        *唯一性
        *数据没有类型，以字节码形式存储

region由一个或多个store组成，每个store保存一个column family；
每个store又由一个memStore和0至多个storeFile组成；
memStore存储在内存中，storeFile存储在hdfs上
</code></pre><h6 id="hbase-旧版本的web监听端口-60010，新版本是16010"><a href="#hbase-旧版本的web监听端口-60010，新版本是16010" class="headerlink" title="hbase 旧版本的web监听端口 60010，新版本是16010"></a>hbase 旧版本的web监听端口 60010，新版本是16010</h6><h3 id="HBASE数据写入流程："><a href="#HBASE数据写入流程：" class="headerlink" title="HBASE数据写入流程："></a>HBASE数据写入流程：</h3><pre><code>put ---&gt; cell
    step0： 先进行记录 ---&gt;HLog 【WAL（write ahead logging:日志预写功能）】 loc：hdfs
            存储的log数据类型是sequenceFile
</code></pre><p>——- WAL的作用是防止写入内存时，region挂掉，数据丢失</p>
<pre><code>step1： 写入memStore     loc：内存
step2： 当内存达到一定阈值时，写入storeFile  ---&gt; loc： hdfs
</code></pre><h3 id="HBASE-用户读取数据流程："><a href="#HBASE-用户读取数据流程：" class="headerlink" title="HBASE 用户读取数据流程："></a>HBASE 用户读取数据流程：</h3><pre><code>1)先到memStore里面去读
2)然后到BlockCache里面读 --&gt; *每个RegionServer只有一个BlockCache*
3)最后到HFile里面读取数据
4)然后对数据进行Merge --&gt; 最后返回数据集
</code></pre><h4 id="MemSore和BlockCache"><a href="#MemSore和BlockCache" class="headerlink" title="MemSore和BlockCache"></a>MemSore和BlockCache</h4><pre><code>*HBase上的RegionServer的内存分为两部分：
    1.Memstore --&gt; 用来 写
        写请求会先写入Memstore，RegionServer会给每个Region提供一个Memstore，当Memstore写满64M以后，会启动flush舒心到磁盘。
    2.BlockCache --&gt; 用来读
        读请求先到Memstore中查询数据，查不到就到BlockCache中查询，再差不读奥就会到磁盘上读，并把读的结果放入BlockCache。
        BlockCache达到上限后，会启动淘汰机制，淘汰掉最老的一批数据
*在注重读取响应时间的场景下，可以将BlockCache设置大些，Memstore设置小些，以加大缓存的命中率。
</code></pre><h3 id="数据检索的三种方式："><a href="#数据检索的三种方式：" class="headerlink" title="数据检索的三种方式："></a>数据检索的三种方式：</h3><pre><code>1.get rowkey --- 最快的方式
2.scan range --- 用的最多的方式 【一般先range扫描，然后get】
3.scan ---全表扫描，基本不用
</code></pre><h4 id="HBase中有类似于RDBMS中Database的概念"><a href="#HBase中有类似于RDBMS中Database的概念" class="headerlink" title="HBase中有类似于RDBMS中Database的概念"></a>HBase中有类似于RDBMS中Database的概念</h4><pre><code>命名空间：
    *用户自定义的表的命名空间，默认情况下 ---&gt;  default
    *系统自带的元数据的表的命名空间 ---&gt; hbase
</code></pre><h3 id="HBase数据迁移"><a href="#HBase数据迁移" class="headerlink" title="HBase数据迁移"></a>HBase数据迁移</h3><pre><code>使用 MapReduce把文件生成hdfs上的HFile，然后进行bulk load into hbase table
</code></pre><h1 id="HBase表RowKey的设计-："><a href="#HBase表RowKey的设计-：" class="headerlink" title="**HBase表RowKey的设计**："></a><strong>**</strong>HBase表RowKey的设计<strong>**</strong>：</h1><h5 id="现实环境中出现的问题："><a href="#现实环境中出现的问题：" class="headerlink" title="**现实环境中出现的问题："></a>**现实环境中出现的问题：</h5><pre><code>*默认情况下创建一张HBase表，自动会为表创建一个Region [startkey,endkey) 前毕后包
    *无论在测试环境还是生产环境中，创建完表以后会往表中导入大量数据
        步骤：
            file/datas -&gt; HFile -&gt; bulk load into hbase table
        此时Region只有一个，而region是被RegionServer管理的，
        当导入数据量慢慢增大后Region就会被split成两个Region，
        但是此时RegionServer很可能就会挂掉，此时就会出现问题...

解决方案[举例]：
    创建表时，多创建一些Region(依据表的rowkey进行设计 + 结合业务)
        比如说：有5个region，他们被多个RegionServer管理
            再插入数据时，会向5个Region中分别插入数据，这样就均衡了
</code></pre><h5 id="具体解决方案："><a href="#具体解决方案：" class="headerlink" title="**具体解决方案："></a>**具体解决方案：</h5><p>表的RowKey设计中：</p>
<pre><code>**如何在海量数据中，查询到我想要的数据???
    核心思想：
        1.依据RowKey查询最快
        2.对RowKey进行范围查询range
        3.前缀查询  
            比如：startkey:15221467820_20180409000053 和 endkey:15221467828_20180419000053
            这时只会比较15221467820 和 15221467828 而后面的_2018.. 则不会进行匹配    
</code></pre><p>解决方法：</p>
<pre><code>a).HBase的预分区：
    Region的划分依赖于RowKey，预先预估一些RowKey(年月日时分秒)[最常用的就是这两种，其他的方法rowkey设计都不能自定义]
        1. create &apos;tb1&apos;,&apos;cf1&apos;,splits =&gt; [&apos;20180409000000&apos;,&apos;20180419000000&apos;,&apos;20180429000000&apos;]
        2. create &apos;tb1&apos;,&apos;cf1&apos;,splits_file =&gt; &apos;/root/data/logs-split.txt&apos;
           [这种情况就是把分段的rowkey写入了文件里]
    这样就分为了4个Region，因为它会把头和尾也算进去
b).根据业务来对RowKey的设计：
        举例：移动运营商电话呼叫查询详单：
            RowKey: 
                phone + time 
                15221467820_20180409010000
            infor: 
                上海  主叫   152216888888   30        国内通话 0.00
                area active opposite_phone talk_time model price 
        依据查询条件：
            phone + (start_time - end_time)
        代码：
            scan 
                startrow
                    15221467820_20180409000000
                stoprow
                    15221467820_20180419000000
    保证了实时性
</code></pre><h3 id="HBase数据存储二级索引-索引表-的设计以及数据同步解决方案："><a href="#HBase数据存储二级索引-索引表-的设计以及数据同步解决方案：" class="headerlink" title="HBase数据存储二级索引[索引表]的设计以及数据同步解决方案："></a>HBase数据存储二级索引[索引表]的设计以及数据同步解决方案：</h3><p>基于以上的移动运营商电话呼叫查询详单：</p>
<pre><code>二级索引表：
    RowKey:
        opposite_phone
    columnFamily对应的colume:
        主表的RowKey
主表和索引表的同步：
    &gt;&gt; phoenix 
            &gt;&gt; jdbc 方式才能同步
    &gt;&gt;先在solr里创建索引表
     solr   //在solr中创建索引表
            国外有个框架交 lily
                cloudera search 封装了lily，只要配置下cloudera
                cloudera search会在主表插入数据时，自动更新solr里的索引表
</code></pre><h2 id="HBase之-表的属性"><a href="#HBase之-表的属性" class="headerlink" title="HBase之 表的属性:"></a>HBase之 表的属性:</h2><h3 id="面试会问到：-每个RegionServer只有一个HLog和一个BlockCache"><a href="#面试会问到：-每个RegionServer只有一个HLog和一个BlockCache" class="headerlink" title="面试会问到：**每个RegionServer只有一个HLog和一个BlockCache"></a>面试会问到：**每个RegionServer只有一个HLog和一个BlockCache</h3><pre><code>1.HBase的表的压缩[HFile compression]--表的属性:
**在企业中通常使用snappy格式进行压缩，可以减少容量的存储，和减少io流，提高传输效率
    跟hive一样，需要一些编译jar包，然后放到lib的native里
    [这里使用软连接 ln -s /xx/xx /xx  具体看官方文档]，还有一些参数的配置，才能使用
        创建表的时候：
            create &apos;user&apos;,{NAME=&apos;cf1&apos;,COMPRESSION=&gt;&apos;SNAPPY&apos;}
2. HBase 标的属性：IN_MEMORY=&gt;&apos;false&apos;、BLOCKCACHE=&gt;&apos;true&apos;
        HBase为什么查询这么快 就是因为这些表的属性的存储机制！！
        通常我们自定义创建的表的相应列簇下的IN_MEMORY属性都是false!
        通过describe &apos;hbase:meta&apos;可以查询到 IN_MEMORY=&gt;&apos;true&apos; BLOCKCACHE =&gt; &apos;true&apos;,
        意为meta元数据存放于blockcache的inmemory内存中
            IN_MEMORY队列存放HBase的meta表元数据信息，因为meta存储了用户表的
            通常 BLOCKCACHE 这个属性的设置要慎用，比如我们查询后的数据后续还会用到这时可以设置为true
            如果后续用不到这些数据就要设置此属性为false
</code></pre><h4 id="HBase的Cache分等级："><a href="#HBase的Cache分等级：" class="headerlink" title="HBase的Cache分等级："></a>HBase的Cache分等级：</h4><pre><code>默认情况下：整个BlockCache的内存分配 
1.IN_MEMORY 1/4    in_memory 用于存储hbase的meta元数据信息    2.Single 1/4   single就是用的次数较少的数据
3.Muti    1/2          muti就是用的次数较多的数据
如果BlockCache的内存到达阈值，会先清理single然后清理muti，in_memory的内存不会被清理!
</code></pre><h3 id="HBase表的compaction"><a href="#HBase表的compaction" class="headerlink" title="HBase表的compaction"></a>HBase表的compaction</h3><h6 id="随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种："><a href="#随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种：" class="headerlink" title="随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种："></a>随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种：</h6><pre><code>1.minor compaction
    Minor Compaction是指选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，
    在这个过程中不会处理已经Deleted或Expired的Cell。一次Minor Compaction的结果是更少并且更大的StoreFile。
2.major compaction
    Major Compaction是指将所有的StoreFile合并成一个StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL过期数据、版本号超过设定版本号的数据。另外，一般情况下，Major Compaction时间会持续比较长，会阻塞数据的吸入和查询!!!，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此线上业务都会将关闭自动触发Major Compaction功能，改为手动在业务低峰期触发。
</code></pre><h4 id="HBase和Hive集成："><a href="#HBase和Hive集成：" class="headerlink" title="HBase和Hive集成："></a>HBase和Hive集成：</h4><pre><code>此时实质上Hive就是HBase的一个客户端!
应用场景：
    日志文件 --&gt; hive --&gt; hive-hbase-table --&gt; insert .. select ...
</code></pre><h4 id="电商订单查询之HBase："><a href="#电商订单查询之HBase：" class="headerlink" title="电商订单查询之HBase："></a>电商订单查询之HBase：</h4><pre><code>1.用户下单后--&gt;存到RDBMS[关系型数据库]中 即未完成的订单
2.订单完成后--&gt;数据迁移到HBase中

查询比如三个月内的订单：
 1.主表[订单显示表]：
    rowkey: userid_orderCreateTime_orderId  
    cfname: orderInfor
    [本来userid+orderCreateTime就可以保证唯一性，加orderId的原因是为了二次查询]
 2.订单详情表:
     rowkey: orderId_orderItemId
    cfname: itemInfor
 3.索引表：
     比如用户输入订单号：我们后台Redis或者MongoDB里会有相对应的订单编码
     获取到订单编码然后到索引表中去查
         索引表的rowkey：订单编号 column: 主表的rowkey
         就可以到hbase主表中查询订单
</code></pre><h5 id="补充文件类型-tsv-tab-csv-comma-："><a href="#补充文件类型-tsv-tab-csv-comma-：" class="headerlink" title="补充文件类型 .tsv[tab] .csv[comma]："></a>补充文件类型 .tsv[tab] .csv[comma]：</h5><pre><code>user.tsv tsv后缀格式的文件：每行数据以tab 制表符分割的 
user.csv csv后缀格式的文件：每行数据以逗号分割的
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-电商分析指标" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/电商分析指标/" class="article-date">
  	<time datetime="2018-04-05T13:29:30.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/电商分析指标/">
        电商常见分析指标
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>电商常见分析指标<br>信息流、物流和资金流三大平台是电子商务的三个最为重要的平台。而电子商务信息系统最核心的能力是大数据能力，包括大数据处理、数据分析和数据挖掘能力。无论是电商平台（如淘宝）还是在电商平台上销售产品的卖家，都需要掌握大数据分析的能力。越成熟的电商平台，越需要以通过大数据能力驱动电子商务运营的精细化，更好的提升运营效果，提升业绩。构建系统的电子商务数据分析指标体系是数据电商精细化运营的重要前提，本文将重点介绍电商数据分析指标体系。<br>电商数据分析指标体系分为八大类指标，包括总体运营指标、网站流量累指标、销售转化指标、客户价值指标、商品及供应链指标、营销活动指标、风险控制指标和市场竞争指标。不同类别指标对应电商运营的不同环节，如网站流量指标对应的是网站运营环节，销售转化、客户价值和营销活动指标对应的是电商销售环节。<br>1.电商总体运营指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaO3t.png" alt="CYaO3t.png"></p>
<p>电商总体运营整体指标主要面向的人群电商运营的高层，通过总体运营指标评估电商运营的整体效果。电商总体运营整体指标包括四方面的指标：</p>
<p>（1）流量类指标</p>
<p>独立访客数（UV），指访问电商网站的不重复用户数。对于PC网站，统计系统会在每个访问网站的用户浏览器上“种”一个cookie来标记这个用户，这样每当被标记cookie的用户访问网站时，统计系统都会识别到此用户。在一定统计周期内如（一天）统计系统会利用消重技术，对同一cookie在一天内多次访问网站的用户仅记录为一个用户。而在移动终端区分独立用户的方式则是按独立设备计算独立用户。</p>
<p>页面访问数（PV），即页面浏览量，用户每一次对电商网站或着移动电商应用中的每个网页访问均被记录一次，用户对同一页面的多次访问，访问量累计。<br>人均页面访问数，即页面访问数（PV）／独立访客数，该指标反映的是网站访问粘性。</p>
<p>（2）订单产生效率指标</p>
<p>总订单数量，即访客完成网上下单的订单数之和。<br>访问到下单的转化率，即电商网站下单的次数与访问该网站的次数之比。</p>
<p>（3）总体销售业绩指标</p>
<p>网站成交额（GMV），电商成交金额，即只要网民下单，生成订单号，便可以计算在GMV里面。<br>销售金额。销售金额是货品出售的金额总额。</p>
<p>注：无论这个订单最终是否成交，有些订单下单未付款或取消，都算GMV，销售金额一般只指实际成交金额，所以，GMV的数字一般比销售金额大。</p>
<p>客单价，即订单金额与订单数量的比值。</p>
<p>（4）整体指标</p>
<p>销售毛利，是销售收入与成本的差值。销售毛利中只扣除了商品原始成本，不扣除没有计入成本的期间费用（管理费用、财务费用、营业费用）。<br>毛利率，是衡量电商企业盈利能力的指标，是销售毛利与销售收入的比值。如京东的2014年毛利率连续四个季度稳步上升，从第一季度的10.0％上升至第四季度的12.7％，体现出京东盈利能力的提升。</p>
<p>2.网站流量指标<br><img src="https://s1.ax1x.com/2018/05/03/CYa6AJ.png" alt="CYa6AJ.png"></p>
<p>（1）流量规模类指标<br>常用的流量规模类指标包括独立访客数和页面访问数，相应的指标定义在前文（电商总体运营指标）已经描述，在此不在赘述</p>
<p>（2）流量成本累指标<br>单位访客获取成本。该指标指在流量推广中，广告活动产生的投放费用与广告活动带来的独立访客数的比值。单位访客成本最好与平均每个访客带来的收入以及这些访客带来的转化率进行关联分析。若单位访客成本上升，但访客转化率和单位访客收入不变或下降，则很可能流量推广出现问题，尤其要关注渠道推广的作弊问题。</p>
<p>（3）流量质量类指标<br>跳出率（Bounce Rate）也被称为蹦失率，为浏览单页即退出的次数/该页访问次数，跳出率只能衡量该页做为着陆页面（LandingPage）的访问。如果花钱做推广，着落页的跳出率高，很可能是因为推广渠道选择出现失误，推广渠道目标人群和和被推广网站到目标人群不够匹配，导致大部分访客来了访问一次就离开。</p>
<p>页面访问时长。页访问时长是指单个页面被访问的时间。并不是页面访问时长越长越好，要视情况而定。对于电商网站，页面访问时间要结合转化率来看，如果页面访问时间长，但转化率低，则页面体验出现问题的可能性很大。</p>
<p>人均页面浏览量。人均页面浏览量是指在统计周期内，平均每个访客所浏览的页面量。人均页面浏览量反应的是网站的粘性。</p>
<p>（4）会员类指标</p>
<p>注册会员数。指一定统计周期内的注册会员数量。<br>活跃会员数。活跃会员数，指在一定时期内有消费或登录行为的会员总数。<br>活跃会员率。即活跃会员占注册会员总数的比重。<br>会员复购率。指在统计周期内产生二次及二次以上购买的会员占购买会员的总数。<br>会员平均购买次数。指在统计周期内每个会员平均购买的次数，即订单总数/购买用户总数。会员复购率高的电商网站平均购买次数也高。<br>会员回购率。指上一期末活跃会员在下一期时间内有购买行为的会员比率。<br>会员留存率。会员在某段时间内开始访问你的网站，经过一段时间后，仍然会继续访问你的网站就被认作是留存，这部分会员占当时新增会员的比例就是新会员留存率，这种留存的计算方法是按照活跃来计算，另外一种计算留存的方法是按消费来计算，即某段的新增消费用户在往后一段时间时间周期（时间周期可以是日、周、月、季度和半年度）还继续消费的会员比率。留存率一般看新会员留存率，当然也可以看活跃会员留存。留存率反应的是电商留住会员的能力。</p>
<p>3.网站销售（转化率）类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaghR.png" alt="CYaghR.png"></p>
<p>（1）购物车类指标<br>基础类指标，包括一定统计周期内加入购物车次数、加入购物车买家数、加入购物车买家数以及加入购物车商品数。<br>转化类指标，主要是购物车支付转化率，即一定周期内加入购物车商品支付买家数与加入购物车购买家数的比值。</p>
<p>（2）下单类指标<br>基础类指标，包括一定统计周期内的下单笔数、下单金额以及下单买家数。<br>转化类指标，主要是浏览下单转化率，即下单买家数与网站访客数（UV）的比值。</p>
<p>（3）支付类指标<br>基础统计类指标，包括一定统计周期内支付金额、支付买家数和支付商品数。<br>转化类指标。包括浏览-支付买家转化率（支付买家数/网站访客数）、下单-支付金额转化率（支付金额/下单金额）、下单-支付买家数转化率（支付买家数/下单买家数）和下单-支付时长（下单时间到支付时间的差值）。</p>
<p>4.客户价值类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaW1x.png" alt="CYaW1x.png"></p>
<p>客户指标。常见客户指标包括一定统计周期内的累计购买客户数和客单价。客单价是指每一个客户平均购买商品的金额，也即是平均交易金额，即成交金额与成交用户数的比值。</p>
<p>新客户指标。常见新客户指标包括一定统计周期内的新客户数量、新客户获取成本和新客户客单价。其中，新客户客单价是指第一次在店铺中产生消费行为的客户所产生交易额与新客户数量的比值。影响新客户客单价的因素除了与推广渠道的质量有关系，还与电商店铺活动以及关联销售有关。</p>
<p>老客户指标。常见老客户指标包括消费频率、最近一次购买时间、消费金额和重复购买率。消费频率是指客户在一定期间内所购买的次数；最近一次购买时间表示客户最近一次购买的时间离现在有多远；客户消费金额指客户在最近一段时间内购买的金额。消费频率越高，最近一次购买时间离现在越近，消费金额越高的客户越有价值。重复购买率则指消费者对该品牌产品或者服务的重复购买次数，重复购买率越多，则反应出消费者对品牌的忠诚度就越高，反之则越低。重复购买率可以按两种口径来统计：第一种，从客户数角度，重复购买率指在一定周期内下单次数在两次及两次以上的人数与总下单人数之比，如在一个月内，有100个客户成交，其中有20个是购买两次及以上，则重复购买率为20%；第二种，按交易计算，即重复购买交易次数与总交易次数的比值，如某月内，一共产生了100笔交易，其中有20个人有了二次购买，这20人中的10个人又有了三次购买，则重复购买次数为30次，重复购买率为30%。</p>
<p>5.商品类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYafc6.png" alt="CYafc6.png"></p>
<p>产品总数指标。包括SKU、SPU和在线SPU。SKU是物理上不可分割的最小存货单位。SPU即Standard Product Unit （标准化产品单元），SPU是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。通俗点讲，属性值、特性相同的商品就可以称为一个SPU。如iphone5S是一个SPU，而iPhone 5S配置为16G版、4G手机、颜色为金色、网络类型为TD-LTE/TD-SCDMA/WCDMA/GSM则是一个SKU。在线SPU则是在线商品的SPU数。</p>
<p>产品优势性指标。主要是独家产品的收入占比，即独家销售的产品收入占总销售收入的比例。</p>
<p>品牌存量指标。包括品牌数和在线品牌数指标。品牌数指商品的品牌总数量。在线品牌数则指在线商品的品牌总数量。</p>
<p>上架。包括上架商品SKU数、上架商品SPU数、上架在线SPU数、上架商品数和上架在线商品数。</p>
<p>首发。包括首次上架商品数和首次上架在线商品数。</p>
<p>6.市场营销活动指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaIBD.png" alt="CYaIBD.png"></p>
<p>市场营销活动指标。包括新增访问人数、新增注册人数、总访问次数、订单数量、下单转化率以及ROI。其中，下单转化率是指活动期间，某活动所带来的下单的次数与访问该活动的次数之比。投资回报率（ROI）是指，某一活动期间，产生的交易金额与活动投放成本金额的比值。</p>
<p>广告投放指标。包括新增访问人数、新增注册人数、总访问次数、订单数量、UV订单转化率、广告投资回报率。其中，下单转化率是指某广告所带来的下单的次数与访问该活动的次数之比。投资回报率（ROI）是指，某广告产生的交易金额与广告投放成本金额的比值。</p>
<p>7.风控类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYa7AH.png" alt="CYa7AH.png"></p>
<p>买家评价指标。包括买家评价数，买家评价卖家数、买家评价上传图片数、买家评价率、买家好评率以及卖家差评率。其中，买家评价率是指某段时间参与评价的卖家与该时间段买家数量的比值，是反映用户对评价的参与度，电商网站目前都在积极引导用户评价，以作为其他买家购物时候的参考。买家好评率指某段时间内好评的买家数量与该时间段买家数量的比值。同样，买家差评率指某段时间内差评的买家数量与该时间段买家数量的比值。尤其是买家差评率，是非常值得关注的指标，需要监控起来，一旦发现买家差评率在加速上升，一定要提高警惕，分析引起差评率上升的原因，及时改进。</p>
<p>买家投诉类指标。包括发起投诉（或申诉），撤销投诉（或申诉），投诉率（买家投诉人数占买家数量的比例）等。投诉量和投诉率都需要及时监控，以发现问题，及时优化。</p>
<p>8.市场竞争类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaHNd.png" alt="CYaHNd.png"></p>
<p>市场份额相关指标，包括市场占有率、市场扩大率和用户份额。市场占有率指电商网站交易额占同期所有同类型电商网站整体交易额的比重；市场扩大率指购物网站占有率较上一个统计周期增长的百分比；用户份额指购物网站独立访问用户数占同期所有B2C购物网站合计独立访问用户数的比例。</p>
<p>网站排名，包括交易额排名和流量排名。交易额排名指电商网站交易额在所有同类电商网站中的排名；流量排名指电商网站独立访客数量在所有同类电商网站中的排名。</p>
<p>总之，本文介绍了电商数据分析的基础指标体系，涵盖了流量、销售转化率、客户价值、商品类目、营销活动、风控和市场竞争指标，这些指标都需要系统化的进行统计和监控，才能更好的发现电商运营健康度的问题，以更好及时改进和优化，提升电商收入。如销售转化率，其本质上是一个漏斗模型，如从网站首页到最终购买各个阶段的转化率的监控和分析是网站运营健康度很重要的分析方向。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-DMP" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/DMP/" class="article-date">
  	<time datetime="2018-04-05T13:29:28.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/DMP/">
        dmp介绍
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>dmp</p>
<p>（数据管理平台）<br> 编辑<br>DMP(Data Management Platform)数据管理平台，是把分散的多方数据进行整合纳入统一的技术平台，并对这些数据进行标准化和细分，让用户可以把这些细分结果推向现有的互动营销环境里的平台。<br>中文名<br>数据管理平台<br>外文名<br>Data Management Platform<br>释    义<br>数据进行整合纳入统一的技术平台<br>简    称<br>DMP<br>目录<br>.    1 作用<br>.    2 类型<br>作用<br>编辑<br>•能快速查询、反馈和快速呈现结果<br>•能帮助客户更快进入到市场周期中<br>•能促成企业用户和合作伙伴之间的合作<br>•能深入的预测分析并作出反应<br>•能带来各方面的竞争优势<br>•能降低信息获取及人力成本<br>类型<br>编辑<br>1、结构化的数据，比如Oracle数据库数据等；<br>　　2、非结构化的数据，比如各种文件、图像、音频等数据，等等。<br>　　结构化数据（即数据库数据）在当今的信息系统中占据最核心、最重要的位置。结构化数据从产生―使用―消亡这样一个完整过程的管理，就是数据生命周期管理（所谓的ILM）。<br>核心元素包括：<br>•数据整合及标准化能力：采用统一化的方式，将各方数据吸纳整合。<br>•数据细分管理能力：创建出独一无二、有意义的客户细分，进行有效营销活动。<br>•功能健全的数据标签：提供数据标签灵活性，便于营销活动的使用。<br>•自助式的用户界面：基于网页web界面或其他集成方案直接获取数据工具，功能和几种形式报表和分析。<br>•相关渠道环境的连接：跟相关渠道的集成，包含网站端、展示广告、电子邮件以及搜索和视频，让营销者能找到、定位和提供细分群体相关高度的营销信息。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-文本相似度怎么比较---TF-IDF算法及应用" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/文本相似度怎么比较---TF-IDF算法及应用/" class="article-date">
  	<time datetime="2018-04-05T11:29:30.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/文本相似度怎么比较---TF-IDF算法及应用/">
        文本相似度比较-----TF-IDF算法及应用
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>【算法】TF-IDF算法及应用</p>
<p>小编邀请您，先思考：</p>
<blockquote>
<p>1 如何计算TF-IDF？<br>2 TF-IDF有什么应用？<br>3 如何提取文本的关键词和摘要？</p>
</blockquote>
<p>有一篇很长的文章，我要用计算机提取它的关键词（Automatic Keyphrase extraction），完全不加以人工干预，请问怎样才能正确做到？</p>
<p>这个问题涉及到数据挖掘、文本处理、信息检索等很多计算机前沿领域，但是出乎意料的是，有一个非常简单的经典算法，可以给出令人相当满意的结果。它简单到都不需要高等数学，普通人只用10分钟就可以理解，这就是我今天想要介绍的TF-IDF（<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a> ）算法。</p>
<p>让我们从一个实例开始讲起。假定现在有一篇长文《中国的蜜蜂养殖》，我们准备用计算机提取它的关键词。</p>
<p>一个容易想到的思路，就是找到出现次数最多的词。如果某个词很重要，它应该在这篇文章中多次出现。于是，我们进行”词频”（Term Frequency，缩写为TF）统计。</p>
<p>结果你肯定猜到了，出现次数最多的词是—-“的”、”是”、”在”—-这一类最常用的词。它们叫做”停用词”（ <a href="http://baike.baidu.com/view/3784680.htm" target="_blank" rel="noopener">http://baike.baidu.com/view/3784680.htm</a> ）（stop words），表示对找到结果毫无帮助、必须过滤掉的词。</p>
<p>假设我们把它们都过滤掉了，只考虑剩下的有实际意义的词。这样又会遇到了另一个问题，我们可能发现”中国”、”蜜蜂”、”养殖”这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？</p>
<p>显然不是这样。因为”中国”是很常见的词，相对而言，”蜜蜂”和”养殖”不那么常见。如果这三个词在一篇文章的出现次数一样多，有理由认为，”蜜蜂”和”养殖”的重要程度要大于”中国”，也就是说，在关键词排序上面，”蜜蜂”和”养殖”应该排在”中国”的前面。</p>
<p>所以，我们需要一个重要性调整系数，衡量一个词是不是常见词。<strong>如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。</strong></p>
<p>用统计学语言表达，就是在词频的基础上，要对每个词分配一个”重要性”权重。最常见的词（”的”、”是”、”在”）给予最小的权重，较常见的词（”中国”）给予较小的权重，较少见的词（”蜜蜂”、”养殖”）给予较大的权重。这个权重叫做”逆文档频率”（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。</p>
<p><strong>知道了”词频”（TF）和”逆文档频率”（IDF）以后，将这两个值相乘，就得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。</strong></p>
<p>下面就是这个算法的细节。<br><strong>第一步，计算词频。</strong><br><img src="https://s1.ax1x.com/2018/05/04/Ct7BC9.png" alt="Ct7BC9.png"><br>考虑到文章有长短之分，为了便于不同文章的比较，进行”词频”标准化。<br><img src="https://s1.ax1x.com/2018/05/04/Ct7cDK.png" alt="Ct7cDK.png"><br>或者<br><img src="https://s1.ax1x.com/2018/05/04/Ct7RED.png" alt="Ct7RED.png"></p>
<p><strong>第二步，计算逆文档频率。</strong><br>这时，需要一个语料库（corpus），用来模拟语言的使用环境。<br><img src="https://s1.ax1x.com/2018/05/04/Ct74Cd.png" alt="Ct74Cd.png"><br>如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。</p>
<p><strong>第三步，计算TF-IDF。</strong><br><img src="https://s1.ax1x.com/2018/05/04/CtHqQ1.png" alt="CtHqQ1.png"><br><strong>可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。</strong>所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。</p>
<p>还是以《中国的蜜蜂养殖》为例，假定该文长度为1000个词，”中国”、”蜜蜂”、”养殖”各出现20次，则这三个词的”词频”（TF）都为0.02。然后，搜索Google发现，包含”的”字的网页共有250亿张，假定这就是中文网页总数。包含”中国”的网页共有62.3亿张，包含”蜜蜂”的网页为0.484亿张，包含”养殖”的网页为0.973亿张。则它们的逆文档频率（IDF）和TF-IDF如下：<br><img src="https://s1.ax1x.com/2018/05/04/CtbUfJ.png" alt="CtbUfJ.png"><br>从上表可见，”蜜蜂”的TF-IDF值最高，”养殖”其次，”中国”最低。（如果还计算”的”字的TF-IDF，那将是一个极其接近0的值。）所以，如果只选择一个词，”蜜蜂”就是这篇文章的关键词。</p>
<p>除了自动提取关键词，TF-IDF算法还可以用于许多别的地方。比如，信息检索时，对于每个文档，都可以分别计算一组搜索词（”中国”、”蜜蜂”、”养殖”）的TF-IDF，将它们相加，就可以得到整个文档的TF-IDF。这个值最高的文档就是与搜索词最相关的文档。</p>
<p>TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以”词频”衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。）</p>
<p><strong>找出相似文章</strong><br>我们再来研究另一个相关的问题。有些时候，除了找到关键词，我们还希望找到与原文章相似的其他文章。比如，”Google新闻”在主新闻下方，还提供多条相似的新闻。<br><img src="https://s1.ax1x.com/2018/05/04/Ctbs0K.jpg" alt="Ctbs0K.jpg"></p>
<p>为了找出相似的文章，需要用到”余弦相似性” （ <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Cosine_similarity</a> ）（cosine similiarity）。下面，我举一个例子来说明，什么是”余弦相似性”。</p>
<p>为了简单起见，我们先从句子着手。</p>
<blockquote>
<p>   句子A：我喜欢看电视，不喜欢看电影。<br>   句子B：我不喜欢看电视，也不喜欢看电影。</p>
</blockquote>
<p>请问怎样才能计算上面两句话的相似程度？<br>基本思路是：如果这两句话的用词越相似，它们的内容就应该越相似。因此，可以从词频入手，计算它们的相似程度。</p>
<p><strong>第一步，分词。</strong></p>
<blockquote>
<p>   句子A：我/喜欢/看/电视，不/喜欢/看/电影。<br>   句子B：我/不/喜欢/看/电视，也/不/喜欢/看/电影。</p>
</blockquote>
<p><strong>第二步，列出所有的词。</strong></p>
<blockquote>
<p>   我，喜欢，看，电视，电影，不，也。</p>
</blockquote>
<p><strong>第三步，计算词频。</strong></p>
<blockquote>
<p>   句子A：我 1，喜欢 2，看 2，电视 1，电影 1，不 1，也 0。<br>   句子B：我 1，喜欢 2，看 2，电视 1，电影 1，不 2，也 1。</p>
</blockquote>
<p><strong>第四步，写出词频向量。</strong></p>
<blockquote>
<p>   句子A：[1, 2, 2, 1, 1, 1, 0]<br>   句子B：[1, 2, 2, 1, 1, 2, 1]</p>
</blockquote>
<p>到这里，问题就变成了如何计算这两个向量的相似程度。<br>我们可以把它们想象成空间中的两条线段，都是从原点（[0, 0, …]）出发，指向不同的方向。两条线段之间形成一个夹角，如果夹角为0度，意味着方向相同、线段重合；如果夹角为90度，意味着形成直角，方向完全不相似；如果夹角为180度，意味着方向正好相反。<strong>因此，我们可以通过夹角的大小，来判断向量的相似程度。夹角越小，就代表越相似。</strong><br><img src="https://s1.ax1x.com/2018/05/04/CtbckD.png" alt="CtbckD.png"><br>以二维空间为例，上图的a和b是两个向量，我们要计算它们的夹角θ。余弦定理告诉我们，可以用下面的公式求得：<br><img src="https://s1.ax1x.com/2018/05/04/Ctb2fH.png" alt="Ctb2fH.png"><br><img src="https://s1.ax1x.com/2018/05/04/Ctbf1A.png" alt="Ctbf1A.png"><br>假定a向量是[x1, y1]，b向量是[x2, y2]，那么可以将余弦定理改写成下面的形式：<br><img src="https://s1.ax1x.com/2018/05/04/CtbInP.png" alt="CtbInP.png"><br><img src="https://s1.ax1x.com/2018/05/04/Ctbo0f.png" alt="Ctbo0f.png"><br>数学家已经证明，余弦的这种计算方法对n维向量也成立。假定A和B是两个n维向量，A是 [A1, A2, …, An] ，B是 [B1, B2, …, Bn] ，则A与B的夹角θ的余弦等于：<br><img src="https://s1.ax1x.com/2018/05/04/CtbvXq.png" alt="CtbvXq.png"><br>使用这个公式，我们就可以得到，句子A与句子B的夹角的余弦。<br><img src="https://s1.ax1x.com/2018/05/04/CtqCAU.png" alt="CtqCAU.png"><br><strong>余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫”余弦相似性”。</strong>所以，上面的句子A和句子B是很相似的，事实上它们的夹角大约为20.3度。<br>由此，我们就得到了”找出相似文章”的一种算法：<br>　　（1）使用TF-IDF算法，找出两篇文章的关键词；<br>　　（2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；<br>　　（3）生成两篇文章各自的词频向量；<br>　　（4）计算两个向量的余弦相似度，值越大就表示越相似。<br>“余弦相似度”是一种非常有用的算法，只要是计算两个向量的相似程度，都可以采用它。</p>
<p><strong>自动摘要</strong><br>有时候，很简单的数学方法，就可以完成很复杂的任务。<br>前两部分就是很好的例子。仅仅依靠统计词频，就能找出关键词和相似文章。虽然它们算不上效果最好的方法，但肯定是最简便易行的方法。<br>接下来讨论如何通过词频，对文章进行自动摘要（Automatic summarization）。</p>
<p>如果能从3000字的文章，提炼出150字的摘要，就可以为读者节省大量阅读时间。由人完成的摘要叫”人工摘要”，由机器完成的就叫”自动摘要”。许多网站都需要它，比如论文网站、新闻网站、搜索引擎等等。2007年，美国学者的论文《A Survey on Automatic Text Summarization》（Dipanjan Das, Andre F.T. Martins, 2007）总结了目前的自动摘要算法。其中，很重要的一种就是词频统计。</p>
<p>这种方法最早出自1958年的IBM公司科学家H.P. Luhn的论文《The Automatic Creation of Literature Abstracts》。<br>Luhn博士认为，文章的信息都包含在句子中，有些句子包含的信息多，有些句子包含的信息少。”自动摘要”就是要找出那些包含信息最多的句子。<br>句子的信息量用”关键词”来衡量。如果包含的关键词越多，就说明这个句子越重要。Luhn提出用”簇”（cluster）表示关键词的聚集。所谓”簇”就是包含多个关键词的句子片段。<br><img src="https://s1.ax1x.com/2018/05/04/CtqPNF.png" alt="CtqPNF.png"><br>上图就是Luhn原始论文的插图，被框起来的部分就是一个”簇”。只要关键词之间的距离小于”门槛值”，它们就被认为处于同一个簇之中。Luhn建议的门槛值是4或5。也就是说，如果两个关键词之间有5个以上的其他词，就可以把这两个关键词分在两个簇。</p>
<p>下一步，对于每个簇，都计算它的重要性分值。<br><img src="https://s1.ax1x.com/2018/05/04/CtqA39.png" alt="CtqA39.png"><br>以前图为例，其中的簇一共有7个词，其中4个是关键词。因此，它的重要性分值等于 ( 4 x 4 ) / 7 = 2.3。</p>
<p>然后，找出包含分值最高的簇的句子（比如5句），把它们合在一起，就构成了这篇文章的自动摘要。具体实现可以参见《Mining the Social Web: Analyzing Data from Facebook, Twitter, LinkedIn, and Other Social Media Sites》（O’Reilly, 2011）一书的第8章，python代码见github。</p>
<p>Luhn的这种算法后来被简化，不再区分”簇”，只考虑句子包含的关键词。下面就是一个例子（采用伪码表示），只考虑关键词首先出现的句子。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">　　Summarizer(originalText, maxSummarySize):</span><br><span class="line">　　　　// 计算原始文本的词频，生成一个数组，比如[(<span class="number">10</span>,<span class="string">'the'</span>), (<span class="number">3</span>,<span class="string">'language'</span>), (<span class="number">8</span>,<span class="string">'code'</span>)...]</span><br><span class="line">　　　　wordFrequences = getWordCounts(originalText)</span><br><span class="line">　　　　// 过滤掉停用词，数组变成[(<span class="number">3</span>, <span class="string">'language'</span>), (<span class="number">8</span>, <span class="string">'code'</span>)...]</span><br><span class="line">　　　　contentWordFrequences = filtStopWords(wordFrequences)</span><br><span class="line">　　　　// 按照词频进行排序，数组变成[<span class="string">'code'</span>, <span class="string">'language'</span>...]</span><br><span class="line">　　　　contentWordsSortbyFreq = sortByFreqThenDropFreq(contentWordFrequences)</span><br><span class="line">　　　　// 将文章分成句子</span><br><span class="line">　　　　sentences = getSentences(originalText)</span><br><span class="line">　　　　// 选择关键词首先出现的句子</span><br><span class="line">　　　　setSummarySentences = &#123;&#125;</span><br><span class="line">　　　　foreach word <span class="keyword">in</span> contentWordsSortbyFreq:</span><br><span class="line">　　　　　　firstMatchingSentence = search(sentences, word)</span><br><span class="line">　　　　　　setSummarySentences.add(firstMatchingSentence)</span><br><span class="line">　　　　　　<span class="keyword">if</span> setSummarySentences.size() = maxSummarySize:</span><br><span class="line">　　　　　　　　<span class="keyword">break</span></span><br><span class="line">　　　　// 将选中的句子按照出现顺序，组成摘要</span><br><span class="line">　　　　summary = <span class="string">""</span></span><br><span class="line">　　　　foreach sentence <span class="keyword">in</span> sentences:</span><br><span class="line">　　　　　　<span class="keyword">if</span> sentence <span class="keyword">in</span> setSummarySentences:</span><br><span class="line">　　　　　　　　summary = summary + <span class="string">" "</span> + sentence</span><br><span class="line">　　　　<span class="keyword">return</span> summary</span><br></pre></td></tr></table></figure></p>
<p>类似的算法已经被写成了工具，比如基于Java的Classifier4J库的SimpleSummariser模块、基于C语言的OTS库、以及基于classifier4J的C#实现和python实现。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2018 rongyuewu
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>