<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">rongyuewu</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="https://github.com/smackgg/hexo-theme-smackdown">smackdown</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">rongyuewu</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">rongyuewu</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-Spark性能调优：" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/Spark性能调优：/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/Spark性能调优：/">
        Spark性能调优
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Spark性能调优："><a href="#Spark性能调优：" class="headerlink" title="Spark性能调优："></a>Spark性能调优：</h1><h2 id="1-分配更多的资源-–-性能调优的王道"><a href="#1-分配更多的资源-–-性能调优的王道" class="headerlink" title="1.分配更多的资源  – 性能调优的王道"></a>1.分配更多的资源  – 性能调优的王道</h2><pre><code>真实项目里的脚本：
    bin/spark-submit \
    --class com.xx.xx \
    --num-executors 80 \
    --driver-memory 6g \
    --executor-cores 3 \
    --master yarn-cluster \
    --queue root.default \
    --conf spark.yarn.executor.memoryOverhead=2048 \
    --conf spark.core.connection.ack.waite.timeout=300 \
    /usr/xx/xx.jar \
    args
</code></pre><h6 id="分配资源之前，要先看机器能够给我们提供多少资源，并且最大化的利用这些资源才是最好的；"><a href="#分配资源之前，要先看机器能够给我们提供多少资源，并且最大化的利用这些资源才是最好的；" class="headerlink" title="分配资源之前，要先看机器能够给我们提供多少资源，并且最大化的利用这些资源才是最好的；"></a>分配资源之前，要先看机器能够给我们提供多少资源，并且最大化的利用这些资源才是最好的；</h6><pre><code>1.standalone模式：
    根据实际情况分配spark作业资源，比如每台机器4G，2个cpu，20台机器
2.spark-on-yarn模式：
    要看spark作业要提交到的资源队列，大概有多少资源？
</code></pre><h6 id="SparkContext、DAGScheduler、taskScheduler，会将我们的算子切割成大量的task提交到Application的executor上去执行-比如分配给其中一个的executor100个task，他的cpu只有2个，那么并行只能执行2个task，如果cpu为5个，那么久先执行5个再执行5个"><a href="#SparkContext、DAGScheduler、taskScheduler，会将我们的算子切割成大量的task提交到Application的executor上去执行-比如分配给其中一个的executor100个task，他的cpu只有2个，那么并行只能执行2个task，如果cpu为5个，那么久先执行5个再执行5个" class="headerlink" title="SparkContext、DAGScheduler、taskScheduler，会将我们的算子切割成大量的task提交到Application的executor上去执行,比如分配给其中一个的executor100个task，他的cpu只有2个，那么并行只能执行2个task，如果cpu为5个，那么久先执行5个再执行5个"></a>SparkContext、DAGScheduler、taskScheduler，会将我们的算子切割成大量的task提交到Application的executor上去执行,比如分配给其中一个的executor100个task，他的cpu只有2个，那么并行只能执行2个task，如果cpu为5个，那么久先执行5个再执行5个</h6><pre><code>a.增加executor的数量：
    如果executor的数量比较少，那么意味着可以并行执行的task的数量就比较少，
    就意味着Application的并行执行能力比较弱；
        比如：
            有3个executor，每个executor有2个cup core，
            那么能够并行执行的task就是6个，6个执行完换下一批6个
    增加executor的个数后，并行执行的task就会变多，性能得到提升
b.增加每个executor的cpu core
        比如：
            之前：3个exexutor，每个executor的cpu core为2个，那么并执行的task是6个
                 把cpu core增加到5个，那么并行执行的task就是15个，提高了性能
c.增加每个executor的内存量：
    1.如果需要对RDD进行cache，那么更多的内存就能缓存更多的数据，
      将更少的数据写入磁盘，甚至不写入磁盘，减少了磁盘IO。
    2.对于shuffle操作，reduce端会需要内存来存储拉取过来的数据进行聚合，如果内存不够，
      也会写入磁盘，增加executor内存，就会有更少的数据写入磁盘，较少磁盘IO，提高性能。
    3.对于task的执行，需要创建很多对象，如果内存比较小，可能导致JVM堆内存满了，
      然后频繁的GC，垃圾回收，minorGC和fullGC，速度会很慢，如果加大内存，
      带来更少的GC，速度提升。
</code></pre><h2 id="2-调节并行度"><a href="#2-调节并行度" class="headerlink" title="2.调节并行度"></a>2.调节并行度</h2><pre><code>并行度：spark作业中，各个stage的task个数，也就代表了saprk作业中各个阶段[stage]的并行度。
[spark作业，Application，jobs，action会触发一个job，每个job会拆成多个stage，发生shuffle的时候，会拆出一个stage]
</code></pre><h5 id="如果不调节并行度，导致并行度过低，会怎么样？？？"><a href="#如果不调节并行度，导致并行度过低，会怎么样？？？" class="headerlink" title="如果不调节并行度，导致并行度过低，会怎么样？？？"></a>如果不调节并行度，导致并行度过低，会怎么样？？？</h5><pre><code>比如：
    1.我们通过上面的分配好资源后，有50个executor，每个executor10G内存，每个executor有3个cpu core
      基本已经达到了集群或者yarn的资源上限
    2. 50个executor * 3个cpu = 150个task,即可以并行执行150个task；
      而我们设置的时候只设置了100个并行度的task，这时候每个executor分配到2个task，同时运行task的数量只有100个，导致每个executor剩下的1个cpu core在那空转，浪费资源。
    资源虽然够了，但是并行度没有和资源相匹配，导致分配下去的资源浪费掉了！！！
    **合理的并行度设置，应该要设置的足够大，大到完全合理的利用集群资源！而且减少了每个task要处理的数据量[比如150g的数据分别分发给100个task处理就是每个task处理1.5G，但是如果是150个task的话，每个task就处理1G]**
总结：
    a. task数量，至少设置成与Spark application的总cpu数相同(理想状态下，比如150个cpu core，分配150个task，差不多同时运行完毕)
    b.官方推荐做法：task的数量设置成 Spark application的cpu core的个数的3~5倍！！
      比如总共150个cpu core，设置成300~500个task
      为什么这么设置呢？？？
           实际情况下和理想状态下是不一样的，因为有些task运行的快，有些运行的慢，
           比如有些运行需要50s，有些需要几分钟运行完毕，如果刚好设置task数量和cpu core的数量相同，可能会导致资源的浪费；
           比如150个task，10个运行完了，还有140个在运行，那么势必会导致10个cpu core的闲置，
           所以如果设置task的数量为cpu数量的2~3倍，一旦有task运行完，另一个task就会立刻补上来，
           尽量让cpu core不要空闲，提升了spark作业运行效率，提升性能。

    c.如何设置一个 Spark application的并行度？？？
        SparkConf sc = new SparkConf()
                       .set(&quot;spark.default.parallelism&quot;,&quot;500&quot;);
</code></pre><h2 id="3-重构RDD架构及RDD持久化"><a href="#3-重构RDD架构及RDD持久化" class="headerlink" title="3.重构RDD架构及RDD持久化:"></a>3.重构RDD架构及RDD持久化:</h2><pre><code>默认情况下 RDD出现的问题：             
                                              RDD4
                                            /
           hdfs --&gt; RDD1 --&gt; RDD2 --&gt;RDD3
                                            \
                                              RDD5
    以上情况： 执行RDD4和RDD5的时候都会从RDD1到RDD2然后到RDD3，执行期间的算子操作，
    而不会说到RDD3的算子操作后的结果给缓存起来，所以这样很麻烦，
    出现了RDD重复计算的情况，导致性能急剧下降！
结论：
    对于要多次计算和使用的公共RDD,一定要进行持久化！
    持久化也就是：BlockManager将RDD的数据缓存到内存或者磁盘上，后续无论对这个RDD进行多少次计算，都只需要到缓存中取就ok了。
    持久化策略：
        rdd.persist(StorageLevel.xx()) 或者 cache
        1.优先会把数据缓存到内存中 -- StorageLevel.MEMORY_ONLY
        2.如果纯内存空间无法支撑公共RDD的数据时，就会优先考虑使用序列化的机制在纯内存中存储，
        将RDD中的每个partition中的数据序列化成一个大的字节数组，也就是一个对象，
        序列化后，大大减少了内存空间的占用。-- StorageLevel.MEMORY_ONLY_SER
            序列化唯一的缺点：在获取数据的时候需要反序列化
        3.如果序列化纯内存的方式还是导致OOM，内存溢出的话，那就要考虑磁盘的方式。
          内存+磁盘的普通方式(无序列化) -- StorageLevel.MEMORY_AND_DISK
        4.如果上面的还是无法存下的话，就用 内存+磁盘+序列化 -- StorageLevel.MEMORY_AND_DISK_SER
另：在机器内存**极度充足**的情况下，可以使用双副本机制，来持久化，保证数据的高可靠性
    如果机器宕机了，那么还有一份副本数据，就不用再次进行算子计算了。[锦上添花--一般不要这么做] -- StorageLevel.MEMORY_ONLY_SER_2
</code></pre><h3 id="4-广播大变量-sc-broadcast-rdd-collect"><a href="#4-广播大变量-sc-broadcast-rdd-collect" class="headerlink" title="4.广播大变量 [sc.broadcast(rdd.collect)]"></a>4.广播大变量 [sc.broadcast(rdd.collect)]</h3><pre><code>问题情景：
    当我们在写程序用到外部的维度表的数据进行使用的时候，程序默认会给每个task都发送相同的这个数据，
    如果这个数据为100M，如果我们有1000个task，100G的数据，通过网络传输到task，
    集群瞬间因为这个原因消耗掉100G的内存，对spark作业运行速度造成极大的影响，性能想想都很可怕！！！
解决方案：
    sc.broadcast(rdd.collect)
    分析原理：
        [BlockManager:负责管理某个executor上的内存和磁盘上的数据]
        广播变量会在Driver上有一份副本，当一个task使用到广播变量的数据时，会在自己本地的executor的BlockManager去取数据，
        发现没有，BlockManager就会到Driver上远程去取数据，并保存在本地，
        然后第二个task需要的时候来找BlockManager直接就可以找到该数据，
        executor的Blockmanager除了可以到Driver远程的取数据，
        还可能到邻近节点的BlockManager上去拉取数据，越近越好!
    举例说明：
        50个executor，1000个task，外部数据10M，
        默认情况下，1000个task，1000个副本，10G数据，网络传输，集群中10G的内存消耗
        如果使用广播，50个executor，500M的数据，网络传输速率大大增加，
        10G=10000M 和 500M的对比 20倍啊。。。
**之前的一个测试[真实]：
        没有经过任何调优的spark作业，运行下来16个小时，
        合理分配资源+合理分配并行度+RDD持久化，作业下来5个小时,
        非常重要的一个调优Shuffle优化之后，2~3个小时，
        应用了其他的性能调优之后，JVM调参+广播等等，30分钟
        16个小时 和 30分钟对比，太可怕了！！！性能调优真的真的很重要！！！
</code></pre><h3 id="5-Kryo序列化机制："><a href="#5-Kryo序列化机制：" class="headerlink" title="5.Kryo序列化机制："></a>5.Kryo序列化机制：</h3><p>默认情况下，Spark内部使用java的序列化机制</p>
<pre><code>ObjectOutPutStream/ObjectInputStream,对象输入输出流机制来进行序列化
这种序列化机制的好处：
    处理方便，只是需要在算子里使用的变量是实现Serializable接口即可
缺点在于：
    默认序列化机制效率不高，序列化的速度比较慢，序列化后的数据占用内存空间还比较大
</code></pre><p> 解决：手动进行序列化格式的优化：Kryo [spark支持的]</p>
<pre><code>Kryo序列化机制比默认的java序列化机制速度快，
序列化后的数据更小，是java序列化后数据的 1/10 。
所以Kryo序列化后，会让网络传输的数据更少，在集群中耗费的资源大大减少。

Kryo序列化机制：[一旦启用，会生效的几个地方]
    a.算子函数中使用到的外部变量[比如广播的外部维度表数据]
        优化网络传输性能，较少集群的内存占用和消耗
    b.持久化RDD时进行序列化，StorageLevel.MEMORY_ONLY_SER
        将每个RDD partition序列化成一个大的字节数组时，就会使用Kryo进一步优化序列化的效率和性能。
        持久化RDD占用的内存越少，task执行的时候，创建的对象，不至于频繁的占满内存，频繁的GC
    c.shuffle
        在stage间的task的shuffle操作时，节点与节点之间的task会通过网络拉取和传输数据，
        此时这些数据也是可能需要序列化的，就会使用Kryo
</code></pre><p> 实现Kryo：</p>
<pre><code>step1. 在SparkConf里设置 new SparkConf()
                       .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KyroSerializer&quot;)
                       .registerKryoClasses(new Class[]{MyCategory.class})
    [Kryo之所以没有没有作为默认的序列化类库，就是因为Kryo要求，如果要达到它的最佳效果的话]
    [一定要注册我们自定义的类，不如：算子函数中使用到了外部自定义的对象变量，这时要求必须注册这个类，否则Kyro就达不到最佳性能]
step2. 注册使用到的，需要Kryo序列化的一些自定义类
</code></pre><h3 id="6-使用FastUtil优化数据格式："><a href="#6-使用FastUtil优化数据格式：" class="headerlink" title="6.使用FastUtil优化数据格式："></a>6.使用FastUtil优化数据格式：</h3><p>FastUtil是什么？？</p>
<pre><code>fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue；
fastutil能够提供更小的内存占用，更快的存取速度；我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，
fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。 
fastutil还提供了一些JDK标准类库中没有的额外功能（比如双向迭代器）。 
fastutil除了对象和原始类型为元素的集合，fastutil也提供引用类型的支持，但是对引用类型是使用等于号（=）进行比较的，而不是equals()方法。 
fastutil尽量提供了在任何场景下都是速度最快的集合类库。
</code></pre><p>Spark中FastUtil运用的场景？？</p>
<pre><code>1.如果算子函数使用了外部变量，
    第一可以使用broadcast广播变量优化；
    第二可以使用Kryo序列化类库，提升序列化性能和效率；
    第三如果外部变量是某种比较大的集合(Map、List等)，可以考虑fastutil来改写外部变量，
        首先从源头上就减少了内存的占用，通过广播变量进一步减少内存占用，
        再通过Kryo类库进一步减少内存占用
    避免了executor内存频繁占满，频繁唤起GC，导致性能下降的现象
</code></pre><p>使用步骤：</p>
<pre><code>step1:导入pom依赖
    &lt;dependency&gt;
        &lt;groupId&gt;fastutil&lt;/groupId&gt;
            &lt;artifactId&gt;fastutil&lt;/artifactId&gt;
        &lt;version&gt;5.0.9&lt;/version&gt;
    &lt;/dependency&gt;
step2:
    List&lt;Integer&gt; =&gt; IntList
     基本都是类似于IntList的格式，前缀就是集合的元素类型，
     特殊的就是Map，Int2IntMap，代表了Key-Value映射的元素类型
</code></pre><h3 id="7-调节数据本地化等待时长"><a href="#7-调节数据本地化等待时长" class="headerlink" title="7.调节数据本地化等待时长:"></a>7.调节数据本地化等待时长:</h3><p>问题发生的场景：</p>
<pre><code>spark在Driver上，对Application的每一个stage的task分配之前，
都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；
spark的分配算法：
    a.优先把每一个task正好分配到他要计算的数据所在的节点，这样的话不用在网络间传输数据
    b.但是，task没有机会分配到数据所在的节点上，为什么呢？？？
        因为那个节点上的计算资源和计算能力都满了，这个时候 spark会等待一段时间，
        默认情况下是3s钟，到最后，实在等不了了，就会选择一个较差的本地化级别，
        比如说会把task分配到靠他要计算的数据的节点最近的节点，然后进行计算
    c.对于b来说肯定要发生网络传输，task会通过其所在节点的executor的BlockManager来获取数据，
    BlockManager发现自己本地没有，就会用getRemote()的方法，通过TransferService(网络数据传输组件)
    从数据所在节点的BlockManager中获取数据，通过网络传输给task所在的节点
总结：
  我们肯定是希望看到 task和数据都在同一个节点上，直接从本地的executor中的BlockManager中去获取数据，
  纯内存或者带点IO，如果通过网络传输，那么大量的网络传输和磁盘IO都是性能的杀手
</code></pre><p>本地化的级别类型：</p>
<pre><code>1.PROCESS_LOCAL: 进程本地化,代码和数据都在同一个进程中，也就是在同一个executor进程中，
  task由executor来执行，数据在executor的BlockManager中，性能最好
2.NODE_LOCAL: 节点本地化，比如说一个节点上有两个executor，其中一个task需要第一个executor的数据，
  但是他被分配给了第二个executor，他会找第二个executor的BlockManager去取数据，但是没有，
  BlockManager会去第一个的executor的BlockManager去取数据，这是发生在进程中的
3.NOPREF: 数据从哪里获取都一样，没有好坏之分
4.RACK_LOCAL: 数据在同一个机架上的不同节点上，需要进行网络间的数据传输
5.ANY: 数据可能在集群中的任何地方，而且不在同一个机架，这种性能最差！！
</code></pre><p>开发时的流程：</p>
<pre><code>观察spark作业时的日志，先测试，先用client模式，在本地就可以看到比较全的日志。
日志里面会显示：starting task...,PROCESS_LOCAL,或者是NODE_LOCAL，观察大部分数据本地化的级别
如果发现大部分都是PROCESS_LOCAL的级别，那就不用调了，如果大部分都是NODE_LOCAL或者ANY，那就要调节一下等待时长了
要反复调节，反复观察本地化级别是否提升，查看spark作业运行的时间有没有缩短
不要本末倒置，如果是 本地化级别提升了，但是因为大量的等待时间，spark作业的运行时常变大了，这就不要调节了
spark.locality.waite
spark.locality.waite.process
spark.locality.waite.node
spark.locality.waite.rack
默认等待时长都是3s
设置方法：
    new SparkConf().set(&quot;spark.locality.waite&quot;,&quot;10&quot;)//不要带s
</code></pre><h3 id="8-JVM调优：1个executor对应1个JVM进程"><a href="#8-JVM调优：1个executor对应1个JVM进程" class="headerlink" title="8.JVM调优：1个executor对应1个JVM进程"></a>8.JVM调优：1个executor对应1个JVM进程</h3><p>A. 降低cache操作的内存占比</p>
<pre><code>JVM模块：
    每一次存放对象的时候都会放入eden区域，其中有一个survivor区域，另一个survivor区域是空闲的[新生代]，
    当eden区域和一个survivor区域放满了以后(spark运行产生的对象太多了)，
    就会触发minor gc，小型垃圾回收，把不再使用的对象从内存中清空，给后面新创建的对象腾出空间
    清理掉了不在使用的对象后，还有一部分存活的对象(还要继续使用的对象)，
    将存活的对象放入空闲的那个survivor区域里，这里默认eden:survivor1: survivor2 = 8:1:1,
    假如对象占了1.5放不下survivor区域了，那么就会放到[老年代]里；
    假如JVM的内存不够大的话，可能导致频繁的新生代内存满溢，频繁的进行minor gc，
    频繁的minor gc会导致短时间内，有些存活的对象，多次垃圾回收都没有回收掉，
    会导致这种短生命周期的对象(其实是不一定要长期使用的对象)年龄过大，
    垃圾回收次数太多，还没有回收到，就已经跑到了老年代；
    老年代中可能会因为内存不足，囤积一大堆短生命周期的对象(本来应该在年轻代中的)，
    可能马上就要回收掉的对象，此时可能造成老年代内存满溢，造成频繁的full gc(全局/全面垃圾回收)，full gc就会去老年代中回收对象；
    由于full gc算法的设计，是针对老年代中的对象，数量很少，满溢进行full gc的频率应该很少，
    因此采取了不太复杂的但是耗费性能和时间的垃圾回收算法。full gc 很慢很慢；
    full gc 和 minor gc，无论是快还是慢，都会导致JVM的工作线程停止工作，即 stop the world，
    简言之：gc的时候，spark停止工作，等待垃圾回收结束;
在spark中，堆内存被分为了两块：
    一块是专门用来给RDD cache和persist操作进行RDD数据缓存用的；
    一块是给spark算子函数的运行使用的，存放函数中自己创建的对象；
默认情况下，给RDD cache的内存占比是60%,但是在某些情况下，比如RDD cache不那么紧张，
而task算子函数中创建的对象过多，内存不太大，导致频繁的minor gc，甚至频繁的full gc，
导致spark频繁的暂停工作，性能影响会非常大，
解决办法：
    集群是spark-onyarn的话就可以通过spark ui来查看，spark的作业情况，
    可以看到每个stage的运行情况，包括每个task的运行时间，gc时间等等，
    如果发现gc太频繁，时间太长，此时可以适当调节这个比例；
总结：
    降低cache的内存占比，大不了用persist操作，选择将一部分的RDD数据存入磁盘，
    或者序列化方式Kryo，来减少RDD缓存的内存占比；
    对应的RDD算子函数的内存占比就增多了，就可以减少minor gc的频率，同时减少full gc的频率，提高性能
具体实现：0.6-&gt;0.5-&gt;0.4-&gt;0.2
    new SparkConf().set(&quot;spark.storage.memoryFraction&quot;,&quot;0.5&quot;)
</code></pre><p>B. executor堆外内存与连接时常</p>
<pre><code>1. executor堆外内存[off-heap memory]:
   场景：
        比如两个stage，第二个stage的executor的task需要第一个executor的数据，
        虽然可以通过Driver的MapOutputTracker可以找到自己数据的地址[也就是第一个executor的BlockManager]，
        但是第一个executor已经挂掉了，关联的BlockManager也没了，就没办法获取到数据；

    有时候，如果你的spark作业处理的数据量特别大，几亿数据量；
    spark作业一运行，是不是报错诸如：shuffle file cannot find,executor task lost,out of memory,
    这时候可能是executor的堆外内存不够用了，导致executor在运行的时候出现了内存溢出；
    导致后续的stage的task在运行的时候，可能从一些executor中拉取shuffle map output 文件，
    但是executor已经挂掉了，关联的BlockManager也没有了，所以可能会报shuffle output file not found，resubmitting task，executor lost，spark作业彻底失败；
  这个时候就可以考虑调节executor的堆外内存，堆外内存调节的比较大的话，也会提升性能；

    怎么调价堆外内存的大小？？
        在spark-submit 的脚本中添加 
                    --conf spark.yarn.executor.memoryOverhead=2048
        注意：这个设置是在spark-submit脚本中，不是在 new SparkConf()里设置的！！！
        这个是在spark-onyarn的集群中设置的，企业也是这么设置的！
        默认情况下,堆外内存是300多M，我们在项目中通常都会出现问题，导致spark作业反复崩溃，
        我们就可以调节这个参数 ，一般来说至少1G(1024M)，有时候也会2G、4G，
        来避免JVM oom的异常问题，提高整体spark作业的性能
2. 连接时常的等待：
          知识回顾：如果JVM处于垃圾回收过程，所有的工作线程将会停止，相当于一旦进行垃圾回收，
          spark/executor就会停止工作，无法提供响应
   场景：
          通常executor优先会从自己关联的BlockManager去取数据，如果本地没有，
          会通过TransferService，去远程连接其他节点上的executor的BlockManager去取；
          如果这个远程的executor正好创建的对象特别大，特别多，频繁的让JVM的内存满溢，进行垃圾回收，
          此时就没有反应，无法建立网络连接，会有卡住的现象。spark默认的网络连接超时时间是60s，
          如果卡住60秒都无法建立网络连接的话，就宣布失败；
          出现的现象：偶尔会出现，一串fileId诸如：hg3y4h5g4j5h5g5h3 not found，file lost，
          报错几次，几次都拉取不到数据的话，可能导致spark作业的崩溃！
          也可能会导致DAGScheduler多次提交stage，TaskScheduler反复提交多次task，
          大大延长了spark作业的运行时间
  解决办法：[注意是在shell脚本上不是在SparkConf上set！！]
          spark-submit 
                       --conf spark.core.connection.ack.waite.timeout=300
</code></pre><h2 id="9-shuffle调优"><a href="#9-shuffle调优" class="headerlink" title="9.shuffle调优"></a>9.shuffle调优</h2><pre><code>shuffle的概念以及场景
    什么情况下会发生shuffle？？
        在spark中，主要是这几个算子：groupByKey、reduceByKey、countByKey、join等
    什么是shuffle？
        a) groupByKey：把分布在集群中各个节点上的数据中同一个key，对应的values都集中到一块，
        集中到集群中的同一个节点上，更严密的说就是集中到一个节点上的一个executor的task中。
        集中一个key对应的values后才能交给我们处理，&lt;key,iterable&lt;value&gt;&gt;
          b) reduceByKey：算子函数对values集中进行reduce操作，最后变成一个value
          c) join  RDD&lt;key,value&gt;    RDD&lt;key,value&gt;,只要两个RDD中key相同的value都会到一个节点的executor的task中，供我们处理
      以reduceByKey为例：
</code></pre><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/62Boojghtc6iLB8KUOFgXQfCRbAwTZoOek9gr3ocl*o!/b/dDMBAAAAAAAA&amp;bo=YgrgAgAAAAADB6g!&amp;rf=viewer_4" alt=""></p>
<h4 id="9-1-shuffle调优之-map端合并输出文件"><a href="#9-1-shuffle调优之-map端合并输出文件" class="headerlink" title="9.1. shuffle调优之 map端合并输出文件"></a>9.1. shuffle调优之 map端合并输出文件</h4><pre><code>默认的shuffle对性能有什么影响？？
    实际生产环境的条件：
        100个节点，每个节点一个executor：100个executor，每个executor2个cpu core，
        总格1000个task，平均到每个executor是10个task；按照第二个stage的task个数和第一个stage的相同，
        那么每个节点map端输出的文件个数就是：10 * 1000 = 10000 个
        总共100个节点，总共map端输出的文件数：10000 * 100 = 100W 个
        100万个。。。太吓人了！！！
    shuffle中的写磁盘操作，基本上是shuffle中性能消耗最严重的部分，
    通过上面的分析可知，一个普通的生产环境的spark job的shuffle环节，会写入磁盘100万个文件，
    磁盘IO性能和对spark作业执行速度的影响，是极其惊人的！！
    基本上，spark作业的性能，都消耗在了shuffle中了，虽然不只是shuffle的map端输出文件这一部分，但是这也是非常大的一个性能消耗点。
怎么解决？
    开启map端输出文件合并机制：
        new SparkConf().set(&apos;spark.shuffle.consolidateFiles&apos;,&apos;true&apos;)
    实际开发中，开启了map端输出文件合并机制后，有什么变化？
        100个节点，100个executor，
        每个节点2个cpu core，
        总共1000个task，每个executor10个task，
        每个节点的输出文件个数：
            2*1000 = 2000 个文件
        总共输出文件个数：
            100 * 2000 = 20万 个文件
        相比开启合并之前的100万个，相差了5倍！！
合并map端输出文件，对spark的性能有哪些影响呢？
    1. map task写入磁盘文件的IO，减少：100万 -&gt; 20万个文件
    2. 第二个stage，原本要拉取第一个stage的task数量文件，1000个task，第二个stage的每个task都会拉取1000份文件，走网络传输；合并以后，100个节点，每个节点2个cpu，第二个stage的每个task只需要拉取 100 * 2 = 200 个文件，网络传输的性能大大增强
    实际生产中，使用了spark.shuffle.consolidateFiles后，实际的调优效果：
        对于上述的生产环境的配置，性能的提升还是相当可观的，从之前的5个小时 降到了 2~3个小时
总结：
    不要小看这个map端输出文件合并机制，实际上在数据量比较大的情况下，本身做了前面的优化，
    executor上去了 -&gt; cpu core 上去了 -&gt; 并行度（task的数量）上去了，但是shuffle没调优，
    这时候就很糟糕了，大量的map端输出文件的产生，会对性能有比较恶劣的影响 
</code></pre><h4 id="9-2-map端内存缓冲与reduce端内存占比"><a href="#9-2-map端内存缓冲与reduce端内存占比" class="headerlink" title="9.2. map端内存缓冲与reduce端内存占比"></a>9.2. map端内存缓冲与reduce端内存占比</h4><pre><code>spark.shuffle.file.buffer,默认32k
spark.shuffle.memoryFraction,占比默认0.2
调优的分量：
    map端内存缓冲和reduce端内存占比，网上对他俩说的是shuffle调优的不二之选，其实这是不对的，
    因为以实际的生产经验来说，这两个参数没那么重要，但是还是有一点效果的，
    就像是很多小的细节综合起来效果就很明显了，
</code></pre><p>原理：</p>
<pre><code>map：
    默认情况下，shuffle的map task输出到磁盘文件的时候，统一都会先写入每个task自己关联的一个内存缓冲区中，
    这个缓冲区默认大小是32k，每一次，当内存缓冲区满溢后，才会进行spill操作，溢写到磁盘文件中
reduce：
    reduce端task，在拉取数据之后，会用hashmap的数据格式来对每个key对应的values进行汇聚，
    针对每个key对应的value，执行我们自定义的聚合函数的代码，比如_+_,(把所有values相加)
    reduce task,在进行汇聚、聚合等操作的时候，实际上，使用的就是自己对应的executor的内存，
    executor(jvm进程，堆),默认executor内存中划分给reduce task进行聚合的比例是20%。
    问题来了，内存占比是20%，所以很有可能会出现，拉取过来的数据很多，那么在内存中，
    放不下，这个时候就会发生spill(溢写)到磁盘文件中取.
</code></pre><p>如果不调优会出现什么问题？？</p>
<pre><code>默认map端内存缓冲是32k，
默认reduce端聚合内存占比是20%
如果map端处理的数据比较大，而内存缓冲是固定的，会出现什么问题呢？
    每个task处理320k，32k的内存缓冲，总共向磁盘溢写10次，
    每个task处理32000k，32k的内存缓冲，总共向磁盘溢写1000次，
    这样就造成了多次的map端往磁盘文件的spill溢写操作，发生大量的磁盘IO，降低性能
map数据量比较大，reduce端拉取过来的数据很多，就会频繁的发生reduce端聚合内存不够用，
频繁发生spill操作，溢写到磁盘上去，这样一来，磁盘上溢写的数据量越大，
后面进行聚合操作的时候，很可能会多次读取磁盘中的数据进行聚合
默认情况下，在数据量比较大的时候，可能频繁的发生reduce端磁盘文件的读写；
这两点是很像的，而且有关联的，数据量变大，map端肯定出现问题，reduce也出现问题，
出的问题都是一样的，都是磁盘IO频繁，变多，影响性能
</code></pre><p>调优解决：</p>
<pre><code>我们要看spark UI，
    1. 如果公司用的是standalone模式，那么很简单，把spark跑起来，会显示sparkUI的地址，
    4040端口号，进去看，依次点击可以看到，每个stage的详情，有哪些executor，有哪些task，
    每个task的shuffle write 和 shuffle read的量，shuffle的磁盘和内存，读写的数据量
    2. 如果是yarn模式提交，从yarn的界面进去，点击对应的application，进入spark ui，查看详情
如果发现磁盘的read和write很大，就意味着要调节一下shuffle的参数，进行调优，
首先当然要考虑map端输出文件合并机制
   调节上面两个的参数，原则是：
      spark.shuffle.buffer，每次扩大一倍，然后看看效果，64k，128k
    spark.shuffle.memoryFraction,每次提高0.1，看看效果
不能调节的过大，因为你这边调节的很大，相对应的其他的就会变得很小，其他环节就会出问题
调节后的效果：
    map task内存缓冲变大了，减少了spill到磁盘文件的次数；
    reduce端聚合内存变大了，减少了spill到磁盘的次数，而且减少了后面聚合时读取磁盘的数量
    new SparkConf()
    .set(&quot;spark.shuffle.file.buffer&quot;,&quot;64&quot;)
    .set(&quot;spark.shuffle.file.memoryFraction&quot;,&quot;0.3&quot;)
</code></pre><h2 id="10-算子调优"><a href="#10-算子调优" class="headerlink" title="10.算子调优"></a>10.算子调优</h2><h5 id="1-算子调优之MapPartitons提升map的操作性能"><a href="#1-算子调优之MapPartitons提升map的操作性能" class="headerlink" title="1.算子调优之MapPartitons提升map的操作性能"></a>1.算子调优之MapPartitons提升map的操作性能</h5><pre><code>在spark中最近本的原则：每个task处理RDD中的每一个partition
优缺点对比：
    普通Map：
        优点：比如处理了一千条数据，内存不够了，那么就可以将已经处理的一千条数据从内存里面垃圾回收掉，
        或者用其他办法腾出空间；通常普通的map操作不会导致内存OOM异常；
        缺点：比如一个partition中有10000条数据，那么function会执行和计算一万次
    MapPartitions:
        优点：一个task仅仅会执行一次function，一次function接收partition中的所有数据
        只要执行一次就可以了，性能比较高
        缺点：对于大数据量来说，比如一个partition100万条数据，一次传入一个function后，
        可能一下子内存就不够了，但是又没办法腾出空间来，可能就OOM，内存溢出
那么什么时候使用MapPartitions呢？
    当数据量不太大的时候，都可以使用MapPartitions来操作，性能还是很不错的，
    不过也有经验表明用了MapPartitions后，内存直接溢出，
    所以在项目中自己先估算一下RDD的数据量，以每个partition的量，还有分配给executor的内存大小，
    可以试一下，如果直接OOM了，那就放弃吧，如果能够跑通，那就可以使用。
</code></pre><h5 id="2-算子调优之filter之后-filter-之后-用-coalesce来减少partition的数量"><a href="#2-算子调优之filter之后-filter-之后-用-coalesce来减少partition的数量" class="headerlink" title="2.算子调优之filter之后 filter 之后  用 coalesce来减少partition的数量"></a>2.算子调优之filter之后 filter 之后  用 coalesce来减少partition的数量</h5><pre><code>默认情况下，RDD经过filter之后，RDD中每个partition的数据量会不太一样，(原本partition里的数据量可能是差不多的)
问题：
    1.每一个partition的数据量变少了，但是在后面进行处理的时候，
    还是要和partition的数量一样的task数量去处理，有点浪费task计算资源
    2.每个partition的数据量不一样，后面会导致每个处理partition的task要处理的数据量不一样，
    这时候很容易出现**数据倾斜**
    比如说，有一个partition的数据量是100，而另一个partition的数据量是900，
    在task处理逻辑一样的情况下，不同task要处理的数据量可能差别就到了9倍，甚至10倍以上，
    同样导致速度差别在9倍或者10倍以上
    这样就是导致了有的task运行的速度很快，有的运行的很慢，这就是数据倾斜。
解决：
    针对以上问题，我们希望把partition压缩，因为数据量变小了，partition完全可以对应的变少，
    比如原来4个partition，现在可以变成2个partition，那么就只要用后面的2个task来处理，
    不会造成task资源的浪费(不必要针对只有一点点数据的partition来启动一个task进行计算)
    避免了数据倾斜的问题
</code></pre><h5 id="3-算子调优之使用foreachPartition优化写入数据库性能"><a href="#3-算子调优之使用foreachPartition优化写入数据库性能" class="headerlink" title="3.算子调优之使用foreachPartition优化写入数据库性能"></a>3.算子调优之使用foreachPartition优化写入数据库性能</h5><pre><code>默认的foreach有哪些缺点？
    首先和map一样，对于每条数据都要去调一次function，task为每个数据，都要去执行一次task；
    如果一个partition有100万条数据，就要调用100万次，性能极差！
    如果每条数据都要创建一个数据库连接，那么就要创建100万个数据库连接，
    但是数据库连接的创建和销毁都是非常耗性能的，虽然我们用了数据库连接池，只要创建固定数量的连接，
    还是得多次通过数据库连接，往数据库里(mysql)发送一条sql语句，mysql需要去执行这条sql语句，
    有100万条数据，那么就是要发送100万次sql语句；
用了foreachPartition以后，有哪些好处？
    1.对于我们写的函数就调用一次就行了，一次传入一个partition的所有数据
    2.主要创建或者获取一个数据库连接就可以了
    3.只要向数据里发送一条sql语句和一组参数就可以了
在实际开发中，我们都是清一色使用foreachPartition算子操作，
但是有个问题，跟mapPartitions操作一样，如果partition的数据量非常大，
比如真的是100万条，那几本就不行了！一下子进来可能会发生OOM，内存溢出的问题
一组数据的对比：
    生产环境中：
        一个partition中有1000条数据，用foreach，跟用foreachPartition，
        性能提高了2~3分；
数据库里是：
    for循环里preparestatement.addBatch
    外面是preparestatement.executeBatch
</code></pre><h5 id="4-算子调优之repartition解决SparkSQL低并行度的问题"><a href="#4-算子调优之repartition解决SparkSQL低并行度的问题" class="headerlink" title="4.算子调优之repartition解决SparkSQL低并行度的问题"></a>4.算子调优之repartition解决SparkSQL低并行度的问题</h5><pre><code>并行度： 我们是可以自己设置的
    1.spark.default.parallelism
    2.sc.textFile(),第二个参数传入指定的数量(这个方法用的非常少)
在生产环境中，我们是要自己手动设置一下并行度的，官网推荐就是在spark-submit脚本中，
指定你的application总共要启动多少个executor，100个，每个executor多少个cpu core，
2~3个，假设application的总cpu core有200个；
官方推荐设置并行度要是总共cpu core个数的2~3倍，一般最大值，所以是 600；
设置的这个并行度，在哪些情况下生效？哪些情况下不生效？
    1.如果没有使用SparkSQL(DataFrame)的话，那么整个spark应用的并行度就是我们设置的那个并行度
    2.如果第一个stage使用了SparkSQL从Hive表中查询了一些数据，然后做了一些transformatin的操作，
    接着做了一个shuffle操作(groupByKey)；下一个stage，在shuffle之后，做了一些transformation的操作
    如果Hive表对应了20个block，而我们自己设置的并行度是100，
    那么第一个stage的并行度是不受我们控制的，就只有20个task，第二个stage的才是我们设置的并行度100个
问题出在哪里了？
    SparkSQL 默认情况下，我们是没办法手动设置并行度的，所以可能造成问题，也可能不造成问题，
    SparkSQL后面的transformation算子操作，可能是很复杂的业务逻辑，甚至是很复杂的算法，
    如果SparkSQL默认的并行度设置的很少，20个，然后每个task要处理为数不少的数据量，
    还要执行很复杂的算法，这就导致第一个stage特别慢，第二个stage 1000个task，特别快！
解决办法：
    repartition：
        使用SparkSQL这一步的并行度和task的数量肯定是没办法改变了，但是可以将SparkSQL查出来的RDD，
        使用repartition算子进行重新分区，比如分多个partition，20 -&gt; 100个；
        然后从repartition以后的RDD，并行度和task数量，就会按照我们预期的来了，
        就可以避免在跟SparkSQL绑定在一起的stage中的算子，只能使用少量的task去处理大量数据以及复杂的算法逻辑
</code></pre><p>5.算子操作reduceByKey：</p>
<pre><code>reduceByKey相较于普通的shuffle操作(不如groupByKey)，他的一个特点就是会进行map端的本地聚合；
对map端给下个stage每个task创建的输出文件中，写数据之前，就会进行本地的combiner操作，也就是多每个key的value，都会执行算子函数(_+_)，减少了磁盘IO，较少了磁盘空间的占用,在reduce端的缓存也变少了
</code></pre><h2 id="11-troubleshooting之控制reduce端缓冲大小以避免内存溢出-OOM"><a href="#11-troubleshooting之控制reduce端缓冲大小以避免内存溢出-OOM" class="headerlink" title="11.troubleshooting之控制reduce端缓冲大小以避免内存溢出(OOM)"></a>11.troubleshooting之控制reduce端缓冲大小以避免内存溢出(OOM)</h2><pre><code>new SparkConf().set(&quot;spark.reducer.maxSizeInFlight&quot;,&quot;24&quot;) //默认是48M
Map端的task是不断地输出数据的，数据量可能是很大的，
    但是其reduce端的task，并不是等到Map端task将属于自己的那个分数据全部写入磁盘后，再去拉取的
    Map端写一点数据，reduce端task就会去拉取一小部分数据，立刻进行后面的聚合，算子函数的应用；
    每次reduce能够拉取多少数据，是由reduce端buffer来定，因为拉取过来的数据都是放入buffer中的，
    然后采用后面的executor分配的堆内存占比(0.2),去进行后续的聚合，函数操作
reduce端buffer 可能会出现什么问题？
    reduce端buffer默认是48M，也许大多时候，还没有拉取满48M，也许是10M，就计算掉了，
    但是有时候，Map端的数据量特别大，写出的速度特别快，reduce端拉取的时候，全部到达了自己缓冲的最大极限48M，全部填满，
    这个时候，再加上reduce端执行的聚合函数代码，可能会创建大量的对象，也许一下子内存就撑不住了，就会造成OOM，reduce端的内存就会造成内存泄漏
如何解决？
    这个时候，我们应该减少reduce端task缓冲的大小，我们宁愿多拉取几次，但是每次同时能拉取到reduce端每个task的数据量比较少，就不容易发生OOM，比如调成12M；
    在实际生产中，这种问题是很常见的，这是典型的以性能换执行的原理，
    reduce的缓冲小了，不容易造成OOM了，但是性能一定是有所下降的，你要拉取的次数多了，
    就会走更多的网络IO流，这时候只能走牺牲性能的方式了；
曾经一个经验：
    曾经写了一个特别复杂的spark作业，写完代码后，半个月就是跑步起来，里面各种各样的问题，
    需要进行troubleshooting，调节了十几个参数，其中里面就有reduce端缓冲的大小，最后，
    总算跑起来了！
</code></pre><h3 id="12-troubleshooting之解决JVM-GC导致的shuffle拉取文件失败："><a href="#12-troubleshooting之解决JVM-GC导致的shuffle拉取文件失败：" class="headerlink" title="12. troubleshooting之解决JVM GC导致的shuffle拉取文件失败："></a>12. troubleshooting之解决JVM GC导致的shuffle拉取文件失败：</h3><pre><code>过程：
    第一个stage的task输出文件的同时 ，会像Driver上记录这些数据信息，然后下一个stage的task想要得到上个stage的数据，
    就得像Driver所要元数据信息，然后去像上一个的stage的task生成的文件中拉取数据。
问题场景：
    在spark作业中，有时候经常出现一种情况，就是log日志报出：shuffle file not found..,
    有时候他会偶尔出现一次，有的时候出现一次后重新提交stage、task，重新执行一遍 就好了。
分析问题：
    executor在JVM进程中，可能内存不太够用，那么此时就很可能执行GC，minor gc 或者 full gc，
    总之一旦发生gc后，就会导致所有工作线程全部停止，比如BlockManager，基于netty的网络通信。
    第二个stage的task去拉取数据的时候，上一个executor正好在进行gc，就导致拉取了半天也没拉取到数据，
    那为什么第二次提交stage的时候，就又可以了呢？
        因为第二次提交的时候，上一个executor已经完成了gc。
解决：
    spark.shuffle.io.maxRetries 3[默认3次]
        shuffle 文件拉取时，如果没有拉取到，最多或者重试几次，默认3次
    spark.shuffle.io.retryWait 5s [默认5s]
        每一次重新拉取文件的时间间隔，默认5s
    默认情况下，第一个stage的executor正在漫长的full gc，第二个stage的executor尝试去拉取数据，
    结果没拉取到，这样会反复重试拉取3次，中间间隔时间5s，也就是总共15s，拉取不成功，就报 shuffle file not found
        我们可以增大上面两个参数的值：
            spark.shuffle.io.maxRetries 60次
            spark.shuffle.io.retryWait 60s
            最多可以忍受一个小时没有拉取到shuffle file，这只是一个设置最大的可能值，
            full gc 也不可能一个小时都没结束把，
            这样就解决了因为gc 而无法拉取到数据的问题
</code></pre><h3 id="13-troubleshooting之解决yarn-cluster模式的JVM栈内存溢出问题"><a href="#13-troubleshooting之解决yarn-cluster模式的JVM栈内存溢出问题" class="headerlink" title="13. troubleshooting之解决yarn-cluster模式的JVM栈内存溢出问题"></a>13. troubleshooting之解决yarn-cluster模式的JVM栈内存溢出问题</h3><pre><code>yarn-cluster运行流程：
    1.本地机器执行spark-submit脚本[yarn-cluster模式]，提交spark application给resourceManager
    2. resourceManager找到一个节点[nodeManager]启动applicationMaster[Driver进程]
    3. applicationMaster找resourceManager申请executor
    4. resourceManager分配container(内存+cpu)
    5. applicationMaster找到对应nodeManager申请启动executor
    6. nodeManager启动executor
    7. executor找applicationMaster进行反向注册
    到这里为止，applicationMaster(Driver)就知道自己有哪些资源可以用(executor)，
    然后就会去执行job，拆分stage，提交stage的task，进行task调度，
    分配到各个executor上面去执行。
yarn-client 和 yarn-cluster的区别：
    yarn-client模式Driver运行在本地机器上；yarn-cluster模式Driver是运行在yarn集群上的某个nodeManager节点上的；
    yarn-client模式会导致本地机器负责spark作业的调用，所以网卡流量会激增，yarn-cluster没有这个问题；
    yarnclient的Driver运行在本地，通常来说本地机器和yarn集群都不会在一个机房，所以性能不是特别好；
    yarn-cluster模式下，Driver是跟yarn集群运行在一个机房内，性能上也会好很好；
实践经验碰到的yarn-cluster的问题：
    有时候运行了包含spark sql的spark作业，可能会遇到 在yarn-client上运行好好地，在yarn-cluster模式下，
    可能无法提交运行，会报出JVM的PermGen(永久代)的内存溢出-OOM；
    Yarn-client模式下，Driver是运行在本地机器的，spark使用的JVM的PerGen的配置，是本地的spark-class文件，
    (spark客户端是默认有配置的),JVM的永久代大小默认是128M，这个是没问题的；
    但是在Yarn-cluster模式下，Driver是运行在yarn集群的某个节点上的，使用的是没有经过配置的默认设置82M(PerGen永久代大小)
    spark sql内部会进行很负责的sl语义解析、语法树的转换，特别复杂，在这种情况下，如果sql特别复杂，
    很可能会导致性能的消耗，内存的消耗，可能对PermGen永久代的内存占比就很大
    所以此时，如果对PermGen的内存占比需求多与82M，但是又小于128M，就会出现类似上面的情况，
    yarn-client可以正常运行因为他的默认permgen大小是128M，但是yarn-cluster的默认是82M，就会出现PermGen OOM -- PermGen out of memory
解决：
    spark-submit脚本中加入参数：
        --conf spark.driver.extraJavaOptions=&apos;-XX:PermSize=128M -XX:MxPermSize=256M&apos;
        这样就设置了永久代的大小默认128M，最大256M，那么这样的话，就可以保证spark作业不会出现上面的PermGen out of memory
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-sparkstreaming&amp;kafka" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/01/sparkstreaming&kafka/" class="article-date">
  	<time datetime="2018-05-01T13:29:30.000Z" itemprop="datePublished">2018-05-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/01/sparkstreaming&kafka/">
        将offsets存储在HBase中
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>将offsets存储在HBase中</p>
<p>HBase可以作为一个可靠的外部数据库来持久化offsets。通过将offsets存储在外部系统中，Spark Streaming应用功能能够重读或者回放任何仍然存储在Kafka中的数据。</p>
<p>根据HBase的设计模式，允许应用能够以rowkey和column的结构将多个Spark Streaming应用和多个Kafka topic存放在一张表格中。在这个例子中，表格以topic名称、消费者group id和Spark Streaming 的batchTime.milliSeconds作为rowkey以做唯一标识。尽管batchTime.milliSeconds不是必须的，但是它能够更好地展示历史的每批次的offsets。表格将存储30天的累积数据，如果超出30天则会被移除。下面是创建表格的DDL和结构</p>
<p>对每一个批次的消息，使用saveOffsets()将从指定topic中读取的offsets保存到HBase中</p>
<p>在执行streaming任务之前，首先会使用getLastCommittedOffsets()来从HBase中读取上一次任务结束时所保存的offsets。该方法将采用常用方案来返回kafka topic分区offsets。</p>
<p>情形1：Streaming任务第一次启动，从zookeeper中获取给定topic的分区数，然后将每个分区的offset都设置为0，并返回。</p>
<p>情形2：一个运行了很长时间的streaming任务停止并且给定的topic增加了新的分区，处理方式是从zookeeper中获取给定topic的分区数，对于所有老的分区，offset依然使用HBase中所保存，对于新的分区则将offset设置为0。</p>
<p>情形3：Streaming任务长时间运行后停止并且topic分区没有任何变化，在这个情形下，直接使用HBase中所保存的offset即可。</p>
<p>在Spark Streaming应用启动之后如果topic增加了新的分区，那么应用只能读取到老的分区中的数据，新的是读取不到的。所以如果想读取新的分区中的数据，那么就得重新启动Spark Streaming应用。</p>
<p>当我们获取到offsets之后我们就可以创建一个Kafka Direct DStream</p>
<p>在完成本批次的数据处理之后调用saveOffsets()保存offsets.<br> 你可以到HBase中去查看不同topic和消费者组的offset数据</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-决策树算法及模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/07/决策树算法及模型/" class="article-date">
  	<time datetime="2018-04-07T04:29:30.000Z" itemprop="datePublished">2018-04-07</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/07/决策树算法及模型/">
        决策树算法及模型
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>决策树</strong></p>
<p>决策树（Decision Tree）是一种简单但是广泛使用的分类器。通过训练数据构建决策树，可以高效的对未知的数据进行分类。决策数有两大优点：1）决策树模型可以读性好，具有描述性，有助于人工分析；2）效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。</p>
<p><strong>如何预测</strong></p>
<p>先看看下面的数据表格：<br><img src="https://s1.ax1x.com/2018/05/08/CdDKU0.png" alt="CdDKU0.png"></p>
<p>上表根据历史数据，记录已有的用户是否可以偿还债务，以及相关的信息。通过该数据，构建的决策树如下：<br><img src="https://s1.ax1x.com/2018/05/08/CdD32F.png" alt="CdD32F.png"></p>
<p>比如新来一个用户：无房产，单身，年收入55K，那么根据上面的决策树，可以预测他无法偿还债务（蓝色虚线路径）。从上面的决策树，还可以知道是否拥有房产可以很大的决定用户是否可以偿还债务，对借贷业务具有指导意义。</p>
<p><strong>基本步骤</strong></p>
<p>决策树构建的基本步骤如下：</p>
<blockquote>
<p>1.开始，所有记录看作一个节点<br>2.遍历每个变量的每一种分割方式，找到最好的分割点<br>3.分割成两个节点N1和N2<br>4.对N1和N2分别继续执行2-3步，直到每个节点足够“纯”为止</p>
</blockquote>
<p><strong>决策树的变量可以有两种：</strong></p>
<p>1） 数字型（Numeric）：变量类型是整数或浮点数，如前面例子中的“年收入”。用“&gt;=”，“&gt;”,“&lt;”或“&lt;=”作为分割条件（排序后，利用已有的分割情况，可以优化分割算法的时间复杂度）。</p>
<p>2） 名称型（Nominal）：类似编程语言中的枚举类型，变量只能重有限的选项中选取，比如前面例子中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”。使用“=”来分割。</p>
<p>如何评估分割点的好坏？如果一个分割点可以将当前的所有节点分为两类，使得每一类都很“纯”，也就是同一类的记录较多，那么就是一个好分割点。比如上面的例子，“拥有房产”，可以将记录分成了两类，“是”的节点全部都可以偿还债务，非常“纯”；“否”的节点，可以偿还贷款和无法偿还贷款的人都有，不是很“纯”，但是两个节点加起来的纯度之和与原始节点的纯度之差最大，所以按照这种方法分割。构建决策树采用贪心算法，只考虑当前纯度差最大的情况作为分割点。</p>
<p><strong>量化纯度</strong></p>
<p>前面讲到，决策树是根据“纯度”来构建的，如何量化纯度呢？这里介绍三种纯度计算方法。如果记录被分为n类，每一类的比例P(i)=第i类的数目/总数目。还是拿上面的例子，10个数据中可以偿还债务的记录比例为P(1) = 7/10 = 0.7，无法偿还的为P(2) = 3/10 = 0.3，N = 2。<br><img src="https://s1.ax1x.com/2018/05/08/CdD0PK.png" alt="CdD0PK.png"></p>
<p><strong>纯度差</strong>，也称为信息增益（Information Gain），公式如下：<br><img src="https://s1.ax1x.com/2018/05/08/CdDD2D.png" alt="CdDD2D.png"></p>
<p>其中，I代表不纯度（也就是上面三个公式的任意一种），K代表分割的节点数，一般K = 2。vj表示子节点中的记录数目。上面公式实际上就是当前节点的不纯度减去子节点不纯度的加权平均数，权重由子节点记录数与当前节点记录数的比例决定。</p>
<p><strong>停止条件</strong></p>
<p>决策树的构建过程是一个递归的过程，所以需要确定停止条件，否则过程将不会结束。一种最直观的方式是当每个子节点只有一种类型的记录时停止，但是这样往往会使得树的节点过多，导致过拟合问题（Overfitting）。另一种可行的方法是当前节点中的记录数低于一个最小的阀值，那么就停止分割，将max(P(i))对应的分类作为当前叶节点的分类。</p>
<p><strong>过渡拟合</strong></p>
<p>采用上面算法生成的决策树在事件中往往会导致过滤拟合。也就是该决策树对训练数据可以得到很低的错误率，但是运用到测试数据上却得到非常高的错误率。过渡拟合的原因有以下几点：</p>
<p>噪音数据：训练数据中存在噪音数据，决策树的某些节点有噪音数据作为分割标准，导致决策树无法代表真实数据。<br>缺少代表性数据：训练数据没有包含所有具有代表性的数据，导致某一类数据无法很好的匹配，这一点可以通过观察混淆矩阵（Confusion Matrix）分析得出。<br>多重比较（Mulitple Comparition）：举个列子，股票分析师预测股票涨或跌。假设分析师都是靠随机猜测，也就是他们正确的概率是0.5。每一个人预测10次，那么预测正确的次数在8次或8次以上的概率为<br><img src="https://s1.ax1x.com/2018/05/08/CdDrxe.png" alt="CdDrxe.png"><br>，只有5%左右，比较低。但是如果50个分析师，每个人预测10次，选择至少一个人得到8次或以上的人作为代表，那么概率为<br><img src="https://s1.ax1x.com/2018/05/08/CdD6rd.png" alt="CdD6rd.png"><br>，概率十分大，随着分析师人数的增加，概率无限接近1。但是，选出来的分析师其实是打酱油的，他对未来的预测不能做任何保证。上面这个例子就是多重比较。这一情况和决策树选取分割点类似，需要在每个变量的每一个值中选取一个作为分割的代表，所以选出一个噪音分割标准的概率是很大的。</p>
<p><strong>优化方案1：修剪枝叶</strong></p>
<p>决策树过渡拟合往往是因为太过“茂盛”，也就是节点过多，所以需要裁剪（Prune Tree）枝叶。裁剪枝叶的策略对决策树正确率的影响很大。主要有两种裁剪策略。</p>
<p>前置裁剪 在构建决策树的过程时，提前停止。那么，会将切分节点的条件设置的很苛刻，导致决策树很短小。结果就是决策树无法达到最优。实践证明这中策略无法得到较好的结果。</p>
<p>后置裁剪 决策树构建好后，然后才开始裁剪。采用两种方法：1）用单一叶节点代替整个子树，叶节点的分类采用子树中最主要的分类；2）将一个字数完全替代另外一颗子树。后置裁剪有个问题就是计算效率，有些节点计算后就被裁剪了，导致有点浪费。</p>
<p><strong>优化方案2：K-Fold Cross Validation</strong></p>
<p>首先计算出整体的决策树T，叶节点个数记作N，设i属于[1,N]。对每个i，使用K-Fold Validataion方法计算决策树，并裁剪到i个节点，计算错误率，最后求出平均错误率。这样可以用具有最小错误率对应的i作为最终决策树的大小，对原始决策树进行裁剪，得到最优决策树。</p>
<p><strong>优化方案3：Random Forest</strong></p>
<p>Random Forest是用训练数据随机的计算出许多决策树，形成了一个森林。然后用这个森林对未知数据进行预测，选取投票最多的分类。实践证明，此算法的错误率得到了经一步的降低。这种方法背后的原理可以用“三个臭皮匠定一个诸葛亮”这句谚语来概括。一颗树预测正确的概率可能不高，但是集体预测正确的概率却很高。</p>
<p><strong>准确率估计</strong></p>
<p>决策树T构建好后，需要估计预测准确率。直观说明，比如N条测试数据，X预测正确的记录数，那么可以估计acc = X/N为T的准确率。但是，这样不是很科学。因为我们是通过样本估计的准确率，很有可能存在偏差。所以，比较科学的方法是估计一个准确率的区间，这里就要用到统计学中的置信区间（Confidence Interval）。</p>
<p>设T的准确率p是一个客观存在的值，X的概率分布为X ~ B(N,p)，即X遵循概率为p，次数为N的二项分布（Binomial Distribution），期望E(X) = N<em>p，方差Var(X) = N</em>p<em>(1-p)。由于当N很大时，二项分布可以近似有正太分布（Normal Distribution）计算，一般N会很大，所以X ~ N(np,n</em>p<em>(1-p))。可以算出，acc = X/N的期望E(acc) = E(X/N) = E(X)/N = p，方差Var(acc) = Var(X/N) = Var(X) / N2 = p</em>(1-p) / N，所以acc ~ N(p,p*(1-p)/N)。这样，就可以通过正太分布的置信区间的计算方式计算执行区间了。</p>
<p>正太分布的置信区间求解如下：</p>
<p>1） 将acc标准化，即<br><img src="https://s1.ax1x.com/2018/05/08/CdDRat.png" alt="CdDRat.png"></p>
<p>2） 选择置信水平α= 95%，或其他值，这取决于你需要对这个区间有多自信。一般来说，α越大，区间越大。</p>
<p>3） 求出 α/2和1-α/2对应的标准正太分布的统计量<br><img src="https://s1.ax1x.com/2018/05/08/CdD4G8.png" alt="CdD4G8.png"><br><img src="https://s1.ax1x.com/2018/05/08/CdD5RS.png" alt="CdD5RS.png"><br>（均为常量）。然后解下面关于p的不等式。acc可以有样本估计得出。即可以得到关于p的执行区间<br><img src="https://s1.ax1x.com/2018/05/08/CdDHqs.png" alt="CdDHqs.png"></p>
<p>another example:</p>
<h2 id="有一天，小明无聊，对宿舍玩CS的舍友进行统计，结果刚记下四行，被舍友认为影响发挥，给踢到床下去了，让我们看看可怜的小明的记录："><a href="#有一天，小明无聊，对宿舍玩CS的舍友进行统计，结果刚记下四行，被舍友认为影响发挥，给踢到床下去了，让我们看看可怜的小明的记录：" class="headerlink" title="有一天，小明无聊，对宿舍玩CS的舍友进行统计，结果刚记下四行，被舍友认为影响发挥，给踢到床下去了，让我们看看可怜的小明的记录："></a>有一天，小明无聊，对宿舍玩CS的舍友进行统计，结果刚记下四行，被舍友认为影响发挥，给踢到床下去了，让我们看看可怜的小明的记录：</h2><h2 id="武器-子弹数量-血-行为"><a href="#武器-子弹数量-血-行为" class="headerlink" title="武器 | 子弹数量 | 血 | 行为"></a>武器 | 子弹数量 | 血 | 行为</h2><p>机枪 |    多    | 少 | 战斗<br>机枪 |    少    | 多 | 逃跑<br>小刀 |    少    | 多 | 战斗</p>
<h2 id="小刀-少-少-逃跑"><a href="#小刀-少-少-逃跑" class="headerlink" title="小刀 |    少    | 少 | 逃跑"></a>小刀 |    少    | 少 | 逃跑</h2><p>为了对得起小明记录的这四条记录，我们对其进行决策树分析，从数据中看：</p>
<blockquote>
<ol>
<li>如果一个玩家子弹很多，那么即使血少他也会战斗，如果子弹少的话，即使血多，他也会逃跑隐蔽起来；</li>
<li>那我们再看子弹少的情况下，武器靠刀子，当血多时候，他还是会打一打得，但是血少，就立即逃跑隐蔽了。</li>
</ol>
</blockquote>
<p>这是我们大脑直觉上去分析，既然本文我是想聊一聊决策树，那么我们就用决策树来对小明的这些数据小试牛刀一下，顺便来慰藉一下小明（从小到大我们已经看过无数的小明了，这里再借用一下大度的小明）。</p>
<p>我们现在将数据分为两块：</p>
<blockquote>
<p>X = {武器类型，子弹数量，血}<br>Y = {行为}<br>我们建立这颗决策树的目的就是，让计算机自动去寻找最合适的映射关系，即：Y = f(X)，所谓听上去大雅的“数据挖掘”学科，干得也差不多就是这回事，X我们称之为样本，Y我们称之为结果（行为/类）。</p>
</blockquote>
<p>样本是多维的，X = {x1,x2,…xn}，如本例：X = {x1=武器类型，x2=子弹数量，x3=血}，我们就是要通过这些不同维度的观测记录数据，和应对的不同结果，找到规律（映射关系），举个例子：</p>
<blockquote>
<p>X = {天气，温度，湿度，女友约会} -&gt; Y = {是否答应兄弟下午去打篮球}<br>X = {老妈说你是胖子，老婆说你是胖子，自己上秤评估自己体重} -&gt; Y = {去办健身卡减肥}</p>
</blockquote>
<p>这样来说，X的多维不同的数据，大个比方，更像是很多大臣，那么我们就是要根据这些大臣的意见，来决策，如本例：</p>
<blockquote>
<p>左大臣：武器类型<br>中大臣：子弹数量<br>右大臣：血</p>
</blockquote>
<p>这些大臣每个人都有想法，左右着皇帝继续战斗还是撤退，但是三个也不能全信，那么我们就要根据他们的陈年老帐（训练样本）来评判他们的话语的重要性，当然，优先级高的肯定话语是有重量的，我们先提前来预览一下这个例子训练出来的决策树的样子：<br><img src="https://s1.ax1x.com/2018/05/08/CdrCL9.png" alt="CdrCL9.png"></p>
<p>这个根据小明的数据训练出来的决策树是不是和我们刚才拍脑门分析出来的结果差不多呢？看，子弹多就开打，子弹少，在看看用什么武器，如果又没子弹又用机枪，那铁定跑，如果用小刀，在掂量一下自己血厚不厚，厚则打，不厚则逃，看来决策树分析的结果还是可以的啊,接下来，我们来研究研究，计算机（这个只会重复人们给它设定的代码的家伙）是如何实现这样的分析的。</p>
<p>既然是三个大臣提意见{左大臣：武器类型，中大臣：子弹数量，右大臣：血}，那么我们要分析一下历史数据（训练数据）他们哪个话更靠谱：</p>
<p>我们先单纯的看看左大臣的历史战绩（统计训练样本）：</p>
<blockquote>
<p>机枪 -&gt; 战斗<br>机枪 -&gt; 逃跑<br>小刀 -&gt; 战斗<br>小刀 -&gt; 逃跑<br>用机枪，你战斗逃跑的概率都是50%，用刀子，你亦似打似逃！看来这个大臣立场不坚定啊！</p>
</blockquote>
<p>再看看中大臣的：</p>
<blockquote>
<p>子弹多 -&gt; 战斗<br>子弹少 -&gt; 逃跑<br>子弹少 -&gt; 战斗<br>子弹少 -&gt; 逃跑<br>用机枪，你战斗概率是100%，用刀子，你33.3%打，你66.6%撤！这位大臣似乎坚定了一些。</p>
</blockquote>
<p>再看看右大臣的：</p>
<blockquote>
<p>血少 -&gt; 战斗<br>血多 -&gt; 逃跑<br>血多 -&gt; 战斗<br>血少 -&gt; 逃跑<br>和左大臣一样，立场不坚定，50:50啊！</p>
</blockquote>
<p>这样，中大臣的话的重量就提升了，因此决策书的第一层就重用中大臣吧（中大臣变成一品大员）</p>
<p>计算机是怎么来做到这一步的呢？且让我一步一步讲：</p>
<p>决策树训练中，有一个很重要的尺子，来衡量大臣的可信度，这个尺子，就是信息论的熵(Entropy)，这个熵是何许人也，竟然朝廷大臣的可信度竟然用次来衡量，让我们对他做个自我介绍吧：<br>熵，洋名为（Entropy），乃测量信息的混乱程度为职，纵横科学界各门学术之中，为人低调，俭朴，就一个很短的公式：E = sum(-p(I)*log(p(I)))，I=1:N（N类结果，如本例两种，战斗或逃跑），当信息一致，所有样本都属于一个类别I，那么熵为0，如果样本完全随机，那么熵为1，表明这个臣子对这种状态的预测就是胡言乱语。</p>
<p>OK，熵，告诉我你对这个数据的看法：</p>
<blockquote>
<p>E(机枪) = -(1/2)Log2(1/2) - (1/2)Log(1/2) = 0.5 + 0.5 = 1<br>E(小刀) = -(1/2)Log2(1/2) - (1/2)Log(1/2) = 0.5 + 0.5 = 1<br>E(子弹多) = -(1/1)Log2(1/1) - (0/1)Log(0/1) = 0 + 0 = 0<br>E(子弹少) = -(1/3)Log2(1/3) - (2/3)Log(2/3) = 0.5283 + 0.39 = 0.9183<br>E(血多) = -(1/2)Log2(1/2) - (1/2)Log(1/2) = 0.5 + 0.5 = 1<br>E(血少) = -(1/2)Log2(1/2) - (1/2)Log(1/2) = 0.5 + 0.5 = 1</p>
</blockquote>
<p>那么我们怎么用这个熵来衡量大臣（每维数据）的可信度呢，这里还要再引出一位仁兄，其是熵的上级，他熟知熵的能力，很会用熵，他就是信息增益(Information Gain)，我们来看看这位上级是如何用熵来衡量的：<br>Gain(Sample,Action) = E(sample) - sum(|Sample(v)|/Sample * E(Sample(v)))</p>
<p>OK，Information Gain，说说你是怎么评估这个例子的三位大臣的！</p>
<blockquote>
<p>Gain(武器类型) = E(S) - (2/4)<em>E(机枪) - (2/4)</em>E(小刀) = 1 - (2/4)<em>1 - (2/4)</em>1 = 0<br>Gain(子弹数量) = E(S) - (1/4)<em>E(子弹多) - (3/4)</em>E(子弹少) = 1 - (1/4)<em>0 - (3/4)</em>0.9183 = 0.3113<br>Gain(血量) = E(S) - (2/4)<em>E(血多) - (2/4)</em>E(血少) = 1 - (2/4)<em>1 - (2/4)</em>1 = 0</p>
</blockquote>
<p>接着，计算机通过信息增益结果，选择最大的，作为一品大员<br><img src="https://s1.ax1x.com/2018/05/08/CdrEi6.png" alt="CdrEi6.png"></p>
<p>且看一品大员对子弹多的情况下料事如神（暂且不说本例样本少），但是其在子弹少的情况下，决策还是不行的，那么，再用同样的方法，再去选择二品，三品，这就是决策树的训练。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-电商分析指标" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/电商分析指标/" class="article-date">
  	<time datetime="2018-04-05T13:29:30.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/电商分析指标/">
        电商常见分析指标
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>电商常见分析指标<br>信息流、物流和资金流三大平台是电子商务的三个最为重要的平台。而电子商务信息系统最核心的能力是大数据能力，包括大数据处理、数据分析和数据挖掘能力。无论是电商平台（如淘宝）还是在电商平台上销售产品的卖家，都需要掌握大数据分析的能力。越成熟的电商平台，越需要以通过大数据能力驱动电子商务运营的精细化，更好的提升运营效果，提升业绩。构建系统的电子商务数据分析指标体系是数据电商精细化运营的重要前提，本文将重点介绍电商数据分析指标体系。<br>电商数据分析指标体系分为八大类指标，包括总体运营指标、网站流量累指标、销售转化指标、客户价值指标、商品及供应链指标、营销活动指标、风险控制指标和市场竞争指标。不同类别指标对应电商运营的不同环节，如网站流量指标对应的是网站运营环节，销售转化、客户价值和营销活动指标对应的是电商销售环节。<br>1.电商总体运营指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaO3t.png" alt="CYaO3t.png"></p>
<p>电商总体运营整体指标主要面向的人群电商运营的高层，通过总体运营指标评估电商运营的整体效果。电商总体运营整体指标包括四方面的指标：</p>
<p>（1）流量类指标</p>
<p>独立访客数（UV），指访问电商网站的不重复用户数。对于PC网站，统计系统会在每个访问网站的用户浏览器上“种”一个cookie来标记这个用户，这样每当被标记cookie的用户访问网站时，统计系统都会识别到此用户。在一定统计周期内如（一天）统计系统会利用消重技术，对同一cookie在一天内多次访问网站的用户仅记录为一个用户。而在移动终端区分独立用户的方式则是按独立设备计算独立用户。</p>
<p>页面访问数（PV），即页面浏览量，用户每一次对电商网站或着移动电商应用中的每个网页访问均被记录一次，用户对同一页面的多次访问，访问量累计。<br>人均页面访问数，即页面访问数（PV）／独立访客数，该指标反映的是网站访问粘性。</p>
<p>（2）订单产生效率指标</p>
<p>总订单数量，即访客完成网上下单的订单数之和。<br>访问到下单的转化率，即电商网站下单的次数与访问该网站的次数之比。</p>
<p>（3）总体销售业绩指标</p>
<p>网站成交额（GMV），电商成交金额，即只要网民下单，生成订单号，便可以计算在GMV里面。<br>销售金额。销售金额是货品出售的金额总额。</p>
<p>注：无论这个订单最终是否成交，有些订单下单未付款或取消，都算GMV，销售金额一般只指实际成交金额，所以，GMV的数字一般比销售金额大。</p>
<p>客单价，即订单金额与订单数量的比值。</p>
<p>（4）整体指标</p>
<p>销售毛利，是销售收入与成本的差值。销售毛利中只扣除了商品原始成本，不扣除没有计入成本的期间费用（管理费用、财务费用、营业费用）。<br>毛利率，是衡量电商企业盈利能力的指标，是销售毛利与销售收入的比值。如京东的2014年毛利率连续四个季度稳步上升，从第一季度的10.0％上升至第四季度的12.7％，体现出京东盈利能力的提升。</p>
<p>2.网站流量指标<br><img src="https://s1.ax1x.com/2018/05/03/CYa6AJ.png" alt="CYa6AJ.png"></p>
<p>（1）流量规模类指标<br>常用的流量规模类指标包括独立访客数和页面访问数，相应的指标定义在前文（电商总体运营指标）已经描述，在此不在赘述</p>
<p>（2）流量成本累指标<br>单位访客获取成本。该指标指在流量推广中，广告活动产生的投放费用与广告活动带来的独立访客数的比值。单位访客成本最好与平均每个访客带来的收入以及这些访客带来的转化率进行关联分析。若单位访客成本上升，但访客转化率和单位访客收入不变或下降，则很可能流量推广出现问题，尤其要关注渠道推广的作弊问题。</p>
<p>（3）流量质量类指标<br>跳出率（Bounce Rate）也被称为蹦失率，为浏览单页即退出的次数/该页访问次数，跳出率只能衡量该页做为着陆页面（LandingPage）的访问。如果花钱做推广，着落页的跳出率高，很可能是因为推广渠道选择出现失误，推广渠道目标人群和和被推广网站到目标人群不够匹配，导致大部分访客来了访问一次就离开。</p>
<p>页面访问时长。页访问时长是指单个页面被访问的时间。并不是页面访问时长越长越好，要视情况而定。对于电商网站，页面访问时间要结合转化率来看，如果页面访问时间长，但转化率低，则页面体验出现问题的可能性很大。</p>
<p>人均页面浏览量。人均页面浏览量是指在统计周期内，平均每个访客所浏览的页面量。人均页面浏览量反应的是网站的粘性。</p>
<p>（4）会员类指标</p>
<p>注册会员数。指一定统计周期内的注册会员数量。<br>活跃会员数。活跃会员数，指在一定时期内有消费或登录行为的会员总数。<br>活跃会员率。即活跃会员占注册会员总数的比重。<br>会员复购率。指在统计周期内产生二次及二次以上购买的会员占购买会员的总数。<br>会员平均购买次数。指在统计周期内每个会员平均购买的次数，即订单总数/购买用户总数。会员复购率高的电商网站平均购买次数也高。<br>会员回购率。指上一期末活跃会员在下一期时间内有购买行为的会员比率。<br>会员留存率。会员在某段时间内开始访问你的网站，经过一段时间后，仍然会继续访问你的网站就被认作是留存，这部分会员占当时新增会员的比例就是新会员留存率，这种留存的计算方法是按照活跃来计算，另外一种计算留存的方法是按消费来计算，即某段的新增消费用户在往后一段时间时间周期（时间周期可以是日、周、月、季度和半年度）还继续消费的会员比率。留存率一般看新会员留存率，当然也可以看活跃会员留存。留存率反应的是电商留住会员的能力。</p>
<p>3.网站销售（转化率）类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaghR.png" alt="CYaghR.png"></p>
<p>（1）购物车类指标<br>基础类指标，包括一定统计周期内加入购物车次数、加入购物车买家数、加入购物车买家数以及加入购物车商品数。<br>转化类指标，主要是购物车支付转化率，即一定周期内加入购物车商品支付买家数与加入购物车购买家数的比值。</p>
<p>（2）下单类指标<br>基础类指标，包括一定统计周期内的下单笔数、下单金额以及下单买家数。<br>转化类指标，主要是浏览下单转化率，即下单买家数与网站访客数（UV）的比值。</p>
<p>（3）支付类指标<br>基础统计类指标，包括一定统计周期内支付金额、支付买家数和支付商品数。<br>转化类指标。包括浏览-支付买家转化率（支付买家数/网站访客数）、下单-支付金额转化率（支付金额/下单金额）、下单-支付买家数转化率（支付买家数/下单买家数）和下单-支付时长（下单时间到支付时间的差值）。</p>
<p>4.客户价值类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaW1x.png" alt="CYaW1x.png"></p>
<p>客户指标。常见客户指标包括一定统计周期内的累计购买客户数和客单价。客单价是指每一个客户平均购买商品的金额，也即是平均交易金额，即成交金额与成交用户数的比值。</p>
<p>新客户指标。常见新客户指标包括一定统计周期内的新客户数量、新客户获取成本和新客户客单价。其中，新客户客单价是指第一次在店铺中产生消费行为的客户所产生交易额与新客户数量的比值。影响新客户客单价的因素除了与推广渠道的质量有关系，还与电商店铺活动以及关联销售有关。</p>
<p>老客户指标。常见老客户指标包括消费频率、最近一次购买时间、消费金额和重复购买率。消费频率是指客户在一定期间内所购买的次数；最近一次购买时间表示客户最近一次购买的时间离现在有多远；客户消费金额指客户在最近一段时间内购买的金额。消费频率越高，最近一次购买时间离现在越近，消费金额越高的客户越有价值。重复购买率则指消费者对该品牌产品或者服务的重复购买次数，重复购买率越多，则反应出消费者对品牌的忠诚度就越高，反之则越低。重复购买率可以按两种口径来统计：第一种，从客户数角度，重复购买率指在一定周期内下单次数在两次及两次以上的人数与总下单人数之比，如在一个月内，有100个客户成交，其中有20个是购买两次及以上，则重复购买率为20%；第二种，按交易计算，即重复购买交易次数与总交易次数的比值，如某月内，一共产生了100笔交易，其中有20个人有了二次购买，这20人中的10个人又有了三次购买，则重复购买次数为30次，重复购买率为30%。</p>
<p>5.商品类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYafc6.png" alt="CYafc6.png"></p>
<p>产品总数指标。包括SKU、SPU和在线SPU。SKU是物理上不可分割的最小存货单位。SPU即Standard Product Unit （标准化产品单元），SPU是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。通俗点讲，属性值、特性相同的商品就可以称为一个SPU。如iphone5S是一个SPU，而iPhone 5S配置为16G版、4G手机、颜色为金色、网络类型为TD-LTE/TD-SCDMA/WCDMA/GSM则是一个SKU。在线SPU则是在线商品的SPU数。</p>
<p>产品优势性指标。主要是独家产品的收入占比，即独家销售的产品收入占总销售收入的比例。</p>
<p>品牌存量指标。包括品牌数和在线品牌数指标。品牌数指商品的品牌总数量。在线品牌数则指在线商品的品牌总数量。</p>
<p>上架。包括上架商品SKU数、上架商品SPU数、上架在线SPU数、上架商品数和上架在线商品数。</p>
<p>首发。包括首次上架商品数和首次上架在线商品数。</p>
<p>6.市场营销活动指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaIBD.png" alt="CYaIBD.png"></p>
<p>市场营销活动指标。包括新增访问人数、新增注册人数、总访问次数、订单数量、下单转化率以及ROI。其中，下单转化率是指活动期间，某活动所带来的下单的次数与访问该活动的次数之比。投资回报率（ROI）是指，某一活动期间，产生的交易金额与活动投放成本金额的比值。</p>
<p>广告投放指标。包括新增访问人数、新增注册人数、总访问次数、订单数量、UV订单转化率、广告投资回报率。其中，下单转化率是指某广告所带来的下单的次数与访问该活动的次数之比。投资回报率（ROI）是指，某广告产生的交易金额与广告投放成本金额的比值。</p>
<p>7.风控类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYa7AH.png" alt="CYa7AH.png"></p>
<p>买家评价指标。包括买家评价数，买家评价卖家数、买家评价上传图片数、买家评价率、买家好评率以及卖家差评率。其中，买家评价率是指某段时间参与评价的卖家与该时间段买家数量的比值，是反映用户对评价的参与度，电商网站目前都在积极引导用户评价，以作为其他买家购物时候的参考。买家好评率指某段时间内好评的买家数量与该时间段买家数量的比值。同样，买家差评率指某段时间内差评的买家数量与该时间段买家数量的比值。尤其是买家差评率，是非常值得关注的指标，需要监控起来，一旦发现买家差评率在加速上升，一定要提高警惕，分析引起差评率上升的原因，及时改进。</p>
<p>买家投诉类指标。包括发起投诉（或申诉），撤销投诉（或申诉），投诉率（买家投诉人数占买家数量的比例）等。投诉量和投诉率都需要及时监控，以发现问题，及时优化。</p>
<p>8.市场竞争类指标<br><img src="https://s1.ax1x.com/2018/05/03/CYaHNd.png" alt="CYaHNd.png"></p>
<p>市场份额相关指标，包括市场占有率、市场扩大率和用户份额。市场占有率指电商网站交易额占同期所有同类型电商网站整体交易额的比重；市场扩大率指购物网站占有率较上一个统计周期增长的百分比；用户份额指购物网站独立访问用户数占同期所有B2C购物网站合计独立访问用户数的比例。</p>
<p>网站排名，包括交易额排名和流量排名。交易额排名指电商网站交易额在所有同类电商网站中的排名；流量排名指电商网站独立访客数量在所有同类电商网站中的排名。</p>
<p>总之，本文介绍了电商数据分析的基础指标体系，涵盖了流量、销售转化率、客户价值、商品类目、营销活动、风控和市场竞争指标，这些指标都需要系统化的进行统计和监控，才能更好的发现电商运营健康度的问题，以更好及时改进和优化，提升电商收入。如销售转化率，其本质上是一个漏斗模型，如从网站首页到最终购买各个阶段的转化率的监控和分析是网站运营健康度很重要的分析方向。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-HBASE：出自Google论文的发表" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/HBASE：出自Google论文的发表/" class="article-date">
  	<time datetime="2018-04-05T13:29:30.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/HBASE：出自Google论文的发表/">
        HBASE总结：出自Google论文的发表
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HBASE：出自Google论文的发表"><a href="#HBASE：出自Google论文的发表" class="headerlink" title="HBASE：出自Google论文的发表"></a>HBASE：出自Google论文的发表</h2><h6 id="列式存储、多版本-timestamp-的、nosql数据库"><a href="#列式存储、多版本-timestamp-的、nosql数据库" class="headerlink" title="[列式存储、多版本(timestamp)的、nosql数据库]"></a>[列式存储、多版本(timestamp)的、nosql数据库]</h6><h6 id="hbase架构图：要能手画出来"><a href="#hbase架构图：要能手画出来" class="headerlink" title="hbase架构图：要能手画出来"></a>hbase架构图：要能<strong>手画</strong>出来</h6><p><img src="http://m.qpic.cn/psb?/V11BXxlE33Kp9F/ynWG7DnR777pXkx0IGWDtEit4H6y7VkXIgYmJDtxY48!/b/dPMAAAAAAAAA&amp;bo=EAUIAwAAAAARBy4!&amp;rf=viewer_4" alt=""></p>
<h6 id="HBase没有单节点故障-因为zookeeper-，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作"><a href="#HBase没有单节点故障-因为zookeeper-，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作" class="headerlink" title="**HBase没有单节点故障(因为zookeeper)，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作"></a>**HBase没有单节点故障(因为zookeeper)，可以有多个Hmaster，跟namenode一样，同一时刻只有一个hmaster在工作</h6><h4 id="HBase的功能："><a href="#HBase的功能：" class="headerlink" title="HBase的功能："></a>HBase的功能：</h4><pre><code>*hadoop数据库:
    1.存储数据 
    2.检索数据
*和RDBMS[关系型数据库]相比：
    1.海量数据：数据条目数--上亿
    2.检索的速度：准时性、秒级别
*基于hdfs： hdfs的优势
    1.数据安全性[副本机制]
    2.普通商用PC server就ok
</code></pre><h5 id="1-Table中的所有行都是按照rowkey的字典序排列"><a href="#1-Table中的所有行都是按照rowkey的字典序排列" class="headerlink" title="1.Table中的所有行都是按照rowkey的字典序排列"></a>1.Table中的所有行都是按照rowkey的字典序排列</h5><h5 id="2-Table在行的方向上分割为多个Region"><a href="#2-Table在行的方向上分割为多个Region" class="headerlink" title="2.Table在行的方向上分割为多个Region"></a>2.Table在行的方向上分割为多个Region</h5><h5 id="3-Region是按照大小分割的，每个表开始只有一个region-会有一个starkey和endkey-前毕后包-，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。"><a href="#3-Region是按照大小分割的，每个表开始只有一个region-会有一个starkey和endkey-前毕后包-，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。" class="headerlink" title="3.Region是按照大小分割的，每个表开始只有一个region[会有一个starkey和endkey,前毕后包]，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。"></a>3.Region是按照大小分割的，每个表开始只有一个region[会有一个starkey和endkey,前毕后包]，随着数据增多，region不断增大，当增大到一定阀值时，region就会等分为两个新的region，之后会有越来越多的region。</h5><h5 id="4-Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上"><a href="#4-Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上" class="headerlink" title="4.Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上."></a>4.Region是HBase中分布式存储和负载均衡的最小单位，不同region分不到不同的RegionServer上.</h5><h5 id="5-Region虽然是分布式存储的最小单元，但是不是存储的最小单元。"><a href="#5-Region虽然是分布式存储的最小单元，但是不是存储的最小单元。" class="headerlink" title="5.Region虽然是分布式存储的最小单元，但是不是存储的最小单元。"></a>5.Region虽然是分布式存储的最小单元，但是不是存储的最小单元。</h5><pre><code>存储的最小单元是cell，{rowkey,column,version}
        *唯一性
        *数据没有类型，以字节码形式存储

region由一个或多个store组成，每个store保存一个column family；
每个store又由一个memStore和0至多个storeFile组成；
memStore存储在内存中，storeFile存储在hdfs上
</code></pre><h6 id="hbase-旧版本的web监听端口-60010，新版本是16010"><a href="#hbase-旧版本的web监听端口-60010，新版本是16010" class="headerlink" title="hbase 旧版本的web监听端口 60010，新版本是16010"></a>hbase 旧版本的web监听端口 60010，新版本是16010</h6><h3 id="HBASE数据写入流程："><a href="#HBASE数据写入流程：" class="headerlink" title="HBASE数据写入流程："></a>HBASE数据写入流程：</h3><pre><code>put ---&gt; cell
    step0： 先进行记录 ---&gt;HLog 【WAL（write ahead logging:日志预写功能）】 loc：hdfs
            存储的log数据类型是sequenceFile
</code></pre><p>——- WAL的作用是防止写入内存时，region挂掉，数据丢失</p>
<pre><code>step1： 写入memStore     loc：内存
step2： 当内存达到一定阈值时，写入storeFile  ---&gt; loc： hdfs
</code></pre><h3 id="HBASE-用户读取数据流程："><a href="#HBASE-用户读取数据流程：" class="headerlink" title="HBASE 用户读取数据流程："></a>HBASE 用户读取数据流程：</h3><pre><code>1)先到memStore里面去读
2)然后到BlockCache里面读 --&gt; *每个RegionServer只有一个BlockCache*
3)最后到HFile里面读取数据
4)然后对数据进行Merge --&gt; 最后返回数据集
</code></pre><h4 id="MemSore和BlockCache"><a href="#MemSore和BlockCache" class="headerlink" title="MemSore和BlockCache"></a>MemSore和BlockCache</h4><pre><code>*HBase上的RegionServer的内存分为两部分：
    1.Memstore --&gt; 用来 写
        写请求会先写入Memstore，RegionServer会给每个Region提供一个Memstore，当Memstore写满64M以后，会启动flush舒心到磁盘。
    2.BlockCache --&gt; 用来读
        读请求先到Memstore中查询数据，查不到就到BlockCache中查询，再差不读奥就会到磁盘上读，并把读的结果放入BlockCache。
        BlockCache达到上限后，会启动淘汰机制，淘汰掉最老的一批数据
*在注重读取响应时间的场景下，可以将BlockCache设置大些，Memstore设置小些，以加大缓存的命中率。
</code></pre><h3 id="数据检索的三种方式："><a href="#数据检索的三种方式：" class="headerlink" title="数据检索的三种方式："></a>数据检索的三种方式：</h3><pre><code>1.get rowkey --- 最快的方式
2.scan range --- 用的最多的方式 【一般先range扫描，然后get】
3.scan ---全表扫描，基本不用
</code></pre><h4 id="HBase中有类似于RDBMS中Database的概念"><a href="#HBase中有类似于RDBMS中Database的概念" class="headerlink" title="HBase中有类似于RDBMS中Database的概念"></a>HBase中有类似于RDBMS中Database的概念</h4><pre><code>命名空间：
    *用户自定义的表的命名空间，默认情况下 ---&gt;  default
    *系统自带的元数据的表的命名空间 ---&gt; hbase
</code></pre><h3 id="HBase数据迁移"><a href="#HBase数据迁移" class="headerlink" title="HBase数据迁移"></a>HBase数据迁移</h3><pre><code>使用 MapReduce把文件生成hdfs上的HFile，然后进行bulk load into hbase table
</code></pre><h1 id="HBase表RowKey的设计-："><a href="#HBase表RowKey的设计-：" class="headerlink" title="**HBase表RowKey的设计**："></a><strong>**</strong>HBase表RowKey的设计<strong>**</strong>：</h1><h5 id="现实环境中出现的问题："><a href="#现实环境中出现的问题：" class="headerlink" title="**现实环境中出现的问题："></a>**现实环境中出现的问题：</h5><pre><code>*默认情况下创建一张HBase表，自动会为表创建一个Region [startkey,endkey) 前毕后包
    *无论在测试环境还是生产环境中，创建完表以后会往表中导入大量数据
        步骤：
            file/datas -&gt; HFile -&gt; bulk load into hbase table
        此时Region只有一个，而region是被RegionServer管理的，
        当导入数据量慢慢增大后Region就会被split成两个Region，
        但是此时RegionServer很可能就会挂掉，此时就会出现问题...

解决方案[举例]：
    创建表时，多创建一些Region(依据表的rowkey进行设计 + 结合业务)
        比如说：有5个region，他们被多个RegionServer管理
            再插入数据时，会向5个Region中分别插入数据，这样就均衡了
</code></pre><h5 id="具体解决方案："><a href="#具体解决方案：" class="headerlink" title="**具体解决方案："></a>**具体解决方案：</h5><p>表的RowKey设计中：</p>
<pre><code>**如何在海量数据中，查询到我想要的数据???
    核心思想：
        1.依据RowKey查询最快
        2.对RowKey进行范围查询range
        3.前缀查询  
            比如：startkey:15221467820_20180409000053 和 endkey:15221467828_20180419000053
            这时只会比较15221467820 和 15221467828 而后面的_2018.. 则不会进行匹配    
</code></pre><p>解决方法：</p>
<pre><code>a).HBase的预分区：
    Region的划分依赖于RowKey，预先预估一些RowKey(年月日时分秒)[最常用的就是这两种，其他的方法rowkey设计都不能自定义]
        1. create &apos;tb1&apos;,&apos;cf1&apos;,splits =&gt; [&apos;20180409000000&apos;,&apos;20180419000000&apos;,&apos;20180429000000&apos;]
        2. create &apos;tb1&apos;,&apos;cf1&apos;,splits_file =&gt; &apos;/root/data/logs-split.txt&apos;
           [这种情况就是把分段的rowkey写入了文件里]
    这样就分为了4个Region，因为它会把头和尾也算进去
b).根据业务来对RowKey的设计：
        举例：移动运营商电话呼叫查询详单：
            RowKey: 
                phone + time 
                15221467820_20180409010000
            infor: 
                上海  主叫   152216888888   30        国内通话 0.00
                area active opposite_phone talk_time model price 
        依据查询条件：
            phone + (start_time - end_time)
        代码：
            scan 
                startrow
                    15221467820_20180409000000
                stoprow
                    15221467820_20180419000000
    保证了实时性
</code></pre><h3 id="HBase数据存储二级索引-索引表-的设计以及数据同步解决方案："><a href="#HBase数据存储二级索引-索引表-的设计以及数据同步解决方案：" class="headerlink" title="HBase数据存储二级索引[索引表]的设计以及数据同步解决方案："></a>HBase数据存储二级索引[索引表]的设计以及数据同步解决方案：</h3><p>基于以上的移动运营商电话呼叫查询详单：</p>
<pre><code>二级索引表：
    RowKey:
        opposite_phone
    columnFamily对应的colume:
        主表的RowKey
主表和索引表的同步：
    &gt;&gt; phoenix 
            &gt;&gt; jdbc 方式才能同步
    &gt;&gt;先在solr里创建索引表
     solr   //在solr中创建索引表
            国外有个框架交 lily
                cloudera search 封装了lily，只要配置下cloudera
                cloudera search会在主表插入数据时，自动更新solr里的索引表
</code></pre><h2 id="HBase之-表的属性"><a href="#HBase之-表的属性" class="headerlink" title="HBase之 表的属性:"></a>HBase之 表的属性:</h2><h3 id="面试会问到：-每个RegionServer只有一个HLog和一个BlockCache"><a href="#面试会问到：-每个RegionServer只有一个HLog和一个BlockCache" class="headerlink" title="面试会问到：**每个RegionServer只有一个HLog和一个BlockCache"></a>面试会问到：**每个RegionServer只有一个HLog和一个BlockCache</h3><pre><code>1.HBase的表的压缩[HFile compression]--表的属性:
**在企业中通常使用snappy格式进行压缩，可以减少容量的存储，和减少io流，提高传输效率
    跟hive一样，需要一些编译jar包，然后放到lib的native里
    [这里使用软连接 ln -s /xx/xx /xx  具体看官方文档]，还有一些参数的配置，才能使用
        创建表的时候：
            create &apos;user&apos;,{NAME=&apos;cf1&apos;,COMPRESSION=&gt;&apos;SNAPPY&apos;}
2. HBase 标的属性：IN_MEMORY=&gt;&apos;false&apos;、BLOCKCACHE=&gt;&apos;true&apos;
        HBase为什么查询这么快 就是因为这些表的属性的存储机制！！
        通常我们自定义创建的表的相应列簇下的IN_MEMORY属性都是false!
        通过describe &apos;hbase:meta&apos;可以查询到 IN_MEMORY=&gt;&apos;true&apos; BLOCKCACHE =&gt; &apos;true&apos;,
        意为meta元数据存放于blockcache的inmemory内存中
            IN_MEMORY队列存放HBase的meta表元数据信息，因为meta存储了用户表的
            通常 BLOCKCACHE 这个属性的设置要慎用，比如我们查询后的数据后续还会用到这时可以设置为true
            如果后续用不到这些数据就要设置此属性为false
</code></pre><h4 id="HBase的Cache分等级："><a href="#HBase的Cache分等级：" class="headerlink" title="HBase的Cache分等级："></a>HBase的Cache分等级：</h4><pre><code>默认情况下：整个BlockCache的内存分配 
1.IN_MEMORY 1/4    in_memory 用于存储hbase的meta元数据信息    2.Single 1/4   single就是用的次数较少的数据
3.Muti    1/2          muti就是用的次数较多的数据
如果BlockCache的内存到达阈值，会先清理single然后清理muti，in_memory的内存不会被清理!
</code></pre><h3 id="HBase表的compaction"><a href="#HBase表的compaction" class="headerlink" title="HBase表的compaction"></a>HBase表的compaction</h3><h6 id="随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种："><a href="#随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种：" class="headerlink" title="随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种："></a>随着写入数据的不断增加，hbase会把小的hfile进行合并，以以减少IO查询次数，合并有两种：</h6><pre><code>1.minor compaction
    Minor Compaction是指选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，
    在这个过程中不会处理已经Deleted或Expired的Cell。一次Minor Compaction的结果是更少并且更大的StoreFile。
2.major compaction
    Major Compaction是指将所有的StoreFile合并成一个StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL过期数据、版本号超过设定版本号的数据。另外，一般情况下，Major Compaction时间会持续比较长，会阻塞数据的吸入和查询!!!，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此线上业务都会将关闭自动触发Major Compaction功能，改为手动在业务低峰期触发。
</code></pre><h4 id="HBase和Hive集成："><a href="#HBase和Hive集成：" class="headerlink" title="HBase和Hive集成："></a>HBase和Hive集成：</h4><pre><code>此时实质上Hive就是HBase的一个客户端!
应用场景：
    日志文件 --&gt; hive --&gt; hive-hbase-table --&gt; insert .. select ...
</code></pre><h4 id="电商订单查询之HBase："><a href="#电商订单查询之HBase：" class="headerlink" title="电商订单查询之HBase："></a>电商订单查询之HBase：</h4><pre><code>1.用户下单后--&gt;存到RDBMS[关系型数据库]中 即未完成的订单
2.订单完成后--&gt;数据迁移到HBase中

查询比如三个月内的订单：
 1.主表[订单显示表]：
    rowkey: userid_orderCreateTime_orderId  
    cfname: orderInfor
    [本来userid+orderCreateTime就可以保证唯一性，加orderId的原因是为了二次查询]
 2.订单详情表:
     rowkey: orderId_orderItemId
    cfname: itemInfor
 3.索引表：
     比如用户输入订单号：我们后台Redis或者MongoDB里会有相对应的订单编码
     获取到订单编码然后到索引表中去查
         索引表的rowkey：订单编号 column: 主表的rowkey
         就可以到hbase主表中查询订单
</code></pre><h5 id="补充文件类型-tsv-tab-csv-comma-："><a href="#补充文件类型-tsv-tab-csv-comma-：" class="headerlink" title="补充文件类型 .tsv[tab] .csv[comma]："></a>补充文件类型 .tsv[tab] .csv[comma]：</h5><pre><code>user.tsv tsv后缀格式的文件：每行数据以tab 制表符分割的 
user.csv csv后缀格式的文件：每行数据以逗号分割的
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-DMP" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/DMP/" class="article-date">
  	<time datetime="2018-04-05T13:29:28.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/DMP/">
        dmp介绍
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>dmp</p>
<p>（数据管理平台）<br> 编辑<br>DMP(Data Management Platform)数据管理平台，是把分散的多方数据进行整合纳入统一的技术平台，并对这些数据进行标准化和细分，让用户可以把这些细分结果推向现有的互动营销环境里的平台。<br>中文名<br>数据管理平台<br>外文名<br>Data Management Platform<br>释    义<br>数据进行整合纳入统一的技术平台<br>简    称<br>DMP<br>目录<br>.    1 作用<br>.    2 类型<br>作用<br>编辑<br>•能快速查询、反馈和快速呈现结果<br>•能帮助客户更快进入到市场周期中<br>•能促成企业用户和合作伙伴之间的合作<br>•能深入的预测分析并作出反应<br>•能带来各方面的竞争优势<br>•能降低信息获取及人力成本<br>类型<br>编辑<br>1、结构化的数据，比如Oracle数据库数据等；<br>　　2、非结构化的数据，比如各种文件、图像、音频等数据，等等。<br>　　结构化数据（即数据库数据）在当今的信息系统中占据最核心、最重要的位置。结构化数据从产生―使用―消亡这样一个完整过程的管理，就是数据生命周期管理（所谓的ILM）。<br>核心元素包括：<br>•数据整合及标准化能力：采用统一化的方式，将各方数据吸纳整合。<br>•数据细分管理能力：创建出独一无二、有意义的客户细分，进行有效营销活动。<br>•功能健全的数据标签：提供数据标签灵活性，便于营销活动的使用。<br>•自助式的用户界面：基于网页web界面或其他集成方案直接获取数据工具，功能和几种形式报表和分析。<br>•相关渠道环境的连接：跟相关渠道的集成，包含网站端、展示广告、电子邮件以及搜索和视频，让营销者能找到、定位和提供细分群体相关高度的营销信息。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-文本相似度怎么比较---TF-IDF算法及应用" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/05/文本相似度怎么比较---TF-IDF算法及应用/" class="article-date">
  	<time datetime="2018-04-05T11:29:30.000Z" itemprop="datePublished">2018-04-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/05/文本相似度怎么比较---TF-IDF算法及应用/">
        文本相似度比较-----TF-IDF算法及应用
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>【算法】TF-IDF算法及应用</p>
<p>小编邀请您，先思考：</p>
<blockquote>
<p>1 如何计算TF-IDF？<br>2 TF-IDF有什么应用？<br>3 如何提取文本的关键词和摘要？</p>
</blockquote>
<p>有一篇很长的文章，我要用计算机提取它的关键词（Automatic Keyphrase extraction），完全不加以人工干预，请问怎样才能正确做到？</p>
<p>这个问题涉及到数据挖掘、文本处理、信息检索等很多计算机前沿领域，但是出乎意料的是，有一个非常简单的经典算法，可以给出令人相当满意的结果。它简单到都不需要高等数学，普通人只用10分钟就可以理解，这就是我今天想要介绍的TF-IDF（<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a> ）算法。</p>
<p>让我们从一个实例开始讲起。假定现在有一篇长文《中国的蜜蜂养殖》，我们准备用计算机提取它的关键词。</p>
<p>一个容易想到的思路，就是找到出现次数最多的词。如果某个词很重要，它应该在这篇文章中多次出现。于是，我们进行”词频”（Term Frequency，缩写为TF）统计。</p>
<p>结果你肯定猜到了，出现次数最多的词是—-“的”、”是”、”在”—-这一类最常用的词。它们叫做”停用词”（ <a href="http://baike.baidu.com/view/3784680.htm" target="_blank" rel="noopener">http://baike.baidu.com/view/3784680.htm</a> ）（stop words），表示对找到结果毫无帮助、必须过滤掉的词。</p>
<p>假设我们把它们都过滤掉了，只考虑剩下的有实际意义的词。这样又会遇到了另一个问题，我们可能发现”中国”、”蜜蜂”、”养殖”这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？</p>
<p>显然不是这样。因为”中国”是很常见的词，相对而言，”蜜蜂”和”养殖”不那么常见。如果这三个词在一篇文章的出现次数一样多，有理由认为，”蜜蜂”和”养殖”的重要程度要大于”中国”，也就是说，在关键词排序上面，”蜜蜂”和”养殖”应该排在”中国”的前面。</p>
<p>所以，我们需要一个重要性调整系数，衡量一个词是不是常见词。<strong>如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。</strong></p>
<p>用统计学语言表达，就是在词频的基础上，要对每个词分配一个”重要性”权重。最常见的词（”的”、”是”、”在”）给予最小的权重，较常见的词（”中国”）给予较小的权重，较少见的词（”蜜蜂”、”养殖”）给予较大的权重。这个权重叫做”逆文档频率”（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。</p>
<p><strong>知道了”词频”（TF）和”逆文档频率”（IDF）以后，将这两个值相乘，就得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。</strong></p>
<p>下面就是这个算法的细节。<br><strong>第一步，计算词频。</strong><br><img src="https://s1.ax1x.com/2018/05/04/Ct7BC9.png" alt="Ct7BC9.png"><br>考虑到文章有长短之分，为了便于不同文章的比较，进行”词频”标准化。<br><img src="https://s1.ax1x.com/2018/05/04/Ct7cDK.png" alt="Ct7cDK.png"><br>或者<br><img src="https://s1.ax1x.com/2018/05/04/Ct7RED.png" alt="Ct7RED.png"></p>
<p><strong>第二步，计算逆文档频率。</strong><br>这时，需要一个语料库（corpus），用来模拟语言的使用环境。<br><img src="https://s1.ax1x.com/2018/05/04/Ct74Cd.png" alt="Ct74Cd.png"><br>如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。</p>
<p><strong>第三步，计算TF-IDF。</strong><br><img src="https://s1.ax1x.com/2018/05/04/CtHqQ1.png" alt="CtHqQ1.png"><br><strong>可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。</strong>所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。</p>
<p>还是以《中国的蜜蜂养殖》为例，假定该文长度为1000个词，”中国”、”蜜蜂”、”养殖”各出现20次，则这三个词的”词频”（TF）都为0.02。然后，搜索Google发现，包含”的”字的网页共有250亿张，假定这就是中文网页总数。包含”中国”的网页共有62.3亿张，包含”蜜蜂”的网页为0.484亿张，包含”养殖”的网页为0.973亿张。则它们的逆文档频率（IDF）和TF-IDF如下：<br><img src="https://s1.ax1x.com/2018/05/04/CtbUfJ.png" alt="CtbUfJ.png"><br>从上表可见，”蜜蜂”的TF-IDF值最高，”养殖”其次，”中国”最低。（如果还计算”的”字的TF-IDF，那将是一个极其接近0的值。）所以，如果只选择一个词，”蜜蜂”就是这篇文章的关键词。</p>
<p>除了自动提取关键词，TF-IDF算法还可以用于许多别的地方。比如，信息检索时，对于每个文档，都可以分别计算一组搜索词（”中国”、”蜜蜂”、”养殖”）的TF-IDF，将它们相加，就可以得到整个文档的TF-IDF。这个值最高的文档就是与搜索词最相关的文档。</p>
<p>TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以”词频”衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。）</p>
<p><strong>找出相似文章</strong><br>我们再来研究另一个相关的问题。有些时候，除了找到关键词，我们还希望找到与原文章相似的其他文章。比如，”Google新闻”在主新闻下方，还提供多条相似的新闻。<br><img src="https://s1.ax1x.com/2018/05/04/Ctbs0K.jpg" alt="Ctbs0K.jpg"></p>
<p>为了找出相似的文章，需要用到”余弦相似性” （ <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Cosine_similarity</a> ）（cosine similiarity）。下面，我举一个例子来说明，什么是”余弦相似性”。</p>
<p>为了简单起见，我们先从句子着手。</p>
<blockquote>
<p>   句子A：我喜欢看电视，不喜欢看电影。<br>   句子B：我不喜欢看电视，也不喜欢看电影。</p>
</blockquote>
<p>请问怎样才能计算上面两句话的相似程度？<br>基本思路是：如果这两句话的用词越相似，它们的内容就应该越相似。因此，可以从词频入手，计算它们的相似程度。</p>
<p><strong>第一步，分词。</strong></p>
<blockquote>
<p>   句子A：我/喜欢/看/电视，不/喜欢/看/电影。<br>   句子B：我/不/喜欢/看/电视，也/不/喜欢/看/电影。</p>
</blockquote>
<p><strong>第二步，列出所有的词。</strong></p>
<blockquote>
<p>   我，喜欢，看，电视，电影，不，也。</p>
</blockquote>
<p><strong>第三步，计算词频。</strong></p>
<blockquote>
<p>   句子A：我 1，喜欢 2，看 2，电视 1，电影 1，不 1，也 0。<br>   句子B：我 1，喜欢 2，看 2，电视 1，电影 1，不 2，也 1。</p>
</blockquote>
<p><strong>第四步，写出词频向量。</strong></p>
<blockquote>
<p>   句子A：[1, 2, 2, 1, 1, 1, 0]<br>   句子B：[1, 2, 2, 1, 1, 2, 1]</p>
</blockquote>
<p>到这里，问题就变成了如何计算这两个向量的相似程度。<br>我们可以把它们想象成空间中的两条线段，都是从原点（[0, 0, …]）出发，指向不同的方向。两条线段之间形成一个夹角，如果夹角为0度，意味着方向相同、线段重合；如果夹角为90度，意味着形成直角，方向完全不相似；如果夹角为180度，意味着方向正好相反。<strong>因此，我们可以通过夹角的大小，来判断向量的相似程度。夹角越小，就代表越相似。</strong><br><img src="https://s1.ax1x.com/2018/05/04/CtbckD.png" alt="CtbckD.png"><br>以二维空间为例，上图的a和b是两个向量，我们要计算它们的夹角θ。余弦定理告诉我们，可以用下面的公式求得：<br><img src="https://s1.ax1x.com/2018/05/04/Ctb2fH.png" alt="Ctb2fH.png"><br><img src="https://s1.ax1x.com/2018/05/04/Ctbf1A.png" alt="Ctbf1A.png"><br>假定a向量是[x1, y1]，b向量是[x2, y2]，那么可以将余弦定理改写成下面的形式：<br><img src="https://s1.ax1x.com/2018/05/04/CtbInP.png" alt="CtbInP.png"><br><img src="https://s1.ax1x.com/2018/05/04/Ctbo0f.png" alt="Ctbo0f.png"><br>数学家已经证明，余弦的这种计算方法对n维向量也成立。假定A和B是两个n维向量，A是 [A1, A2, …, An] ，B是 [B1, B2, …, Bn] ，则A与B的夹角θ的余弦等于：<br><img src="https://s1.ax1x.com/2018/05/04/CtbvXq.png" alt="CtbvXq.png"><br>使用这个公式，我们就可以得到，句子A与句子B的夹角的余弦。<br><img src="https://s1.ax1x.com/2018/05/04/CtqCAU.png" alt="CtqCAU.png"><br><strong>余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫”余弦相似性”。</strong>所以，上面的句子A和句子B是很相似的，事实上它们的夹角大约为20.3度。<br>由此，我们就得到了”找出相似文章”的一种算法：<br>　　（1）使用TF-IDF算法，找出两篇文章的关键词；<br>　　（2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；<br>　　（3）生成两篇文章各自的词频向量；<br>　　（4）计算两个向量的余弦相似度，值越大就表示越相似。<br>“余弦相似度”是一种非常有用的算法，只要是计算两个向量的相似程度，都可以采用它。</p>
<p><strong>自动摘要</strong><br>有时候，很简单的数学方法，就可以完成很复杂的任务。<br>前两部分就是很好的例子。仅仅依靠统计词频，就能找出关键词和相似文章。虽然它们算不上效果最好的方法，但肯定是最简便易行的方法。<br>接下来讨论如何通过词频，对文章进行自动摘要（Automatic summarization）。</p>
<p>如果能从3000字的文章，提炼出150字的摘要，就可以为读者节省大量阅读时间。由人完成的摘要叫”人工摘要”，由机器完成的就叫”自动摘要”。许多网站都需要它，比如论文网站、新闻网站、搜索引擎等等。2007年，美国学者的论文《A Survey on Automatic Text Summarization》（Dipanjan Das, Andre F.T. Martins, 2007）总结了目前的自动摘要算法。其中，很重要的一种就是词频统计。</p>
<p>这种方法最早出自1958年的IBM公司科学家H.P. Luhn的论文《The Automatic Creation of Literature Abstracts》。<br>Luhn博士认为，文章的信息都包含在句子中，有些句子包含的信息多，有些句子包含的信息少。”自动摘要”就是要找出那些包含信息最多的句子。<br>句子的信息量用”关键词”来衡量。如果包含的关键词越多，就说明这个句子越重要。Luhn提出用”簇”（cluster）表示关键词的聚集。所谓”簇”就是包含多个关键词的句子片段。<br><img src="https://s1.ax1x.com/2018/05/04/CtqPNF.png" alt="CtqPNF.png"><br>上图就是Luhn原始论文的插图，被框起来的部分就是一个”簇”。只要关键词之间的距离小于”门槛值”，它们就被认为处于同一个簇之中。Luhn建议的门槛值是4或5。也就是说，如果两个关键词之间有5个以上的其他词，就可以把这两个关键词分在两个簇。</p>
<p>下一步，对于每个簇，都计算它的重要性分值。<br><img src="https://s1.ax1x.com/2018/05/04/CtqA39.png" alt="CtqA39.png"><br>以前图为例，其中的簇一共有7个词，其中4个是关键词。因此，它的重要性分值等于 ( 4 x 4 ) / 7 = 2.3。</p>
<p>然后，找出包含分值最高的簇的句子（比如5句），把它们合在一起，就构成了这篇文章的自动摘要。具体实现可以参见《Mining the Social Web: Analyzing Data from Facebook, Twitter, LinkedIn, and Other Social Media Sites》（O’Reilly, 2011）一书的第8章，python代码见github。</p>
<p>Luhn的这种算法后来被简化，不再区分”簇”，只考虑句子包含的关键词。下面就是一个例子（采用伪码表示），只考虑关键词首先出现的句子。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">　　Summarizer(originalText, maxSummarySize):</span><br><span class="line">　　　　// 计算原始文本的词频，生成一个数组，比如[(<span class="number">10</span>,<span class="string">'the'</span>), (<span class="number">3</span>,<span class="string">'language'</span>), (<span class="number">8</span>,<span class="string">'code'</span>)...]</span><br><span class="line">　　　　wordFrequences = getWordCounts(originalText)</span><br><span class="line">　　　　// 过滤掉停用词，数组变成[(<span class="number">3</span>, <span class="string">'language'</span>), (<span class="number">8</span>, <span class="string">'code'</span>)...]</span><br><span class="line">　　　　contentWordFrequences = filtStopWords(wordFrequences)</span><br><span class="line">　　　　// 按照词频进行排序，数组变成[<span class="string">'code'</span>, <span class="string">'language'</span>...]</span><br><span class="line">　　　　contentWordsSortbyFreq = sortByFreqThenDropFreq(contentWordFrequences)</span><br><span class="line">　　　　// 将文章分成句子</span><br><span class="line">　　　　sentences = getSentences(originalText)</span><br><span class="line">　　　　// 选择关键词首先出现的句子</span><br><span class="line">　　　　setSummarySentences = &#123;&#125;</span><br><span class="line">　　　　foreach word <span class="keyword">in</span> contentWordsSortbyFreq:</span><br><span class="line">　　　　　　firstMatchingSentence = search(sentences, word)</span><br><span class="line">　　　　　　setSummarySentences.add(firstMatchingSentence)</span><br><span class="line">　　　　　　<span class="keyword">if</span> setSummarySentences.size() = maxSummarySize:</span><br><span class="line">　　　　　　　　<span class="keyword">break</span></span><br><span class="line">　　　　// 将选中的句子按照出现顺序，组成摘要</span><br><span class="line">　　　　summary = <span class="string">""</span></span><br><span class="line">　　　　foreach sentence <span class="keyword">in</span> sentences:</span><br><span class="line">　　　　　　<span class="keyword">if</span> sentence <span class="keyword">in</span> setSummarySentences:</span><br><span class="line">　　　　　　　　summary = summary + <span class="string">" "</span> + sentence</span><br><span class="line">　　　　<span class="keyword">return</span> summary</span><br></pre></td></tr></table></figure></p>
<p>类似的算法已经被写成了工具，比如基于Java的Classifier4J库的SimpleSummariser模块、基于C语言的OTS库、以及基于classifier4J的C#实现和python实现。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2018 rongyuewu
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>